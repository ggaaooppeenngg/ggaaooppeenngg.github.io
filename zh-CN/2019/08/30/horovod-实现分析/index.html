<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="_85tctgPWrqH2EPVuuD5IT6KE-tW8nH0hTISJDMnShg">
  <meta name="baidu-site-verification" content="bb16c5b1fd3302c18e0015bef11eea42">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ggaaooppeenngg.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12,"onmobile":false},"hljswrap":true,"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="背景Horovod 是一个兼容主流计算框架的分布式机器学习训练框架，主要基于的算法是 AllReduce，这个是 baidu-research 在17年做的一个实现，这个东西原来是高性能计算范畴里的东西应用了 MPI 并行计算接口来实现，这是并行计算里的一个框架，已经很老了，这里有一个介绍 MPI 的 tutorial 写的比较好。 在介绍 horovod 的之前需要解释一下 AllReduce。">
<meta property="og:type" content="article">
<meta property="og:title" content="horovod 实现分析">
<meta property="og:url" content="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="ggaaooppeenngg">
<meta property="og:description" content="背景Horovod 是一个兼容主流计算框架的分布式机器学习训练框架，主要基于的算法是 AllReduce，这个是 baidu-research 在17年做的一个实现，这个东西原来是高性能计算范畴里的东西应用了 MPI 并行计算接口来实现，这是并行计算里的一个框架，已经很老了，这里有一个介绍 MPI 的 tutorial 写的比较好。 在介绍 horovod 的之前需要解释一下 AllReduce。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_reduce_1.png">
<meta property="og:image" content="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_allreduce_1.png">
<meta property="og:image" content="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_allgather_1.png">
<meta property="og:image" content="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_broadcast_1.png">
<meta property="article:published_time" content="2019-08-29T17:22:44.000Z">
<meta property="article:modified_time" content="2025-03-28T10:39:05.292Z">
<meta property="article:author" content="ggaaooppeenngg">
<meta property="article:tag" content="horovod">
<meta property="article:tag" content="分布式训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_reduce_1.png">


<link rel="canonical" href="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","path":"zh-CN/2019/08/30/horovod-实现分析/","title":"horovod 实现分析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>horovod 实现分析 | ggaaooppeenngg</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-62096626-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-62096626-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?bb16c5b1fd3302c18e0015bef11eea42"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ggaaooppeenngg</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">为什么计算机科学是无限的但生命是有限的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">135</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">14</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">79</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Horovod-%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.</span> <span class="nav-text">Horovod 的介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8"><span class="nav-number">2.1.</span> <span class="nav-text">使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.2.</span> <span class="nav-text">实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%82%E9%85%8D%E5%B1%82%E5%92%8C%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95"><span class="nav-number">2.2.1.</span> <span class="nav-text">适配层和压缩算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E5%B1%82"><span class="nav-number">2.2.2.</span> <span class="nav-text">统一层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E5%AD%90%E5%AE%9E%E7%8E%B0%E5%B1%82"><span class="nav-number">2.3.</span> <span class="nav-text">算子实现层</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ggaaooppeenngg</p>
  <div class="site-description" itemprop="description">为什么计算机科学是无限的但生命是有限的</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">79</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">135</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ggaaooppeenngg" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ggaaooppeenngg" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:peng.gao.dut@gmail.com" title="E-Mail → mailto:peng.gao.dut@gmail.com" rel="noopener me" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="horovod 实现分析 | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          horovod 实现分析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-08-30 01:22:44" itemprop="dateCreated datePublished" datetime="2019-08-30T01:22:44+08:00">2019-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2019/08/30/horovod-实现分析/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Horovod 是一个兼容主流计算框架的分布式机器学习训练框架，主要基于的算法是 AllReduce，这个是 baidu-research 在17年做的一个实现，这个东西原来是高性能计算范畴里的东西应用了 MPI 并行计算接口来实现，这是并行计算里的一个框架，已经很老了，<a target="_blank" rel="noopener" href="https://mpitutorial.com/tutorials/">这里</a>有一个介绍 MPI 的 tutorial 写的比较好。</p>
<p>在介绍 horovod 的之前需要解释一下 AllReduce。在 MapReduce 里面 reduce 被翻译成了规约，在上面提到的 MPI tutorial 里面的解释是</p>
<blockquote>
<p>Reduce is a classic concept from functional programming. Data reduction involves reducing a set of numbers into a smaller set of numbers via a function. For example, let’s say we have a list of numbers <code>[1, 2, 3, 4, 5]</code>. Reducing this list of numbers with the sum function would produce <code>sum([1, 2, 3, 4, 5]) = 15</code>. Similarly, the multiplication reduction would yield <code>multiply([1, 2, 3, 4, 5]) = 120</code>.</p>
</blockquote>
<p>就是说把一个大的集合“缩减”成了小的集合，这里要注意的是这种缩减的计算是要满足交换律的，也就是减法或者除法是不行的，因为在并行计算当中不太好去控制计算的顺序。Reduce 就是这个意思，具体到 MPI_Reduce 就是把不同节点的数字“缩减”到一个节点上，支持的计算方式有加法乘法和取大小值等。</p>
<p>教程中给出的 Reduce 是求和。</p>
<img data-src="/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_reduce_1.png" class="">
<p>AllReduce 就是在每个节点都获得 Reduce 的结果</p>
<img data-src="/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_allreduce_1.png" class="">
<p>基于这个标准就有很多的 All-Reduce 的实现，比如 Ring-Reduce，这个实现分两部分，一部分是 Scatter-Reduce 另一部分是 All-Gather。最早是在<a target="_blank" rel="noopener" href="http://andrew.gibiansky.com/">这篇 post</a>里提到的。这个算法的好处是可以摆脱之前 PS 非常依赖 Parameter-Server 的带宽，Parameter-Server 的带宽会成为计算瓶颈的问题，而 AllReduce 可以让每个节点在带宽传输中的位置是对等的，并且减少传输次数。具体的算法可以看文章的解释，scatter-reduce 就是让每个节点有 K/N 的一个 reduce(也就是 sum)，然后把自己的一个 K/N 的 reduce 再传递给其他节点，每个节点只和自己相邻的节点通信。</p>
<blockquote>
<p>In the system we described, each of the N GPUs will send and receive values N-1 times for the scatter-reduce, and N-1 times for the allgather. Each time, the GPUs will send K / N values, where K is the total number of values in array being summed across the different GPUs. Therefore, the total amount of data transferred to and from every GPU is</p>
</blockquote>
<blockquote>
<p>Data Transferred=2(N−1)KN</p>
</blockquote>
<p>数据传输量在 N 比较大的时候越没有影响，这就消弭了多节点给 Parameter-Server 造成的瓶颈。</p>
<p>还有一些其他术语，假设有 4 台 4 卡的 GPU 服务器。size 是工作进程（GPU）的数量(6)，rank 是所有工作进程的 id(0-15)，local rank 是当前服务器上的 id(0-3)。</p>
<h2 id="Horovod-的介绍"><a href="#Horovod-的介绍" class="headerlink" title="Horovod 的介绍"></a>Horovod 的介绍</h2><p>使用 horovod 有一定的侵入性，代码需要一定的修改才能变成适配分布式训练，但是有一个好处就是适配的成本不高，并且 horovod 提供的各种框架的支持可以让 horovod 比较好的在各个框架的基础上使用，他支持 tensorflow/keras/mxnet/pytorch，MPI 的实现也有很多，比如 OpenMPI 还有 Nvidia 的 NCCL，还有 facebook 的 gloo，他们都实现了一种并行计算的通信和计算方式。而且 horovod 的本身的实现也很简单。</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>以 <a target="_blank" rel="noopener" href="https://github.com/horovod/horovod/blob/master/examples/keras_imagenet_resnet50.py">Keras 用 ResNet50 训练 ImageNet </a>为例，主要侵入了几部分 <code>hvd.init()</code> 这个是 MPI 的初始化，让并行进程能够知道自己的 rank/local_rank 等信息。</p>
<p>第二部根据 local_rank（相当于单节点上的第n张卡），并且设置不占用全部显存，按需分配（可能因内没有统一管理导致显存碎片），然后传递给 keras 设置 session。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Horovod: pin GPU to be used to process local rank (one GPU per process)</span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = True</span><br><span class="line">config.gpu_options.visible_device_list = str(hvd.local_rank())</span><br><span class="line">K.set_session(tf.Session(config=config))</span><br></pre></td></tr></table></figure>
<p>然后在 rank 0 上恢复一个 checkpoint 并且广播给其他节点，这里的 broadcast 后面会介绍。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># If set &gt; 0, will resume training from a given checkpoint.</span><br><span class="line">resume_from_epoch = 0</span><br><span class="line">for try_epoch in range(args.epochs, 0, -1):</span><br><span class="line">    if os.path.exists(args.checkpoint_format.format(epoch=try_epoch)):</span><br><span class="line">        resume_from_epoch = try_epoch</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line"># Horovod: broadcast resume_from_epoch from rank 0 (which will have</span><br><span class="line"># checkpoints) to other ranks.</span><br><span class="line">resume_from_epoch = hvd.broadcast(resume_from_epoch, 0, name=&#x27;resume_from_epoch&#x27;)</span><br><span class="line"></span><br><span class="line"># Horovod: print logs on the first worker.</span><br><span class="line">verbose = 1 if hvd.rank() == 0 else 0</span><br></pre></td></tr></table></figure>
<p>设定传输的压缩函数，具体的压缩后面会提到，然后要么从之前的模型恢复要么重新训练。关键的 wrapper 在 <code>opt</code> 上，会给本地的 <code>opt</code> 包装一个 <code>DistributedOptimizer</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># Horovod: (optional) compression algorithm.</span><br><span class="line">compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none</span><br><span class="line"></span><br><span class="line"># Restore from a previous checkpoint, if initial_epoch is specified.</span><br><span class="line"># Horovod: restore on the first worker which will broadcast both model and optimizer weights</span><br><span class="line"># to other workers.</span><br><span class="line">if resume_from_epoch &gt; 0 and hvd.rank() == 0:</span><br><span class="line">    model = hvd.load_model(args.checkpoint_format.format(epoch=resume_from_epoch),</span><br><span class="line">                           compression=compression)</span><br><span class="line">else:</span><br><span class="line">    # ResNet-50 model that is included with Keras is optimized for inference.</span><br><span class="line">    # Add L2 weight decay &amp; adjust BN settings.</span><br><span class="line">    model_config = model.get_config()</span><br><span class="line">    for layer, layer_config in zip(model.layers, model_config[&#x27;layers&#x27;]):</span><br><span class="line">        if hasattr(layer, &#x27;kernel_regularizer&#x27;):</span><br><span class="line">            regularizer = keras.regularizers.l2(args.wd)</span><br><span class="line">            layer_config[&#x27;config&#x27;][&#x27;kernel_regularizer&#x27;] = \</span><br><span class="line">                &#123;&#x27;class_name&#x27;: regularizer.__class__.__name__,</span><br><span class="line">                 &#x27;config&#x27;: regularizer.get_config()&#125;</span><br><span class="line">        if type(layer) == keras.layers.BatchNormalization:</span><br><span class="line">            layer_config[&#x27;config&#x27;][&#x27;momentum&#x27;] = 0.9</span><br><span class="line">            layer_config[&#x27;config&#x27;][&#x27;epsilon&#x27;] = 1e-5</span><br><span class="line"></span><br><span class="line">    model = keras.models.Model.from_config(model_config)</span><br><span class="line"></span><br><span class="line">    # Horovod: adjust learning rate based on number of GPUs.</span><br><span class="line">    opt = keras.optimizers.SGD(lr=args.base_lr * hvd.size(),</span><br><span class="line">                               momentum=args.momentum)</span><br><span class="line"></span><br><span class="line">    # Horovod: add Horovod Distributed Optimizer.</span><br><span class="line">    opt = hvd.DistributedOptimizer(opt, compression=compression)</span><br><span class="line"></span><br><span class="line">    model.compile(loss=keras.losses.categorical_crossentropy,</span><br><span class="line">                  optimizer=opt,</span><br><span class="line">                  metrics=[&#x27;accuracy&#x27;, &#x27;top_k_categorical_accuracy&#x27;])</span><br></pre></td></tr></table></figure>
<p>然后设置一些回调函数，<code>hvd.callbacks.BroadcastGlobalVariablesCallback(0)</code> 保证的是 rank 0 上的所有参数只在 rank 0 初始化，然后广播给其他节点，后面是学习率 decay 的设置和一些统计信息的回调打印。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">callbacks = [</span><br><span class="line">    # Horovod: broadcast initial variable states from rank 0 to all other processes.</span><br><span class="line">    # This is necessary to ensure consistent initialization of all workers when</span><br><span class="line">    # training is started with random weights or restored from a checkpoint.</span><br><span class="line">    hvd.callbacks.BroadcastGlobalVariablesCallback(0),</span><br><span class="line"></span><br><span class="line">    # Horovod: average metrics among workers at the end of every epoch.</span><br><span class="line">    #</span><br><span class="line">    # Note: This callback must be in the list before the ReduceLROnPlateau,</span><br><span class="line">    # TensorBoard, or other metrics-based callbacks.</span><br><span class="line">    hvd.callbacks.MetricAverageCallback(),</span><br><span class="line"></span><br><span class="line">    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final</span><br><span class="line">    # accuracy. Scale the learning rate `lr = 1.0` ---&gt; `lr = 1.0 * hvd.size()` during</span><br><span class="line">    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.</span><br><span class="line">    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=args.warmup_epochs, verbose=verbose),</span><br><span class="line"></span><br><span class="line">    # Horovod: after the warmup reduce learning rate by 10 on the 30th, 60th and 80th epochs.</span><br><span class="line">    hvd.callbacks.LearningRateScheduleCallback(start_epoch=args.warmup_epochs, end_epoch=30, multiplier=1.),</span><br><span class="line">    hvd.callbacks.LearningRateScheduleCallback(start_epoch=30, end_epoch=60, multiplier=1e-1),</span><br><span class="line">    hvd.callbacks.LearningRateScheduleCallback(start_epoch=60, end_epoch=80, multiplier=1e-2),</span><br><span class="line">    hvd.callbacks.LearningRateScheduleCallback(start_epoch=80, multiplier=1e-3),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>最后直接用 allreduce 计算一个 evaluation score。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Evaluate the model on the full data set.</span><br><span class="line">score = hvd.allreduce(model.evaluate_generator(input_fn(False, args.train_dir, args.val_batch_size),NUM_IMAGES[&#x27;validation&#x27;]))</span><br></pre></td></tr></table></figure>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><h4 id="适配层和压缩算法"><a href="#适配层和压缩算法" class="headerlink" title="适配层和压缩算法"></a>适配层和压缩算法</h4><p>horovod 的实现主要分几部分，第一部分是一个适配层，用于兼容各种框架，比如 tensorflow 的适配就是实现一个新的 Op，这个可以参考 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/extend/op">add new op</a>，里面规范了 Tensorflow 自定义算子的实现。</p>
<blockquote>
<p>请注意，生成的函数将获得一个蛇形名称（以符合 PEP8）。因此，如果您的操作在 C++ 文件中命名为 ZeroOut，则 Python 函数将称为 zero_out。</p>
</blockquote>
<p>C++ 的定义是驼峰的，生成出来的 python 函数是下划线小写的，所以最后对应的是，适配Op的代码在 <a target="_blank" rel="noopener" href="https://github.com/horovod/horovod/tree/master/horovod/tensorflow">horovod/tensorflow</a> 目录下面</p>
<table>
<thead>
<tr>
<th>C++</th>
<th>Python</th>
</tr>
</thead>
<tbody><tr>
<td>HorovodAllgather</td>
<td>horovod_allgather</td>
</tr>
<tr>
<td>HorovodAllreduce</td>
<td>horovod_allreduce</td>
</tr>
<tr>
<td>HorovodBroadcast</td>
<td>horovod_broadcast</td>
</tr>
</tbody></table>
<p>另外在适配层可以加入一些压缩算法(在 <code>horovod/[framework]/compression.py</code>)，我觉得压缩算法和框架无关的，放到适配层下面可能有别的原因，比如 tensorflow 默认带了一个 float16 压缩，具体的其他压缩算法比如<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.07389">3LC</a>，可以通过有损压缩或者无损压缩提高带宽利用率。</p>
<h4 id="统一层"><a href="#统一层" class="headerlink" title="统一层"></a>统一层</h4><p>这一层的实现是统一的，所有的适配层最后都是发出一些 Op+Tensor 的 Message 到队列中，后台初始化的时候会有一个专门的线程专门消费这个队列。他有一个同步消息的过程，相当于这个 tensor 在所有节点上都就绪以后就可以开始计算了，主体的流程是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">// The coordinator currently follows a master-worker paradigm. Rank zero acts</span><br><span class="line">// as the master (the &quot;coordinator&quot;), whereas all other ranks are simply</span><br><span class="line">// workers. Each rank runs its own background thread which progresses in ticks.</span><br><span class="line">// In each tick, the following actions happen:</span><br><span class="line">//</span><br><span class="line">//      a) The workers send a Request to the coordinator, indicating what</span><br><span class="line">//      they would like to do (which tensor they would like to gather and</span><br><span class="line">//      reduce, as well as their shape and type). They repeat this for every</span><br><span class="line">//      tensor that they would like to operate on.</span><br><span class="line">//</span><br><span class="line">//      b) The workers send an empty &quot;DONE&quot; message to the coordinator to</span><br><span class="line">//      indicate that there are no more tensors they wish to operate on.</span><br><span class="line">//</span><br><span class="line">//      c) The coordinator receives the Requests from the workers, as well</span><br><span class="line">//      as from its own TensorFlow ops, and stores them in a [request table]. The</span><br><span class="line">//      coordinator continues to receive Request messages until it has</span><br><span class="line">//      received MPI_SIZE number of empty &quot;DONE&quot; messages.</span><br><span class="line">//</span><br><span class="line">//      d) The coordinator finds all tensors that are ready to be reduced,</span><br><span class="line">//      gathered, or all operations that result in an error. For each of those,</span><br><span class="line">//      it sends a Response to all the workers. When no more Responses</span><br><span class="line">//      are available, it sends a &quot;DONE&quot; response to the workers. If the process</span><br><span class="line">//      is being shutdown, it instead sends a &quot;SHUTDOWN&quot; response.</span><br><span class="line">//</span><br><span class="line">//      e) The workers listen for Response messages, processing each one by</span><br><span class="line">//      doing the required reduce or gather, until they receive a &quot;DONE&quot;</span><br><span class="line">//      response from the coordinator. At that point, the tick ends.</span><br><span class="line">//      If instead of &quot;DONE&quot; they receive &quot;SHUTDOWN&quot;, they exit their background</span><br><span class="line">//      loop.</span><br></pre></td></tr></table></figure>

<p>简单来讲就是说 coordinator 集 size 个 request DONE，然后找出就绪的 tensor （在 message_table 里面查找）构造出一个 read_to_reduce 的列表，然后发出 size 个 request 告知进程进行计算，然后 worker 接受到 response 开始真正的计算过程(通过 op_manager 具体执行)。</p>
<p>这是整体同步的过程，如果打开 horovod 的 trace log(<code>HOROVOD_LOG_LEVEL=trace</code>) 就能看到同步的过程。horovod 的主要 Op 除了 AllReduce 之外还有 allgather 和 broadcast。</p>
<h3 id="算子实现层"><a href="#算子实现层" class="headerlink" title="算子实现层"></a>算子实现层</h3><p>具体的 op 在 <a target="_blank" rel="noopener" href="https://github.com/horovod/horovod/tree/master/horovod/common/ops">common/op</a> 可以看到有 NCCL/Gloo/MPI 等等的，这些由 op_manager 管理，他会根据优先级找到可以用来计算的 op 进行计算，比如 MPI 用的就是 MPI_Allreduce，具体 scatter-gather 和 all-gather openMPI 有现成的实现，NCCL 就直接调用 <code>ncclAllReduce</code>，比较新的 nccl 也支持跨节点的 allreduce 了，不用自己再套一层。</p>
<p>除了 allreduce 之外，还有两个比较重要的算子。</p>
<p>allgather 主要是比 allreduce 少一层 reduce，所有数据被发送到所有进程就可以。allreduce 的第二步就是把每个进程的 scatter-reduce 的 reduce 结果发送到所有进程。</p>
<img data-src="/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_allgather_1.png" class="">
<p>broadcast 的作用是一对多的广播，主要是把初始化的参数同步给其他进程的时候使用。</p>
<img data-src="/zh-CN/2019/08/30/horovod-%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/mpi_broadcast_1.png" class="">




    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/horovod/" rel="tag"># horovod</a>
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/" rel="tag"># 分布式训练</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/zh-CN/2019/08/12/SysML2019-%E6%A6%82%E8%A7%881/" rel="prev" title="SysML2019 概览1">
                  <i class="fa fa-angle-left"></i> SysML2019 概览1
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/zh-CN/2019/10/14/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E4%B9%8B%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B%E5%92%8C%E5%BA%94%E7%94%A8/" rel="next" title="贝叶斯优化之高斯过程和应用">
                  贝叶斯优化之高斯过程和应用 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2014 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">ggaaooppeenngg</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"ggaaooppeenngg","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
