<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="_85tctgPWrqH2EPVuuD5IT6KE-tW8nH0hTISJDMnShg">
  <meta name="baidu-site-verification" content="bb16c5b1fd3302c18e0015bef11eea42">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ggaaooppeenngg.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12,"onmobile":false},"hljswrap":true,"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="参考Pytorch版本的llama-from-scratch。原文中的RmsNorm的平均值多算了一个维度，这里改成了正确的版本。 首先需要下载TinyShakespeare数据集，这是一个莎士比亚文字数据集。本文档将大致遵循论文的布局，并跳过一些明显的步骤，比如设置虚拟环境和安装依赖项。 我们最终将实现的内容预览： 1234567891011121314println!(generate(lla">
<meta property="og:type" content="article">
<meta property="og:title" content="Rust从零实现llama">
<meta property="og:url" content="https://ggaaooppeenngg.github.io/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/index.html">
<meta property="og:site_name" content="ggaaooppeenngg">
<meta property="og:description" content="参考Pytorch版本的llama-from-scratch。原文中的RmsNorm的平均值多算了一个维度，这里改成了正确的版本。 首先需要下载TinyShakespeare数据集，这是一个莎士比亚文字数据集。本文档将大致遵循论文的布局，并跳过一些明显的步骤，比如设置虚拟环境和安装依赖项。 我们最终将实现的内容预览： 1234567891011121314println!(generate(lla">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-12-23T08:57:18.000Z">
<meta property="article:modified_time" content="2025-03-28T10:39:05.187Z">
<meta property="article:author" content="ggaaooppeenngg">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Rust">
<meta property="article:tag" content="LLAMA">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://ggaaooppeenngg.github.io/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/","path":"zh-CN/2024/12/23/Rust从零实现llama/","title":"Rust从零实现llama"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Rust从零实现llama | ggaaooppeenngg</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-62096626-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-62096626-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?bb16c5b1fd3302c18e0015bef11eea42"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">ggaaooppeenngg</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">为什么计算机科学是无限的但生命是有限的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">134</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">14</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">78</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E5%B7%A5%E4%BD%9C%EF%BC%9A%E4%BB%8E%E5%B0%8F%E6%A8%A1%E5%9D%97%E5%BC%80%E5%A7%8B%EF%BC%8C%E4%BF%9D%E6%8C%81%E7%A1%AE%E5%AE%9A%E6%80%A7%EF%BC%8C%E7%84%B6%E5%90%8E%E9%80%90%E6%AD%A5%E6%9E%84%E5%BB%BA"><span class="nav-number">1.</span> <span class="nav-text">迭代工作：从小模块开始，保持确定性，然后逐步构建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A1%AE%E4%BF%9D%E4%BD%A0%E7%9A%84%E5%B1%82%E6%8C%89%E9%A2%84%E6%9C%9F%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.</span> <span class="nav-text">确保你的层按预期工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E-Llama"><span class="nav-number">3.</span> <span class="nav-text">关于 Llama</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.</span> <span class="nav-text">设置数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E5%B7%A5%E4%BD%9C%E7%9A%84%E7%AE%80%E5%8D%95%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">设置一个可以工作的简单模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Llama-%E5%85%B7%E4%BD%93%E7%BB%86%E8%8A%82"><span class="nav-number">6.</span> <span class="nav-text">Llama 具体细节</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSNorm"><span class="nav-number">6.1.</span> <span class="nav-text">RMSNorm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rotary-Embeddings"><span class="nav-number">6.2.</span> <span class="nav-text">Rotary Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-Attention"><span class="nav-number">6.3.</span> <span class="nav-text">Self Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MultiHeadRopeAttention"><span class="nav-number">6.4.</span> <span class="nav-text">MultiHeadRopeAttention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SwiGLU"><span class="nav-number">6.5.</span> <span class="nav-text">SwiGLU</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A9%E5%B1%95"><span class="nav-number">7.</span> <span class="nav-text">扩展</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ggaaooppeenngg</p>
  <div class="site-description" itemprop="description">为什么计算机科学是无限的但生命是有限的</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">78</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">134</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ggaaooppeenngg" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ggaaooppeenngg" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:peng.gao.dut@gmail.com" title="E-Mail → mailto:peng.gao.dut@gmail.com" rel="noopener me" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Rust从零实现llama | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Rust从零实现llama
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-23 16:57:18" itemprop="dateCreated datePublished" datetime="2024-12-23T16:57:18+08:00">2024-12-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/23/Rust从零实现llama/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>参考Pytorch版本的<a target="_blank" rel="noopener" href="https://github.com/bkitano/llama-from-scratch">llama-from-scratch</a>。原文中的RmsNorm的平均值多算了一个维度，这里改成了正确的版本。</p>
<p>首先需要下载<a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt">TinyShakespeare</a>数据集，这是一个莎士比亚文字数据集。本文档将<em>大致</em>遵循论文的布局，并跳过一些明显的步骤，比如设置虚拟环境和安装依赖项。</p>
<p>我们最终将实现的内容预览：</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">println!</span>(<span class="title function_ invoke__">generate</span>(llama, MASTER_CONFIG, <span class="number">500</span>, device)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">ZELBETH:</span><br><span class="line">Sey solmenter! <span class="symbol">&#x27;tis</span> tonguerered <span class="keyword">if</span> berryishdd, and What his stabe, you, and, but all I pilJefals, mode with,</span><br><span class="line">Vurint <span class="keyword">as</span> steolated have loven OlD the queen<span class="symbol">&#x27;d</span> refore</span><br><span class="line">Are been, good plmp:</span><br><span class="line"></span><br><span class="line">Proforne, wift<span class="symbol">&#x27;es</span> swleen, was no bunderes<span class="symbol">&#x27;d</span> a a quain beath!</span><br><span class="line">Tybell is my gateer stalk smen<span class="symbol">&#x27;d</span> <span class="keyword">as</span> be matious dazest brink thou</span><br><span class="line">lord</span><br><span class="line">Enves were cIUll, afe and whwas seath This a is, an tale hoice his his onety Meall-tearn not murkawn, fase bettizen<span class="symbol">&#x27;d</span> her,</span><br><span class="line">To belacquesterer? baxewed wupl usweggs yet tall</span><br><span class="line">An</span><br></pre></td></tr></table></figure>

<p>实现过程中可能涉及一些 Rust 的使用方法，与 Python 有所不同，这里不做过多说明，具体的 Rust 语法可以参考其他文档。</p>
<h2 id="迭代工作：从小模块开始，保持确定性，然后逐步构建"><a href="#迭代工作：从小模块开始，保持确定性，然后逐步构建" class="headerlink" title="迭代工作：从小模块开始，保持确定性，然后逐步构建"></a>迭代工作：从小模块开始，保持确定性，然后逐步构建</h2><ol>
<li>创建所有需要的辅助函数，以便定量测试模型（数据拆分、训练、绘制损失）。</li>
<li>从论文中挑选出不同的组件，然后逐一实现，边训练边评估。</li>
</ol>
<h2 id="确保你的层按预期工作"><a href="#确保你的层按预期工作" class="headerlink" title="确保你的层按预期工作"></a>确保你的层按预期工作</h2><ol>
<li>经常使用 <code>.shape()</code>。<code>assert</code> 是你的朋友。</li>
<li>先在不进行矩阵乘法的情况下计算结果，然后使用 <code>candle</code> 函数使其高效。</li>
<li>有一个测试来确保你的层是正确的。例如，RoPE 嵌入有一个特定的属性，你可以测试它。对于 Transformer，你可以通过查看注意力矩阵来测试注意力是否正常工作。</li>
<li>在各种批次、序列和嵌入大小上测试你的层。即使它适用于一种大小，它可能不适用于其他大小，这将在推理时导致问题。</li>
</ol>
<h2 id="关于-Llama"><a href="#关于-Llama" class="headerlink" title="关于 Llama"></a>关于 Llama</h2><p>Llama 是一种基于 Transformer 的语言建模模型。它是一个自回归模型，也称为 CausalModel，模型会将输出中的 token 加入到输入中，不断迭代推理，直到超过上下文长度或遇到停止符。Meta AI <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">开源</a>了 Llama，并明确表示他们的目标是使模型在推理时更高效，而不是优化训练成本。</p>
<p>接下来，我们将加载库并开始实现。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> candle_core::&#123;DType, Device, IndexOp, <span class="type">Result</span>, Tensor, D&#125;;</span><br><span class="line"><span class="keyword">use</span> candle_nn::ops::softmax;</span><br><span class="line"><span class="keyword">use</span> candle_nn::&#123;</span><br><span class="line">    embedding, linear, loss, AdamW, Embedding, Init, Linear, Module, Optimizer, ParamsAdamW,</span><br><span class="line">    VarBuilder,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">use</span> core::<span class="type">f32</span>;</span><br><span class="line"><span class="keyword">use</span> rand::Rng;</span><br><span class="line"><span class="keyword">use</span> std::collections::HashMap;</span><br><span class="line"><span class="keyword">use</span> std::fs;</span><br><span class="line"><span class="keyword">use</span> std::time;</span><br></pre></td></tr></table></figure>

<h2 id="设置数据集"><a href="#设置数据集" class="headerlink" title="设置数据集"></a>设置数据集</h2><p>虽然 Llama 在 1.4T 个标记上进行训练，但我们的数据集 TinyShakespeare，即莎士比亚所有作品的集合，大约只有 100 万个字符。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> std::collections::HashMap;</span><br><span class="line"><span class="keyword">use</span> std::fs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="comment">// Read the entire content of the file</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">lines</span> = fs::<span class="title function_ invoke__">read_to_string</span>(<span class="string">&quot;./input.txt&quot;</span>)</span><br><span class="line">        .<span class="title function_ invoke__">expect</span>(<span class="string">&quot;Failed to read the file&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a sorted set of unique characters</span></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">vocab</span>: <span class="type">Vec</span>&lt;<span class="type">char</span>&gt; = lines.<span class="title function_ invoke__">chars</span>().<span class="title function_ invoke__">collect</span>();</span><br><span class="line">    vocab.<span class="title function_ invoke__">sort_unstable</span>();</span><br><span class="line">    vocab.<span class="title function_ invoke__">dedup</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create itos and stoi mappings</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">itos</span>: HashMap&lt;<span class="type">usize</span>, <span class="type">char</span>&gt; = vocab.<span class="title function_ invoke__">iter</span>().<span class="title function_ invoke__">enumerate</span>().<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (i, ch)).<span class="title function_ invoke__">collect</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">stoi</span>: HashMap&lt;<span class="type">char</span>, <span class="type">usize</span>&gt; = vocab.<span class="title function_ invoke__">iter</span>().<span class="title function_ invoke__">enumerate</span>().<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (ch, i)).<span class="title function_ invoke__">collect</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print the first 30 characters of the file</span></span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, &amp;lines[..<span class="number">30</span>.<span class="title function_ invoke__">min</span>(lines.<span class="title function_ invoke__">len</span>())]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<pre><code>First Citizen:
Before we proce
</code></pre>
<p>他们使用了<a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a>字节对编码分词器，但我们将只使用一个简单的字符级分词器。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> std::collections::HashMap;</span><br><span class="line"><span class="keyword">use</span> std::fs;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Vocab</span> &#123;</span><br><span class="line">    itos: HashMap&lt;<span class="type">u32</span>, <span class="type">char</span>&gt;,</span><br><span class="line">    stoi: HashMap&lt;<span class="type">char</span>, <span class="type">u32</span>&gt;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Vocab</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(itos: HashMap&lt;<span class="type">u32</span>, <span class="type">char</span>&gt;, stoi: HashMap&lt;<span class="type">char</span>, <span class="type">u32</span>&gt;) <span class="punctuation">-&gt;</span> Vocab &#123;</span><br><span class="line">        Vocab &#123; itos, stoi &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">decode</span>(&amp;<span class="keyword">self</span>, ids: &amp;[<span class="type">u32</span>]) <span class="punctuation">-&gt;</span> <span class="type">String</span> &#123;</span><br><span class="line">        ids.<span class="title function_ invoke__">iter</span>().<span class="title function_ invoke__">map</span>(|&amp;id| <span class="keyword">self</span>.itos[&amp;id]).<span class="title function_ invoke__">collect</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">encode</span>(&amp;<span class="keyword">self</span>, text: &amp;<span class="type">str</span>) <span class="punctuation">-&gt;</span> <span class="type">Vec</span>&lt;<span class="type">u32</span>&gt; &#123;</span><br><span class="line">        text.<span class="title function_ invoke__">chars</span>().<span class="title function_ invoke__">map</span>(|ch| <span class="keyword">self</span>.stoi[&amp;ch]).<span class="title function_ invoke__">collect</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">len</span>(&amp;<span class="keyword">self</span>) <span class="punctuation">-&gt;</span> <span class="type">usize</span> &#123;</span><br><span class="line">        <span class="keyword">self</span>.itos.<span class="title function_ invoke__">len</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">build</span>(lines: &amp;<span class="type">str</span>) <span class="punctuation">-&gt;</span> <span class="keyword">Self</span> &#123;</span><br><span class="line">        <span class="comment">// Create a sorted set of unique characters</span></span><br><span class="line">        <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">vocab</span>: <span class="type">Vec</span>&lt;<span class="type">char</span>&gt; = lines.<span class="title function_ invoke__">chars</span>().<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        vocab.<span class="title function_ invoke__">sort</span>();</span><br><span class="line">        vocab.<span class="title function_ invoke__">dedup</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Create itos and stoi mappings</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">itos</span>: HashMap&lt;<span class="type">u32</span>, <span class="type">char</span>&gt; = vocab</span><br><span class="line">            .<span class="title function_ invoke__">iter</span>()</span><br><span class="line">            .<span class="title function_ invoke__">enumerate</span>()</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (i <span class="keyword">as</span> <span class="type">u32</span>, ch))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">stoi</span>: HashMap&lt;<span class="type">char</span>, <span class="type">u32</span>&gt; = vocab</span><br><span class="line">            .<span class="title function_ invoke__">iter</span>()</span><br><span class="line">            .<span class="title function_ invoke__">enumerate</span>()</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (ch, i <span class="keyword">as</span> <span class="type">u32</span>))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">Self</span> &#123; itos, stoi &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="comment">// Read the entire content of the file</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">lines</span> = fs::<span class="title function_ invoke__">read_to_string</span>(<span class="string">&quot;./input.txt&quot;</span>).<span class="title function_ invoke__">expect</span>(<span class="string">&quot;Failed to read the file&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">vocab</span> = Vocab::<span class="title function_ invoke__">build</span>(&amp;lines);</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;vocab size = &#123;&#125;&quot;</span>, vocab.<span class="title function_ invoke__">len</span>());</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, vocab.<span class="title function_ invoke__">decode</span>(&amp;vocab.<span class="title function_ invoke__">encode</span>(<span class="string">&quot;hello&quot;</span>)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<pre><code>vocab size = 65
hello
</code></pre>
<p>由于数据集较小，我们无需担心内存存储问题。</p>
<p>我们创建了一个 <code>config</code> 对象来存储基本的模型参数。这样可以提高代码的可读性，并且便于修改配置。Rust 是强类型语言，因此所有变量都有明确的类型。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">modeConfig</span> = ModelConfig &#123;</span><br><span class="line">    vocab_size: vocab.<span class="title function_ invoke__">len</span>(),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable">dataset</span> = Tensor::<span class="title function_ invoke__">from_slice</span>(&amp;vocab.<span class="title function_ invoke__">encode</span>(&amp;lines), (lines.<span class="title function_ invoke__">len</span>(),), &amp;Device::Cpu).<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;:?&#125;&quot;</span>, dataset.<span class="title function_ invoke__">shape</span>());</span><br></pre></td></tr></table></figure>

<pre><code>[1115394]
</code></pre>
<p>让我们创建一个方法 <code>get_batches</code> 来生成训练数据和目标的批次。我们将使用相同的方法来生成验证和测试数据，通过 <code>split</code> 参数来控制。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">get_batches</span>(</span><br><span class="line">    dataset: &amp;Tensor,</span><br><span class="line">    split: &amp;<span class="type">str</span>,</span><br><span class="line">    batch_size: <span class="type">usize</span>,</span><br><span class="line">    context_length: <span class="type">usize</span>,</span><br><span class="line">) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, Tensor)&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">len_of_dataset</span> = dataset.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>).<span class="title function_ invoke__">unwrap</span>() <span class="keyword">as</span> <span class="type">f32</span>;</span><br><span class="line">    <span class="comment">// 按照 0.8 0.1 0.1 的比例切分训练集, 验证集和测试集</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">batch_data</span> = <span class="keyword">match</span> split &#123;</span><br><span class="line">        <span class="string">&quot;val&quot;</span> =&gt; &amp;dataset.<span class="title function_ invoke__">i</span>((<span class="number">0.8</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>..(<span class="number">0.9</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>)?,</span><br><span class="line">        <span class="string">&quot;test&quot;</span> =&gt; &amp;dataset.<span class="title function_ invoke__">i</span>((<span class="number">0.9</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>..)?,</span><br><span class="line">        _ =&gt; &amp;dataset.<span class="title function_ invoke__">i</span>(..(<span class="number">0.8</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>)?,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// 生成随机index</span></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">rng</span> = rand::<span class="title function_ invoke__">thread_rng</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">data_len</span> = batch_data.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">indices</span>: <span class="type">Vec</span>&lt;<span class="type">usize</span>&gt; = (<span class="number">0</span>..batch_size)</span><br><span class="line">        .<span class="title function_ invoke__">map</span>(|_| rng.<span class="title function_ invoke__">gen_range</span>(<span class="number">0</span>..data_len - context_length - <span class="number">1</span>))</span><br><span class="line">        .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">x_batches</span> = <span class="type">Vec</span>::<span class="title function_ invoke__">with_capacity</span>(batch_size);</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">y_batches</span> = <span class="type">Vec</span>::<span class="title function_ invoke__">with_capacity</span>(batch_size);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="variable">idx</span> <span class="keyword">in</span> indices &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = batch_data.<span class="title function_ invoke__">i</span>(idx..(idx + context_length))?;</span><br><span class="line">        <span class="comment">// y 是 x 后面的一个字符</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = batch_data.<span class="title function_ invoke__">i</span>((idx + <span class="number">1</span>)..(idx + context_length + <span class="number">1</span>))?;</span><br><span class="line">        x_batches.<span class="title function_ invoke__">push</span>(x);</span><br><span class="line">        y_batches.<span class="title function_ invoke__">push</span>(y);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// stack 和 cat 的区别是, stack 是在新的维度上堆叠, cat 是在已有的维度上堆叠</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x_tensor</span> = Tensor::<span class="title function_ invoke__">stack</span>(&amp;x_batches, <span class="number">0</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">y_tensor</span> = Tensor::<span class="title function_ invoke__">stack</span>(&amp;y_batches, <span class="number">0</span>)?;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>((x_tensor, y_tensor))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// in fn main</span></span><br><span class="line">modeConfig.context_length = <span class="number">16</span>;</span><br><span class="line">modeConfig.batch_size = <span class="number">8</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">batch</span> = <span class="title function_ invoke__">get_batches</span>(</span><br><span class="line">    &amp;dataset,</span><br><span class="line">    <span class="string">&quot;train&quot;</span>,</span><br><span class="line">    modeConfig.batch_size,</span><br><span class="line">    modeConfig.context_length,</span><br><span class="line">)?;</span><br><span class="line"><span class="built_in">println!</span>(</span><br><span class="line">    <span class="string">&quot;batch size &#123;&#125;, context_length &#123;&#125;&quot;</span>,</span><br><span class="line">    batch.<span class="number">0</span>.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?,</span><br><span class="line">    batch.<span class="number">0</span>.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?</span><br><span class="line">);</span><br><span class="line"><span class="keyword">for</span> <span class="variable">i</span> <span class="keyword">in</span> <span class="number">0</span>..modeConfig.batch_size &#123;</span><br><span class="line">    <span class="built_in">println!</span>(</span><br><span class="line">        <span class="string">&quot;&#123;:?&#125;, &#123;:?&#125;&quot;</span>,</span><br><span class="line">        vocab.<span class="title function_ invoke__">decode</span>(&amp;batch.<span class="number">0</span>.<span class="title function_ invoke__">i</span>(i)?.<span class="title function_ invoke__">to_vec1</span>()?),</span><br><span class="line">        vocab.<span class="title function_ invoke__">decode</span>(&amp;batch.<span class="number">1</span>.<span class="title function_ invoke__">i</span>(i)?.<span class="title function_ invoke__">to_vec1</span>()?),</span><br><span class="line">    );</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&quot;:\nBut, that I&#39;ll&quot;, &quot;\nBut, that I&#39;ll &quot;
&quot;ng?\nWhy, then th&quot;, &quot;g?\nWhy, then the&quot;
&quot;s so blind, but &quot;, &quot; so blind, but s&quot;
&quot;thy offices,\nSo &quot;, &quot;hy offices,\nSo r&quot;
&quot;ords, how plainl&quot;, &quot;rds, how plainly&quot;
&quot;IET:\nHere&#39;s such&quot;, &quot;ET:\nHere&#39;s such &quot;
&quot;wer\nTo take off &quot;, &quot;er\nTo take off s&quot;
&quot; hurry from the &quot;, &quot;hurry from the f&quot;
</code></pre>
<p>实现论文有趣的一点在于，模型“工作”有两个方面：编译（你的张量是否在各层之间匹配）和训练（损失是否下降）。<br>我们还要定义评估模型的方法。我们希望在定义模型之前就这样做，因为我们希望在训练模型时使用它来评估模型的性能。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">evaluate_loss</span>(</span><br><span class="line">    model: &amp;SimpleBrokenModel,</span><br><span class="line">    dataset: &amp;Tensor,</span><br><span class="line">    vocab: &amp;Vocab,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;HashMap&lt;<span class="type">String</span>, <span class="type">f32</span>&gt;&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">out</span> = HashMap::<span class="title function_ invoke__">new</span>();</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">split</span> <span class="keyword">in</span> [<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>] &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">losses</span> = <span class="type">Vec</span>::<span class="title function_ invoke__">new</span>();</span><br><span class="line">        <span class="keyword">for</span> <span class="variable">_</span> <span class="keyword">in</span> <span class="number">0</span>..<span class="number">10</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> (xs, ys) = <span class="title function_ invoke__">get_batches</span>(&amp;dataset, split, config.batch_size, config.context_length)?;</span><br><span class="line">            <span class="keyword">let</span> (_, loss) = model.forward(&amp;xs, <span class="title function_ invoke__">Some</span>(&amp;ys))?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss.<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line">            losses.<span class="title function_ invoke__">push</span>(loss.to_scalar::&lt;<span class="type">f32</span>&gt;()?);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">avg_loss</span> = losses.<span class="title function_ invoke__">iter</span>().sum::&lt;<span class="type">f32</span>&gt;() / losses.<span class="title function_ invoke__">len</span>() <span class="keyword">as</span> <span class="type">f32</span>;</span><br><span class="line">        out.<span class="title function_ invoke__">insert</span>(split.<span class="title function_ invoke__">to_owned</span>(), avg_loss);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(out)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="设置一个可以工作的简单模型"><a href="#设置一个可以工作的简单模型" class="headerlink" title="设置一个可以工作的简单模型"></a>设置一个可以工作的简单模型</h2><p>这是一个带有嵌入的基本前馈神经网络。它是我们将要开始的基础模型，然后我们将逐步替换其部分内容，直到最终得到 Llama 论文中描述的模型。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="comment">// 潜入层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 线性和激活层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;embeds)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果提供了targets就计算loss，不然视为推理，计算logits就可以。</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="comment">// 负的似然函数</span></span><br><span class="line">            <span class="comment">// -log(x) 越大，loss 越小</span></span><br><span class="line">            <span class="comment">// y =  [0, 0 , 0, 0, 1, ...,0,0]</span></span><br><span class="line">            <span class="comment">// y&#x27; = [4, 5,  6, 7, 8, ...,11,12 ]</span></span><br><span class="line">            <span class="comment">// 这个 cross_entropy 帮我们做了一个 log softmax</span></span><br><span class="line">            <span class="comment">// y&#x27; = [0.1, 0.12, 0.13, 0.64, ..., 0,0]</span></span><br><span class="line">            <span class="comment">// loss = -log(0.64)</span></span><br><span class="line">            <span class="comment">// 当 -log(q) = 4.17 q = 0.015 大概 1/64,vocab_size = 65,所以基本是在瞎猜。</span></span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// VarBuilder是用来构建参数的，我们目前不加载和保存模型参数，但是candle的用法必须基于这个。</span></span><br><span class="line">    <span class="comment">// vb.pp 会在参数树中加入参数的前缀，这样可以方便的查看参数的结构。</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// in fn main</span></span><br><span class="line">modeConfig.d_model = <span class="number">128</span>;</span><br><span class="line">modeConfig.batch_size = <span class="number">32</span>;</span><br><span class="line"><span class="keyword">let</span> (xs, ys) = <span class="title function_ invoke__">get_batches</span>(</span><br><span class="line">    &amp;dataset,</span><br><span class="line">    <span class="string">&quot;train&quot;</span>,</span><br><span class="line">    modeConfig.batch_size,</span><br><span class="line">    modeConfig.context_length,</span><br><span class="line">)?;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">varmap</span> = candle_nn::VarMap::<span class="title function_ invoke__">new</span>();</span><br><span class="line"><span class="keyword">let</span> <span class="variable">vb</span> = candle_nn::VarBuilder::<span class="title function_ invoke__">from_varmap</span>(&amp;varmap, DType::F32, &amp;Device::Cpu);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">model</span> = SimpleBrokenModel::<span class="title function_ invoke__">load</span>(vb, modeConfig)?;</span><br><span class="line"><span class="keyword">let</span> (logits, loss) = model.forward(&amp;xs, <span class="title function_ invoke__">Some</span>(&amp;ys))?;</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;:?&#125; &#123;:?&#125;&quot;</span>, logits, loss);</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">params_count</span>: <span class="type">usize</span> = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (name, var) <span class="keyword">in</span> varmap.<span class="title function_ invoke__">data</span>().<span class="title function_ invoke__">lock</span>().<span class="title function_ invoke__">unwrap</span>().<span class="title function_ invoke__">iter</span>() &#123;</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;: &#123;:?&#125;&quot;</span>, name, var.<span class="title function_ invoke__">elem_count</span>());</span><br><span class="line">    params_count += var.<span class="title function_ invoke__">elem_count</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;params count: &#123;&#125;&quot;</span>, params_count);</span><br></pre></td></tr></table></figure>

<pre><code>Tensor[dims 32, 16, 65; f32] Some(Tensor[5.266067; f32])
model.fc2.weight: 8320
model.embed_tokens.weight: 8320
model.fc1.bias: 128
model.fc2.bias: 65
model.fc1.weight: 16384
params count: 33217
</code></pre>
<p>在这一点上，我们必须开始关注张量的形状，并让矩阵的维度匹配。查看我们模型定义中的这一行：</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">    &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">    &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">)?;</span><br></pre></td></tr></table></figure>

<p>我们必须调整 <code>logits</code> 和 <code>targets</code> 张量的形状，以便在比较时它们的维度匹配。我们使用 <code>reshape</code> 方法来实现这一点。<br><code>()</code> 参数的意思是“从其他维度推断这个维度”。所以，在这种情况下，我们是在说“将 <code>logits</code> 和 <code>targets</code> 重新调整为具有相同行数的形状，并使用所需的列数来实现这一点”。这是处理批量数据时的常见模式。</p>
<p>让我们训练我们的 <code>SimpleBrokenModel</code> 以确保梯度流动。在确认这一点之后，我们可以替换它的部分内容以匹配 Llama，再次训练并跟踪我们的进展。在这一点上，我开始记录我的训练运行日志，这样如果我搞砸了，我可以轻松地回到之前的运行。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">train</span>(</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">    model: &amp;SimpleBrokenModel,</span><br><span class="line">    opt: &amp;<span class="keyword">mut</span> AdamW,</span><br><span class="line">    dataset: &amp;Tensor,</span><br><span class="line">    vocab: &amp;Vocab,</span><br><span class="line">) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;()&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">start_time</span> = std::time::Instant::<span class="title function_ invoke__">now</span>();</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">epoch</span> <span class="keyword">in</span> <span class="number">0</span>..config.epochs &#123;</span><br><span class="line">        <span class="keyword">let</span> (xs, ys) = <span class="title function_ invoke__">get_batches</span>(&amp;dataset, <span class="string">&quot;train&quot;</span>, config.batch_size, config.context_length)?;</span><br><span class="line">        <span class="keyword">let</span> (_, loss) = model.forward(&amp;xs, <span class="title function_ invoke__">Some</span>(&amp;ys))?;</span><br><span class="line">        opt.<span class="title function_ invoke__">backward_step</span>(&amp;loss.<span class="title function_ invoke__">unwrap</span>())?;</span><br><span class="line">        <span class="keyword">if</span> epoch % config.log_interval == <span class="number">0</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">batch_duration</span> = start_time.<span class="title function_ invoke__">elapsed</span>().<span class="title function_ invoke__">as_secs_f32</span>();</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = <span class="title function_ invoke__">evaluate_loss</span>(&amp;model, dataset, vocab, config)?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">val_loss</span> = loss.<span class="title function_ invoke__">get</span>(<span class="string">&quot;val&quot;</span>).<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">eta</span> = batch_duration * (config.epochs - epoch) <span class="keyword">as</span> <span class="type">f32</span>;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">eta</span> = eta.<span class="title function_ invoke__">round</span>();</span><br><span class="line">            <span class="built_in">println!</span>(</span><br><span class="line">                <span class="string">&quot;Epoch: &#123;epoch&#125; | Loss: &#123;val_loss&#125; | Time: &#123;batch_duration&#125; | ETA in seconds &#123;eta&#125;&quot;</span></span><br><span class="line">            );</span><br><span class="line">            start_time = time::Instant::<span class="title function_ invoke__">now</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(())</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// in fn main</span></span><br><span class="line">modeConfig.log_interval = <span class="number">10</span>;</span><br><span class="line">modeConfig.epochs = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">opt</span> = candle_nn::AdamW::<span class="title function_ invoke__">new</span>(varmap.<span class="title function_ invoke__">all_vars</span>(), ParamsAdamW::<span class="title function_ invoke__">default</span>())?;</span><br><span class="line"><span class="title function_ invoke__">train</span>(modeConfig, &amp;model, &amp;<span class="keyword">mut</span> opt, &amp;dataset, &amp;vocab)?;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">out</span> = <span class="title function_ invoke__">evaluate_loss</span>(&amp;model, &amp;dataset, &amp;vocab, modeConfig);</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;:?&#125;&quot;</span>, out);</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 10 | Loss: 3.9159875 | Time: 6.5813394 | ETA in seconds 599
Epoch: 20 | Loss: 3.26492 | Time: 6.3639965 | ETA in seconds 515
Epoch: 30 | Loss: 2.9944448 | Time: 6.3596206 | ETA in seconds 452
Epoch: 40 | Loss: 2.8793342 | Time: 6.357106 | ETA in seconds 388
Epoch: 50 | Loss: 2.7827232 | Time: 6.3562865 | ETA in seconds 324
Epoch: 60 | Loss: 2.764416 | Time: 6.352279 | ETA in seconds 260
Epoch: 70 | Loss: 2.7196321 | Time: 6.356127 | ETA in seconds 197
Epoch: 80 | Loss: 2.7631993 | Time: 6.357493 | ETA in seconds 134
Epoch: 90 | Loss: 2.696882 | Time: 6.358631 | ETA in seconds 70
Epoch: 100 | Loss: 2.670012 | Time: 6.3603354 | ETA in seconds 6
Ok(&#123;&quot;train&quot;: 2.591057, &quot;val&quot;: 2.6625311&#125;)
</code></pre>
<!-- 注意我们得到的训练曲线几乎没有下降。我们怎么知道它几乎没有训练？
我们必须使用基本原理。训练前的交叉熵损失是4.17，1000个epoch后的损失是3.93。我们如何直观地理解这一点？ -->

<!-- 在这种情况下，交叉熵指的是我们选择错误单词的可能性。所以这里，

$$
H(T, q) = - \sum_{i = 1}^{N} \frac{1}{N} \log q(x_i)
$$

其中 $q(x_i)$ 是模型估计的选择正确单词的概率。如果 $q(x_i)$ 接近1，那么 $\log q$ 接近0；同样，如果 $q$ 很小，那么 $\log q$ 是一个大的负数，
所以 $-\log q$ 将是一个大的正数。现在建立直觉：开始时，$-\log q = 4.17$，所以 $q = 0.015$，大约是 $\frac{1}{64.715}$。
回想一下，词汇量 $|V| = 65$，所以我们基本上在说模型在选择下一个字母时和从我们的词汇表中随机选择一样好。
训练后，$-\log q = 3.93$，所以我们现在基本上在50个字母之间选择。这是一个非常小的改进，所以可能有问题。

为了直观地理解损失与模型性能的关系，想象模型在 $\tilde V$ 个标记之间选择；当 $\tilde V$ 很小时，模型更有可能猜对。此外，我们知道 $\max \tilde V = V$，这可以帮助我们理解我们的模型是否在学习。

$$\tilde V = \exp(L)$$

让我们尝试调试发生了什么。注意在我们的模型中，我们在logits上使用了softmax层，这是一个将数字向量压缩成概率分布的函数。但是对于使用内置的 `F.cross_entropy` 函数，我们需要直接传递[未归一化的logits](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)。所以让我们从模型中删除它并再次尝试。 -->
<!-- 
很好，现在我们的损失是 $2.54$，所以我们从 $12.67$ 个字符中进行选择。这比我们开始时的 65 个字符要好得多。让我们为我们的模型添加一个生成方法，这样我们就可以直观地看到模型的结果。 -->


<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">generate</span>(model: &amp;SimpleBrokenModel, vocab: &amp;Vocab, max_tokens: <span class="type">usize</span>) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;()&gt; &#123;</span><br><span class="line">    <span class="comment">// batch size 5, initial token = 0</span></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">token_ids</span> = Tensor::<span class="title function_ invoke__">zeros</span>((<span class="number">5</span>, <span class="number">1</span>), DType::U32, &amp;Device::Cpu).<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">_</span> <span class="keyword">in</span> <span class="number">0</span>..max_tokens &#123;</span><br><span class="line">        <span class="keyword">let</span> (logits, _) = model.forward(&amp;token_ids, <span class="literal">None</span>)?;</span><br><span class="line">        <span class="built_in">assert!</span>(logits.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?, <span class="number">65</span>]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">last_step_logits</span> = logits.<span class="title function_ invoke__">i</span>((.., logits.<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)? - <span class="number">1</span>))?;</span><br><span class="line">        <span class="built_in">assert!</span>(last_step_logits.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, <span class="number">65</span>]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">probs</span> = <span class="title function_ invoke__">softmax</span>(&amp;last_step_logits, last_step_logits.<span class="title function_ invoke__">dims</span>().<span class="title function_ invoke__">len</span>() - <span class="number">1</span>)?;</span><br><span class="line">        <span class="built_in">assert!</span>(probs.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, <span class="number">65</span>]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">next_token</span> = probs.<span class="title function_ invoke__">argmax</span>(probs.<span class="title function_ invoke__">dims</span>().<span class="title function_ invoke__">len</span>() - <span class="number">1</span>)?;</span><br><span class="line">        <span class="built_in">assert!</span>(next_token.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?]);</span><br><span class="line">        token_ids = Tensor::<span class="title function_ invoke__">cat</span>(&amp;[token_ids, next_token.<span class="title function_ invoke__">reshape</span>(((), <span class="number">1</span>))?], <span class="number">1</span>)?;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">lines</span> = fs::<span class="title function_ invoke__">read_to_string</span>(<span class="string">&quot;./input.txt&quot;</span>).<span class="title function_ invoke__">expect</span>(<span class="string">&quot;Failed to read the file&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">v</span> <span class="keyword">in</span> &amp;token_ids.<span class="title function_ invoke__">to_vec2</span>()? &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">text</span> = vocab.<span class="title function_ invoke__">decode</span>(v);</span><br><span class="line">        <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, text);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(())</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// fn in main</span></span><br><span class="line"><span class="title function_ invoke__">generate</span>(&amp;model, &amp;vocab, <span class="number">10</span>, device)?;</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;\nFind!\nD:\nAr t,\nLis sthte o t l&#39;,
 &#39;\nAnd ronnot ar\nBE:\nKINRDYOrspr;&#39;,
 &#39;\nI t athe momyengthend thanswal&#39;,
 &#39;\nFis t bp he\nLacarn.\nA:\nYOMI wi&#39;,
 &#39;\nWh ly sck\nB-de pll t\nHERIns ou&#39;]
</code></pre>
<p>这还算不错，但也不算太好。不过现在我们有了一个可以训练到验证损失的工作模型。因此，我们将在此基础上迭代我们的模型，使其更接近 Llama。</p>
<h2 id="Llama-具体细节"><a href="#Llama-具体细节" class="headerlink" title="Llama 具体细节"></a>Llama 具体细节</h2><p>Llama 对原始 Transformer 进行了三项架构修改：</p>
<ol>
<li>用于预归一化的 RMSNorm</li>
<li>旋转嵌入 RoPE</li>
<li>SwiGLU 激活函数</li>
</ol>
<p>我们将逐一添加每个修改到我们的基础模型，并进行迭代。</p>
<h3 id="RMSNorm"><a href="#RMSNorm" class="headerlink" title="RMSNorm"></a>RMSNorm</h3><p>在 Vaswani 2017 中，原始的 Transformer 使用了 BatchNormalization。在 Llama 中，作者使用了 RMSNorm，这是一种在不进行中心化的情况下通过方差来缩放向量的方法。此外，虽然 Vaswani 将归一化应用于注意力层的输出（后归一化），但 Llama 将其应用于输入之前（前归一化）。</p>
<p><a target="_blank" rel="noopener" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials/rmsnorm.html">这篇文章</a>对于RMSNorm有一个很好的解释。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pub</span> <span class="keyword">struct</span> <span class="title class_">RmsNorm</span> &#123;</span><br><span class="line">    scale: Tensor,</span><br><span class="line">    eps: <span class="type">f64</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">RmsNorm</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(size: <span class="type">usize</span>, vb: VarBuilder) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(RmsNorm &#123;</span><br><span class="line">            scale: vb.<span class="title function_ invoke__">get_with_hints</span>(size, <span class="string">&quot;weight&quot;</span>, Init::<span class="title function_ invoke__">Const</span>(<span class="number">1</span>.))?,</span><br><span class="line">            eps: <span class="number">1e-6</span>,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">pub</span> <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x_sqr</span> = x.<span class="title function_ invoke__">sqr</span>()?;</span><br><span class="line">        <span class="built_in">assert!</span>(x_sqr.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>());</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">norm_x</span> = (x.<span class="title function_ invoke__">mean</span>(D::Minus1)? + <span class="keyword">self</span>.eps)?.<span class="title function_ invoke__">sqrt</span>()?;</span><br><span class="line">        <span class="built_in">assert!</span>(norm_x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x_normed</span> = x.<span class="title function_ invoke__">broadcast_div</span>(&amp;norm_x.<span class="title function_ invoke__">reshape</span>((</span><br><span class="line">            norm_x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?,</span><br><span class="line">            norm_x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?,</span><br><span class="line">            (),</span><br><span class="line">        ))?)?;</span><br><span class="line">        <span class="built_in">assert!</span>(x_normed.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>());</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (x_normed.<span class="title function_ invoke__">broadcast_mul</span>(&amp;<span class="keyword">self</span>.scale))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="variable">varmap</span> = candle_nn::VarMap::<span class="title function_ invoke__">new</span>();</span><br><span class="line"><span class="keyword">let</span> <span class="variable">vb</span> = candle_nn::VarBuilder::<span class="title function_ invoke__">from_varmap</span>(&amp;varmap, DType::F32, &amp;Device::Cpu);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="variable">rms_norms</span> = RmsNorm::<span class="title function_ invoke__">new</span>(<span class="number">2</span>, vb)?;</span><br><span class="line"><span class="comment">// (2,3,2)</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable">batch</span> = Tensor::<span class="title function_ invoke__">new</span>(</span><br><span class="line">    <span class="built_in">vec!</span>[</span><br><span class="line">        <span class="built_in">vec!</span>[<span class="built_in">vec!</span>[<span class="number">1f32</span>, <span class="number">1f32</span>], <span class="built_in">vec!</span>[<span class="number">1.2f32</span>, <span class="number">2f32</span>], <span class="built_in">vec!</span>[<span class="number">3f32</span>, <span class="number">3f32</span>]],</span><br><span class="line">        <span class="built_in">vec!</span>[<span class="built_in">vec!</span>[<span class="number">4f32</span>, <span class="number">43f32</span>], <span class="built_in">vec!</span>[<span class="number">5f32</span>, <span class="number">5f32</span>], <span class="built_in">vec!</span>[<span class="number">61f32</span>, <span class="number">6f32</span>]],</span><br><span class="line">    ],</span><br><span class="line">    &amp;Device::Cpu,</span><br><span class="line">)?;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">vb</span> = candle_nn::VarBuilder::<span class="title function_ invoke__">from_varmap</span>(&amp;varmap, DType::F32, &amp;Device::Cpu);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">out</span> = rms_norms.forward(&amp;batch)?;</span><br></pre></td></tr></table></figure>

<pre><code>Tensor[dims 2, 3, 2; f32]
</code></pre>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    rms_norm: RmsNorm,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="comment">// Embedding</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line">        <span class="comment">// RMSNorm</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">normed_embeds</span> = <span class="keyword">self</span>.rms_norm.forward(&amp;embeds)?;</span><br><span class="line">        <span class="comment">// Linear layers and activation</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;normed_embeds)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Calculate loss if targets are provided</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="comment">// 负的似然函数</span></span><br><span class="line">            <span class="comment">// log(x) 越大，loss 越小</span></span><br><span class="line">            <span class="comment">// y =  [0, 0 , 0, 0, 1, ...,0,0]</span></span><br><span class="line">            <span class="comment">// y&#x27; = [4, 5,  6, 7, 8, ...,11,12 ]</span></span><br><span class="line">            <span class="comment">// 这个 cross_entropy 帮我们做了一个 log softmax</span></span><br><span class="line">            <span class="comment">// y&#x27; = [0.1, 0.12, 0.13, 0.64, ..., 0,0]</span></span><br><span class="line">            <span class="comment">// loss = -log(0.64)</span></span><br><span class="line">            <span class="comment">// -log(q) = 4.17 q = 0.015 大概 1/64,vocab_size = 65,所以基本是在瞎猜。</span></span><br><span class="line">            <span class="comment">// println!(&quot;&#123;:?&#125;&quot;, targets.shape());</span></span><br><span class="line">            <span class="comment">// println!(&quot;&#123;:?&#125;&quot;, logits.shape());</span></span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// VarBuilder是用来构建参数的。</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_norm</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.rms_norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">            rms_norm,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 10 | Loss: 4.1559505 | Time: 6.779387 | ETA in seconds 617
Epoch: 20 | Loss: 4.14648 | Time: 6.7727704 | ETA in seconds 549
Epoch: 30 | Loss: 4.1364665 | Time: 6.776428 | ETA in seconds 481
Epoch: 40 | Loss: 4.125594 | Time: 6.772582 | ETA in seconds 413
Epoch: 50 | Loss: 4.120083 | Time: 6.7661977 | ETA in seconds 345
Epoch: 60 | Loss: 4.1099877 | Time: 6.760399 | ETA in seconds 277
Epoch: 70 | Loss: 4.0996284 | Time: 6.7623253 | ETA in seconds 210
Epoch: 80 | Loss: 4.0902996 | Time: 6.761824 | ETA in seconds 142
Epoch: 90 | Loss: 4.0833025 | Time: 6.76845 | ETA in seconds 74
Epoch: 100 | Loss: 4.070025 | Time: 6.7624454 | ETA in seconds 7
Ok(&#123;&quot;train&quot;: 4.072861, &quot;val&quot;: 4.0711236&#125;)
</code></pre>
<p>从这里得到的结果来看，范化以后，模型的表现并没有提升，所以我们需要继续迭代，只是梯度的下降变得比较平滑了。</p>
<h3 id="Rotary-Embeddings"><a href="#Rotary-Embeddings" class="headerlink" title="Rotary Embeddings"></a>Rotary Embeddings</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864.pdf">RoPE</a> 是一种用于 Transformer 的位置编码方法。在《Attention is All You Need》中，作者提出了两种位置编码方法：学习的和固定的。在 RoPE 中，作者通过旋转嵌入来表示序列中标记的位置，并在每个位置使用不同的旋转角度。</p>
<p>其中的 cos 和 sin 值可以预先计算并缓存，避免重复计算，后续会统一存放在一个缓存结构中。</p>
<p>RoPE 将 <code>hidden_state</code> 中每两个 x 组成的向量与旋转矩阵相乘来实现位置编码。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[x0, x1, .... ,xn]</span><br><span class="line">y0 = x0 * <span class="title function_ invoke__">cos</span>(theta) - x1 * <span class="title function_ invoke__">sin</span>(theta)</span><br><span class="line">y1 = x0 * <span class="title function_ invoke__">sin</span>(theta) + x1 * <span class="title function_ invoke__">cos</span>(theta)</span><br><span class="line">[y0, y1, ...., yn]</span><br></pre></td></tr></table></figure>
<p><code>n</code> 是 <code>d_model</code> 的一半。<br><code>theta</code> 是一个根据位置得到的固定值，计算公式为：<br><code>theta = m / 10000^(2i/n)</code><br>其中，<code>m</code> 是在序列中的位置，<code>i</code> 是在 <code>d_model</code> 中的位置。</p>
<p>这个公式的含义是将特征向量中的 <code>x0</code> 和 <code>x1</code> 进行一个固定的旋转，这个旋转不是通过学习得到的，而是预先计算的。它可以用于表示相对位置信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">```Rust</span><br><span class="line">pos_index = 0, 中的 x0,x1</span><br><span class="line">pos_index = 1, 中的 x0,x1</span><br></pre></td></tr></table></figure>
<p>隔了一个恒定的调度旋转。</p>
<p>freq_cis缓存住提前算好的cos和sin的值，这部分不用重复计算。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    cos: Tensor,</span><br><span class="line">    sin: Tensor,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(context_length: <span class="type">usize</span>, n_elem: <span class="type">usize</span>, vb: VarBuilder) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Cache&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">theta</span>: <span class="type">Vec</span>&lt;_&gt; = (<span class="number">0</span>..n_elem)</span><br><span class="line">            .<span class="title function_ invoke__">step_by</span>(<span class="number">2</span>)</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|i| <span class="number">1f32</span> / <span class="number">10000f32</span>.<span class="title function_ invoke__">powf</span>(i <span class="keyword">as</span> <span class="type">f32</span> / n_elem <span class="keyword">as</span> <span class="type">f32</span>))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">theta</span> = Tensor::<span class="title function_ invoke__">new</span>(theta.<span class="title function_ invoke__">as_slice</span>(), vb.<span class="title function_ invoke__">device</span>())?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">idx_theta</span> = Tensor::<span class="title function_ invoke__">arange</span>(<span class="number">0</span>, context_length <span class="keyword">as</span> <span class="type">u32</span>, vb.<span class="title function_ invoke__">device</span>())?</span><br><span class="line">            .<span class="title function_ invoke__">to_dtype</span>(DType::F32)?</span><br><span class="line">            .<span class="title function_ invoke__">reshape</span>((context_length, <span class="number">1</span>))?</span><br><span class="line">            .<span class="title function_ invoke__">matmul</span>(&amp;theta.<span class="title function_ invoke__">reshape</span>((<span class="number">1</span>, theta.<span class="title function_ invoke__">elem_count</span>()))?)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">freq_cis_real</span> = idx_theta.<span class="title function_ invoke__">cos</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">freq_cis_imag</span> = idx_theta.<span class="title function_ invoke__">sin</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = freq_cis_real.<span class="title function_ invoke__">reshape</span>((context_length, n_elem / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = freq_cis_imag.<span class="title function_ invoke__">reshape</span>((context_length, n_elem / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(Cache &#123; cos, sin&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>rope计算的时候</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">apply_rotary_emb</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> (b_sz, seq_len, n_embd) = x.<span class="title function_ invoke__">dims3</span>()?;</span><br><span class="line">    <span class="comment">// println!(&quot;shape of cache.cos &#123;:?&#125;&quot;, cache.cos.shape());</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">cos</span> = cache.cos.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">sin</span> = cache.sin.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">    <span class="comment">// println!(&quot;cos shape &#123;:?&#125;&quot;, cos.shape());</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">cos</span> = cos.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">sin</span> = sin.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">    <span class="comment">// println!(&quot;broadcast cos shape &#123;:?&#125;&quot;, cos.shape());</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x</span> = x.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_embd / <span class="number">2</span>, <span class="number">2</span>))?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x0</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">0</span>, <span class="number">1</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x1</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">1</span>, <span class="number">1</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">dst0</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)? - x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)?)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">dst1</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)? + x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)?)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">rope</span> = Tensor::<span class="title function_ invoke__">cat</span>(&amp;[&amp;dst0, &amp;dst1], D::Minus1)?.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_embd))?;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(rope)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h3><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    rms_norm: RmsNorm,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">    self_attention: SelfAttention,</span><br><span class="line">    cache: Cache,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="comment">// 嵌入层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line">        <span class="comment">// 范化层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">normed_embeds</span> = <span class="keyword">self</span>.rms_norm.forward(&amp;embeds)?;</span><br><span class="line">        <span class="comment">// 自注意力层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.self_attention.forward(&amp;normed_embeds, &amp;<span class="keyword">self</span>.cache)?;</span><br><span class="line">        <span class="comment">// 线性和激活层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;normed_embeds)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="comment">// 负的似然函数</span></span><br><span class="line">            <span class="comment">// -log(x) 越大，loss 越小</span></span><br><span class="line">            <span class="comment">// y =  [0, 0 , 0, 0, 1, ...,0,0]</span></span><br><span class="line">            <span class="comment">// y&#x27; = [4, 5,  6, 7, 8, ...,11,12 ]</span></span><br><span class="line">            <span class="comment">// 这个 cross_entropy 帮我们做了一个 log softmax</span></span><br><span class="line">            <span class="comment">// y&#x27; = [0.1, 0.12, 0.13, 0.64, ..., 0,0]</span></span><br><span class="line">            <span class="comment">// loss = -log(0.64)</span></span><br><span class="line">            <span class="comment">// 例如 -log(q) = 4.17 q = 0.015 大概 1/64,vocab_size = 65,所以基本是在瞎猜。</span></span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_norm</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.rms_norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">self_attention</span> = SelfAttention::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.self_attention&quot;</span>), config)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">            rms_norm,</span><br><span class="line">            self_attention,</span><br><span class="line">            cache: Cache::<span class="title function_ invoke__">new</span>(config.context_length, config.d_model, vb)?,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：了解训练时张量维度与推理时张量维度的区别。</p>
</blockquote>
<p>虽然在训练时，你可以期望张量维度与模型参数紧密匹配，例如 <code>batch.shape = (config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;])</code>，但在推理时，你可能需要处理单个示例，例如 <code>batch.shape = (1, 1, config[&#39;d_model&#39;])</code>。因此，你需要确保在 <code>forward</code> 传递中进行索引时，使用从输入派生的形状，而不一定是模型参数。</p>
<h3 id="MultiHeadRopeAttention"><a href="#MultiHeadRopeAttention" class="headerlink" title="MultiHeadRopeAttention"></a>MultiHeadRopeAttention</h3><p>让我们为这个单一的注意力头设置一个多头注意力层，看看训练时会发生什么。</p>
<p>这里实现的是GQA的注意力头。<code>n_kv_head=1</code>时就是MQA，<code>n_kv_head&gt;1</code>且<code>n_kv_head&lt;n_head</code>时就是GQA，<code>n_kv_head=n_head</code>时就是原本的MHA。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    q_proj: Linear,</span><br><span class="line">    k_proj: Linear,</span><br><span class="line">    v_proj: Linear,</span><br><span class="line">    o_proj: Linear,</span><br><span class="line">    n_head: <span class="type">usize</span>,</span><br><span class="line">    n_kv_head: <span class="type">usize</span>,</span><br><span class="line">    head_dim: <span class="type">usize</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.q_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.k_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.v_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">o_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.o_proj&quot;</span>))?;</span><br><span class="line">        <span class="built_in">println!</span>(</span><br><span class="line">            <span class="string">&quot;MHA config n_head &#123;&#125; n_kv_head &#123;&#125;&quot;</span>,</span><br><span class="line">            config.n_head, config.n_kv_head</span><br><span class="line">        );</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            q_proj,</span><br><span class="line">            k_proj,</span><br><span class="line">            v_proj,</span><br><span class="line">            o_proj,</span><br><span class="line">            n_head: config.n_head,</span><br><span class="line">            n_kv_head: config.n_kv_head,</span><br><span class="line">            head_dim: config.d_model / config.n_head,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">apply_rotary_emb</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (b_sz, seq_len, h, n_embd) = x.<span class="title function_ invoke__">dims4</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = cache.cos.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = cache.sin.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = cos.<span class="title function_ invoke__">unsqueeze</span>(<span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = sin.<span class="title function_ invoke__">unsqueeze</span>(<span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = cos.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, <span class="number">1</span>, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = sin.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, <span class="number">1</span>, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = x.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, h, n_embd / <span class="number">2</span>, <span class="number">2</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x0</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">0</span>, <span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x1</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">1</span>, <span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">dst0</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)? - x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)?)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">dst1</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)? + x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)?)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rope</span> = Tensor::<span class="title function_ invoke__">cat</span>(&amp;[&amp;dst0, &amp;dst1], D::Minus1)?.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, h, n_embd))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(rope)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (b_sz, seq_len, n_embd) = x.<span class="title function_ invoke__">dims3</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算 q k v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.q_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.k_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.v_proj.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">assert!</span>(n_embd == <span class="keyword">self</span>.n_head * <span class="keyword">self</span>.head_dim);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对  q  和 k 做位置编码</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;q, cache)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;k, cache)?;</span><br><span class="line">        <span class="comment">// 复制成 n_head / n_kv_head 份</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(k)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(v)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 把 seq_len 和 n_head 交换</span></span><br><span class="line">        <span class="comment">// 这转换一下是为了做一个 cat single head 的简单操作</span></span><br><span class="line">        <span class="comment">// 相当于 n_head 个的seq_len*seq_len的注意力。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// q*k^T / sqrt(d_k) d_k = d_model</span></span><br><span class="line">        <span class="comment">// 这里是 (bs,n_head) 个 (seq_len, seq_len) 的注意力</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = (q.<span class="title function_ invoke__">matmul</span>(&amp;k.<span class="title function_ invoke__">t</span>()?)? / (<span class="keyword">self</span>.head_dim <span class="keyword">as</span> <span class="type">f64</span>).<span class="title function_ invoke__">sqrt</span>())?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里是头内的softmax (seq_len,seq_len)的行总和为1</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = <span class="title function_ invoke__">softmax</span>(&amp;attn, D::Minus1)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 再乘 * (bs, n_head, seq_len, head_dim) 得到 (bs, n_head）个注意力头对应的加权的v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = attn.<span class="title function_ invoke__">matmul</span>(&amp;v)?;</span><br><span class="line">        <span class="comment">// 把 n_head 和 seq_len 交换回来，得到 (bs, seq_len, n_head, head_dim) 然后reshape以后</span></span><br><span class="line">        <span class="comment">// 得到 (bs, seq_len, n_head * head_dim) 把头给cat到一起。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = y.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">reshape</span>(&amp;[b_sz, seq_len, n_embd])?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.o_proj.forward(&amp;y)?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(y)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">repeat_kv</span>(&amp;<span class="keyword">self</span>, x: Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">n_rep</span> = <span class="keyword">self</span>.n_head / <span class="keyword">self</span>.n_kv_head;</span><br><span class="line">        <span class="keyword">if</span> n_rep == <span class="number">1</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> (b_sz, seq_len, n_kv_head, head_dim) = x.<span class="title function_ invoke__">dims4</span>()?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">x</span> = x</span><br><span class="line">                .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">3</span>)?</span><br><span class="line">                .<span class="title function_ invoke__">expand</span>((b_sz, seq_len, n_kv_head, n_rep, head_dim))?</span><br><span class="line">                .<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_kv_head * n_rep, head_dim))?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>完整模型</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    rms_norm: RmsNorm,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">    self_attention: MultiHeadAttention,</span><br><span class="line">    cache: Cache,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">normed_embeds</span> = <span class="keyword">self</span>.rms_norm.forward(&amp;embeds)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.self_attention.forward(&amp;normed_embeds, &amp;<span class="keyword">self</span>.cache)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;y)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_norm</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.rms_norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">self_attention</span> = MultiHeadAttention::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.multi_head_attention&quot;</span>), config)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">            rms_norm,</span><br><span class="line">            self_attention,</span><br><span class="line">            cache: Cache::<span class="title function_ invoke__">new</span>(config.context_length, config.d_model/config.n_head, vb)?,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_ invoke__">generate</span>(&amp;model, &amp;vocab, <span class="number">10</span>, device)?;</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;\n\n\n\n\n\n\n\nI\n\nOOOOOOOOOFOOtOOOOOOO&#39;,
 &#39;\nIIIIII IIIIIIIIIIIIIIIIIIIIIII&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naaame&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&#39;]
</code></pre>
<p>所以看起来很糟糕。这里发生了什么？让我们通过查看注意力来开始调试。</p>
<p>目前的注意力是没有masked的，任何位置的字符都在关注任何其他位置的字符。<br>这有什么不好呢？我们试图仅基于之前的标记来预测下一个标记，但这里我们看到模型正在关注之后的标记。<br>换句话说，模型在作弊，或者从未来泄露信息。这是一个问题，这就是为什么我们需要使用因果掩码。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">pub</span> <span class="keyword">struct</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// mask 也可以 cached</span></span><br><span class="line">    mask: Tensor,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(context_length: <span class="type">usize</span>, n_elem: <span class="type">usize</span>, vb: VarBuilder) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Cache&gt; &#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        <span class="comment">// _ 表示类型由编译器推断，</span></span><br><span class="line">        <span class="comment">// 默认的collect 是 [_]，但是这个大小是不可变的要编译期间决定</span></span><br><span class="line">        <span class="comment">// 所以这里还是要提示编译器要 collect 成 vec.</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mask</span>: <span class="type">Vec</span>&lt;_&gt; = (<span class="number">0</span>..context_length)</span><br><span class="line">            .<span class="title function_ invoke__">flat_map</span>(|i| (<span class="number">0</span>..context_length).<span class="title function_ invoke__">map</span>(<span class="keyword">move</span> |j| <span class="type">u8</span>::<span class="title function_ invoke__">from</span>(j &gt; i)))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mask</span> = Tensor::<span class="title function_ invoke__">from_slice</span>(&amp;mask, (context_length, context_length), vb.<span class="title function_ invoke__">device</span>())?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(Cache &#123; cos, sin, mask &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    q_proj: Linear,</span><br><span class="line">    k_proj: Linear,</span><br><span class="line">    v_proj: Linear,</span><br><span class="line">    o_proj: Linear,</span><br><span class="line">    n_head: <span class="type">usize</span>,</span><br><span class="line">    n_kv_head: <span class="type">usize</span>,</span><br><span class="line">    head_dim: <span class="type">usize</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.q_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.k_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.v_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">o_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.o_proj&quot;</span>))?;</span><br><span class="line">        <span class="built_in">println!</span>(</span><br><span class="line">            <span class="string">&quot;MHA config n_head &#123;&#125; n_kv_head &#123;&#125;&quot;</span>,</span><br><span class="line">            config.n_head, config.n_kv_head</span><br><span class="line">        );</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            q_proj,</span><br><span class="line">            k_proj,</span><br><span class="line">            v_proj,</span><br><span class="line">            o_proj,</span><br><span class="line">            n_head: config.n_head,</span><br><span class="line">            n_kv_head: config.n_kv_head,</span><br><span class="line">            head_dim: config.d_model / config.n_head,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (b_sz, seq_len, n_embd) = x.<span class="title function_ invoke__">dims3</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算 q k v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.q_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.k_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.v_proj.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">assert!</span>(n_embd == <span class="keyword">self</span>.n_head * <span class="keyword">self</span>.head_dim);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对  q  和 k 做位置编码</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;q, cache)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;k, cache)?;</span><br><span class="line">        <span class="comment">// 复制成 n_head / n_kv_head 份</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(k)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(v)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// println!(&quot;q.shape &#123;:?&#125;&quot;, q.shape());</span></span><br><span class="line">        <span class="comment">// 把 seq_len 和 n_head 交换</span></span><br><span class="line">        <span class="comment">// 这转换一下是为了做一个 cat single head 的简单操作</span></span><br><span class="line">        <span class="comment">// 相当于 n_head 个的seq_len*seq_len的注意力。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// let tmp = q.matmul(&amp;k.t()?)?;</span></span><br><span class="line">        <span class="comment">// 这个结果是有负数的，但是注意力层不会有负数。</span></span><br><span class="line">        <span class="comment">// 计算结果出了很多 NaN，感觉应该是范化没做好。</span></span><br><span class="line">        <span class="comment">// 后面发现是没有用 sqrt 用了开方，导致负数变成了NaN。</span></span><br><span class="line">        <span class="comment">// q*k^T / sqrt(d_k) d_k = d_model</span></span><br><span class="line">        <span class="comment">// 这里是 (bs,n_head) 个 (seq_len, seq_len) 的注意力</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = (q.<span class="title function_ invoke__">matmul</span>(&amp;k.<span class="title function_ invoke__">t</span>()?)? / (<span class="keyword">self</span>.head_dim <span class="keyword">as</span> <span class="type">f64</span>).<span class="title function_ invoke__">sqrt</span>())?;</span><br><span class="line">        <span class="comment">// 在 softmax 之前，把未来的token位置变为负无穷，这样在softmax之后，这些位置的概率就会变为0</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mask</span> = cache</span><br><span class="line">            .mask</span><br><span class="line">            .<span class="title function_ invoke__">i</span>((..seq_len, ..seq_len))?</span><br><span class="line">            .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">0</span>)?</span><br><span class="line">            .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">0</span>)?</span><br><span class="line">            .<span class="title function_ invoke__">broadcast_as</span>(attn.<span class="title function_ invoke__">shape</span>())?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">on_true</span> =</span><br><span class="line">            Tensor::<span class="title function_ invoke__">new</span>(<span class="type">f32</span>::NEG_INFINITY, attn.<span class="title function_ invoke__">device</span>())?.<span class="title function_ invoke__">broadcast_as</span>(mask.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>())?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = mask.<span class="title function_ invoke__">where_cond</span>(&amp;on_true, &amp;attn)?;</span><br><span class="line">        <span class="comment">// 取一个例子</span></span><br><span class="line">        <span class="comment">// 这里是头内的softmax (seq_len,seq_len)的每行 (seq_len) 总和为1</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = <span class="title function_ invoke__">softmax</span>(&amp;attn, D::Minus1)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 再乘 * (bs, n_head, seq_len, head_dim) 得到 (bs, n_head）个注意力头对应的加权的v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = attn.<span class="title function_ invoke__">matmul</span>(&amp;v)?;</span><br><span class="line">        <span class="comment">// 把 n_head 和 seq_len 交换回来，得到 (bs, seq_len, n_head, head_dim) 然后reshape以后</span></span><br><span class="line">        <span class="comment">// 得到 (bs, seq_len, n_head * head_dim) 把头给cat到一起。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = y.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">reshape</span>(&amp;[b_sz, seq_len, n_embd])?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.o_proj.forward(&amp;y)?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(y)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">repeat_kv</span>(&amp;<span class="keyword">self</span>, x: Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">n_rep</span> = <span class="keyword">self</span>.n_head / <span class="keyword">self</span>.n_kv_head;</span><br><span class="line">        <span class="keyword">if</span> n_rep == <span class="number">1</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> (b_sz, seq_len, n_kv_head, head_dim) = x.<span class="title function_ invoke__">dims4</span>()?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">x</span> = x</span><br><span class="line">                .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">3</span>)?</span><br><span class="line">                .<span class="title function_ invoke__">expand</span>((b_sz, seq_len, n_kv_head, n_rep, head_dim))?</span><br><span class="line">                .<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_kv_head * n_rep, head_dim))?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>现在，我们可以让注意力激活的上三角部分（对应未来的部分）几乎被归零了。让我们看看训练时会发生什么。</p>
<h3 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h3><p>正如论文中所述，“我们用SwiGLU激活函数替换了ReLU非线性函数……我们使用$\frac{2}{3} 4d$的维度，而不是PaLM中的$4d$。” SwiGLU定义为：<br>$$<br>\text{SwiGLU}(x) = \text{Swish}_\beta (xW + b) \otimes (xV + c)<br>$$<br>其中$\otimes$是逐元素乘积。Swish函数定义为：<br>$$<br>\text{Swish}_\beta(x) = x \sigma(\beta x)<br>$$<br>其中$\beta$是一个可学习的参数。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">silu</span>(xs: &amp;Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">    xs / (xs.<span class="title function_ invoke__">neg</span>()?.<span class="title function_ invoke__">exp</span>()? + <span class="number">1.0</span>)?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">SwiGLU</span> &#123;</span><br><span class="line">    c_fc1: Linear,</span><br><span class="line">    c_fc2: Linear,</span><br><span class="line">    c_proj: Linear,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 新的 mlp 是三层的带gate</span></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">SwiGLU</span> &#123;</span><br><span class="line">    <span class="comment">// silu 的特征是允许有一点点的负数</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="comment">// 这里就是 SwiGLU SiLU(W_1 * x) * (W_2 * x) 是 element wise 的，这个可以作为gate门信号</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (<span class="title function_ invoke__">silu</span>(&amp;<span class="keyword">self</span>.c_fc1.forward(x)?)? * <span class="keyword">self</span>.c_fc2.forward(x)?)?;</span><br><span class="line">        <span class="keyword">self</span>.c_proj.forward(&amp;x)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, cfg: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">h_size</span> = cfg.d_model;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">i_size</span> = cfg.hidden_dim;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">c_fc1</span> = <span class="title function_ invoke__">linear</span>(h_size, i_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;gate_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">c_fc2</span> = <span class="title function_ invoke__">linear</span>(h_size, i_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;up_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">c_proj</span> = <span class="title function_ invoke__">linear</span>(i_size, h_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;down_proj&quot;</span>))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            c_fc1,</span><br><span class="line">            c_fc2,</span><br><span class="line">            c_proj,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一个llama block res 两次，一次在attention之前，一次在swiglu之前。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Block</span> &#123;</span><br><span class="line">    rms_1: RmsNorm,</span><br><span class="line">    attn: MultiHeadAttention,</span><br><span class="line">    rms_2: RmsNorm,</span><br><span class="line">    swiglu: SwiGLU,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Block</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">residual</span> = x;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = <span class="keyword">self</span>.rms_1.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (<span class="keyword">self</span>.attn.forward(&amp;x, cache)? + residual)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">residual</span> = &amp;x;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (<span class="keyword">self</span>.swiglu.forward(&amp;<span class="keyword">self</span>.rms_2.forward(&amp;x)?)? + residual)?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, cfg: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = MultiHeadAttention::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;self_attn&quot;</span>), cfg)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">swiglu</span> = SwiGLU::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;mlp&quot;</span>), cfg)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_1</span> = RmsNorm::<span class="title function_ invoke__">new</span>(cfg.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;input_layernorm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_2</span> = RmsNorm::<span class="title function_ invoke__">new</span>(cfg.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;post_attention_layernorm&quot;</span>))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            rms_1,</span><br><span class="line">            attn,</span><br><span class="line">            rms_2,</span><br><span class="line">            swiglu,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>现在，让我们通过创建块来添加多个层的最后完整的模型。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Llama</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    blocks: <span class="type">Vec</span>&lt;Block&gt;,</span><br><span class="line">    ln_f: RmsNorm,</span><br><span class="line">    lm_head: Linear,</span><br><span class="line">    cache: Cache,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Llama</span> &#123;</span><br><span class="line">    <span class="keyword">pub</span> <span class="keyword">fn</span> <span class="title function_">forward</span>(</span><br><span class="line">        &amp;<span class="keyword">self</span>,</span><br><span class="line">        x: &amp;Tensor,</span><br><span class="line">        targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;,</span><br><span class="line">    ) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (_b_sz, _seq_len) = x.<span class="title function_ invoke__">dims2</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">x</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line">        <span class="keyword">for</span> <span class="variable">block</span> <span class="keyword">in</span> &amp;<span class="keyword">self</span>.blocks &#123;</span><br><span class="line">            x = block.forward(&amp;x, &amp;<span class="keyword">self</span>.cache)?;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = <span class="keyword">self</span>.ln_f.forward(&amp;x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.lm_head.forward(&amp;x)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">pub</span> <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embed_layer</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">lm_head</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.vocab_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;lm_head&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">ln_f</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">blocks</span>: <span class="type">Vec</span>&lt;_&gt; = (<span class="number">0</span>..config.n_layers)</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|i| Block::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="built_in">format!</span>(<span class="string">&quot;model.layers.&#123;i&#125;&quot;</span>)), config).<span class="title function_ invoke__">unwrap</span>())</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding: embed_layer,</span><br><span class="line">            blocks,</span><br><span class="line">            ln_f,</span><br><span class="line">            lm_head,</span><br><span class="line">            config,</span><br><span class="line">            cache: Cache::<span class="title function_ invoke__">new</span>(config.context_length, config.d_model / config.n_head, vb)?,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>这里主要是训练的部分，在推理的过程中还涉及到一个比较重要的kv cache。<br>kv cache主要是缓存自回归过程中的kv，这个kv是不变的，因为这个k 和 v 只和之前的token有关系，所以可以缓存下来，<br>这样在推理的时候就不用重复计算了。<br>围绕着prompt产生第一个Token的prefill的阶段和计算完prompt之后的decode阶段也是目前业界比较关注的推理优化的方向。<br>源代码在<a target="_blank" rel="noopener" href="https://github.com/ggaaooppeenngg/rust-llama-from-scratch">这里</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LLM/" rel="tag"># LLM</a>
              <a href="/tags/Rust/" rel="tag"># Rust</a>
              <a href="/tags/LLAMA/" rel="tag"># LLAMA</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/" rel="prev" title="vLLM 分析 3 推理优化">
                  <i class="fa fa-angle-left"></i> vLLM 分析 3 推理优化
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/zh-CN/2024/12/25/Mooncacke%E5%88%86%E6%9E%90/" rel="next" title="Mooncacke分析">
                  Mooncacke分析 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  <div class="comments" id="disqus_thread">
    <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
  </div>
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2014 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">ggaaooppeenngg</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"ggaaooppeenngg","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
