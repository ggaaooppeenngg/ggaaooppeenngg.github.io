<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="_85tctgPWrqH2EPVuuD5IT6KE-tW8nH0hTISJDMnShg">
  <meta name="baidu-site-verification" content="bb16c5b1fd3302c18e0015bef11eea42">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ggaaooppeenngg.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12,"onmobile":false},"hljswrap":true,"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="为什么计算机科学是无限的但生命是有限的">
<meta property="og:type" content="website">
<meta property="og:title" content="ggaaooppeenngg">
<meta property="og:url" content="https://ggaaooppeenngg.github.io/page/2/index.html">
<meta property="og:site_name" content="ggaaooppeenngg">
<meta property="og:description" content="为什么计算机科学是无限的但生命是有限的">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="ggaaooppeenngg">
<meta property="article:tag" content="ggaaooppeenngg,kernel,sysml,golang,python,rust">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://ggaaooppeenngg.github.io/page/2/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ggaaooppeenngg</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-62096626-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-62096626-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?bb16c5b1fd3302c18e0015bef11eea42"></script>







  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">ggaaooppeenngg</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">为什么计算机科学是无限的但生命是有限的</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">136</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">14</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">80</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ggaaooppeenngg</p>
  <div class="site-description" itemprop="description">为什么计算机科学是无限的但生命是有限的</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">80</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">136</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/ggaaooppeenngg" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ggaaooppeenngg" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:peng.gao.dut@gmail.com" title="E-Mail → mailto:peng.gao.dut@gmail.com" rel="noopener me" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2025/01/13/DeepSeek-V3%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2025/01/13/DeepSeek-V3%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">DeepSeek V3分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-13 12:10:49" itemprop="dateCreated datePublished" datetime="2025-01-13T12:10:49+08:00">2025-01-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2025/01/13/DeepSeek-V3%E5%88%86%E6%9E%90/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2025/01/13/DeepSeek-V3分析/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="DeepSeek-V3分析"><a href="#DeepSeek-V3分析" class="headerlink" title="DeepSeek V3分析"></a>DeepSeek V3分析</h1><blockquote>
<p>During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K<br>H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs.</p>
</blockquote>
<p>DeepSeek实现了非常便宜的训练成本，是一个700B的MoE模型。</p>
<h2 id="基础设施"><a href="#基础设施" class="headerlink" title="基础设施"></a>基础设施</h2><ul>
<li><strong>计算集群</strong>：在配备 2048 个 NVIDIA H800 GPU 的集群上训练，节点内通过 NVLink 和 NVSwitch 连接，节点间使用 InfiniBand 互连。</li>
<li><strong>训练框架</strong>：基于 HAI - LLM 框架，采用 16 路管道并行（PP）、64 路专家并行（EP）和 ZeRO - 1 数据并行（DP）。设计 DualPipe 算法减少管道气泡并重叠计算与通信，开发高效的跨节点全对全通信内核，优化内存占用，无需使用昂贵的张量并行（TP）。</li>
<li><strong>FP8 训练</strong>：提出 FP8 混合精度训练框架，对多数计算密集型操作采用 FP8 精度，对部分关键操作保留原始精度，引入细粒度量化策略、提高累积精度、采用 E4M3 格式及在线量化，还降低了内存和通信开销。</li>
<li><strong>推理与部署</strong>：部署在 H800 集群上，通过分离预填充和解码阶段确保服务水平目标（SLO）和高吞吐量。预填充阶段最小部署单元为 4 节点 32 个 GPU，采用特定并行策略和冗余专家策略确保负载均衡；解码阶段最小部署单元为 40 节点 320 个 GPU，采用相应并行策略和冗余专家策略，并探索动态冗余策略。</li>
<li><strong>硬件设计建议</strong>：针对通信硬件，期望未来硬件能卸载通信任务，统一网络接口；针对计算硬件，建议提高 FP8 GEMM 累积精度、支持细粒度量化、在线量化和转置 GEMM 操作。</li>
</ul>
<h2 id="并行度配置"><a href="#并行度配置" class="headerlink" title="并行度配置"></a>并行度配置</h2><p>在<strong>prefill阶段</strong>，attention模块采用4路张量并行+8路数据并行，moe模块采用32路专家并行。这样并行的目的是在满足首token时延的要求下，最大化系统吞吐（和训练任务类似）。</p>
<p>在<strong>decode阶段</strong>，DeepSeek-V3采取320路专家并行（256个小专家+64个热点专家），有效降低解码时延，并缓解负载不均衡的问题。</p>
<p>DeepSeek-V3 采用了多种并行策略，包括 16 路流水线并行（PP），这一策略有助于提高训练效率，加快模型的处理速度。同时，还应用了 64 路专家并行（EP），且在 8 个节点上进行，能够充分发挥多节点的计算优势。此外，ZeRO-1 数据并行（DP）也被运用到训练中，进一步提升了模型的训练效果。</p>
<p>ZeRO-1 优化器被切分到不同的GPU上。 《大模型动力引擎——PyTorch性能与显存优化手册》有提到这个优化，总结的很好。</p>
<blockquote>
<p>假设我们有N=64块GPU进行数据并行训练，在ZeRO-1阶段，优化器的状态量首先被分散存储到所有GPU中，此时单张GPU上的内存使用量骤降到(4+4+8/64)*7.5=60.9GB。ZeRO-2阶段进一步地将模型的梯度也分散存储，此时单张GPU上的内存使用量便是(4+(4+8)/64)7.5=31.4GB。而ZeRO-3阶段将模型的参数也分散存储到N个节点，此时每张GPU的内存消耗只有(4+4+8)/647.5=1.875GB。从单卡需要120GB到仅需不到2GB内存，这个优化效果是不是有点惊艳？不过需要再次强调的是，这样巨大的显存优化是有代价的，显存切分的程度越高，相应的通信开销也会增加。因此，根据实际需求合理地进行显存切分是非常重要的。</p>
</blockquote>
<h2 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h2><p>采用类似 LoRA 的架构，借助一个低秩矩阵 “compressed laten vector”，kvcache 仅需对低秩的 key-value 对以及附带旋转位置编码（RoPE）的 key 进行缓存。</p>
<h2 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h2><p>除了针对 Top k、routed experts 运用添加了激活函数的加权求和方式外，还额外引入了 shared experts。在 gate 的激活函数里增添一个 bias，以此来化解 balance 失衡的难题，在训练阶段，通过调节这个 bias 对 balance 状况予以奖惩，这一调节过程被称作 bias update speed。<br>就一个 batch、一个序列而言，每个 token 倘若倾向于特定的一些 expert，那么未被选中的 expert 实际上仅相当于训练了极小的 batch size，或者极短的序列，正因如此，才有了这样一种策略，用以平衡 expert 的 batch size 以及序列当中的 token 数量，毕竟序列通常都很长。<br>DeepSeek-V3 着重凭借辅助损失策略达成负载均衡，与此同时，引入互为补充的序列平衡损失，以防单个序列内部出现极度不平衡的现象。</p>
<h2 id="MTP"><a href="#MTP" class="headerlink" title="MTP"></a>MTP</h2><p>类似于 speculative decoding，它同样会计算多个 token，不过具体方式存在一定差异。其 embedding 与 output head 是共用的，这一点和 sd 里的 Medusa 有所不同，Medusa 是由多个头来推测不同位置，而 MTP 则是依靠多个相同的头（只是 attention 有别）去推断不同位置。</p>
<p>MTP 的核心目的在于提升主模型的性能表现，在推理阶段能够直接将 MTP 模块舍去，主模型依旧可以独自正常运作。不仅如此，MTP 模块还能够应用于推测解码环节，以此进一步优化生成延迟问题，让整个流程更加高效流畅。</p>
<h2 id="DualPipe"><a href="#DualPipe" class="headerlink" title="DualPipe"></a>DualPipe</h2><p>双流水线pipeline的优化。它实现了前向和后向过程中计算与通信阶段的重叠，有效解决了跨节点专家并行带来的通信负载问题。</p>
<h2 id="FP8"><a href="#FP8" class="headerlink" title="FP8"></a>FP8</h2><p>能够不依赖硬件能力做FP8精度的训练，这个点是非常厉害的。</p>
<p>首先，为提高模型训练速度，大部分核心计算操作（尤其是 GEMM 运算），均采用 FP8 精度实现。这些 GEMM 运算接收 FP8 格式的张量输入，输出 BF16 或 FP32 格式的结果。如图6所示，线性运算相关的三个 GEMM 操作，包括 Fprop（前向传播）、Dgrad（激活值反向传播）和 Wgrad（权重反向传播），均采用 FP8 执行。这种设计策略理论上将计算速度提升至原有 BF16 方法的两倍。同时，FP8 格式的 Wgrad GEMM 使得激活值能够以 FP8 格式存储用于反向传播，显著降低了内存使用量。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2025/01/07/GaLore%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2025/01/07/GaLore%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">GaLore分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-07 15:41:06" itemprop="dateCreated datePublished" datetime="2025-01-07T15:41:06+08:00">2025-01-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2025/01/07/GaLore%E5%88%86%E6%9E%90/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2025/01/07/GaLore分析/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>LoRA一般的设定是认为微调任务应该只需要在一个更小的子空间去训练即可不需要复用基座模型的大空间，从而实现低成本的微调。<br>LoRA的前提是问题是不是在子空间能得到最优解。在线性回归 y=W x 中，如果最优 W * 是高秩的，那么对 W 施加低秩假设永远不会导致最优解，无论使用什么优化器。</p>
<p>Gradient Low-Rank Projection (GaLore) 允许全参数学习，但比 LoRA 等常见的低秩自适应方法更具内存效率。</p>
<p>使用单个批处理大小从头开始预训练 LLaMA7B 模型至少需要 58 GB 内存（14GB 用于可训练参数，42GB 用于 Adam 优化器状态和权重梯度，2GB 用于激活函数）。这使得训练在消费级 GPU 上不可行，例如具有 24GB 内存的 NVIDIA RTX 4090。</p>
<p>他证明梯度可能具有低秩结构，如果我们能够在优化器状态中保留梯度的一个小 “核心” 的梯度统计信息，而不是完整的梯度本身，那么内存消耗就可以大幅降低。这就引出了 GaLore 策略。<br>他的关键思想是利用权重矩阵 W 的梯度 G 上做LoRA，而不是试图将权重矩阵本身近似为低秩。他的核心逻辑用Torch写出来如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> weight <span class="keyword">in</span> model.parameters():</span><br><span class="line">    grad = weight.grad</span><br><span class="line">    <span class="comment"># original space -&gt; compact space</span></span><br><span class="line">    lor_grad = project(grad)</span><br><span class="line">    <span class="comment"># update by Adam, Adafactor, etc.</span></span><br><span class="line">    lor_update = update(lor_grad)</span><br><span class="line">    <span class="comment"># compact space -&gt; original space </span></span><br><span class="line">    update = project_back(lor_update) </span><br><span class="line">    weight.data += update</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/26/Sequence-Parallelism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/12/26/Sequence-Parallelism/" class="post-title-link" itemprop="url">Sequence Parallelism</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-26 16:12:55" itemprop="dateCreated datePublished" datetime="2024-12-26T16:12:55+08:00">2024-12-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/26/Sequence-Parallelism/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/26/Sequence-Parallelism/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Sequence-Parallelism"><a href="#Sequence-Parallelism" class="headerlink" title="Sequence Parallelism"></a>Sequence Parallelism</h1><p>假设有4个chunk，切四份。</p>
<img data-src="/zh-CN/2024/12/26/Sequence-Parallelism/step_0.png" class="">

<p>初始化状态，每个GPU都有自己的 Qn Kn，可以计算出对应的注意力矩阵，然后类似AllReduce的方式传递切分的K。</p>
<img data-src="/zh-CN/2024/12/26/Sequence-Parallelism/step_1.png" class="">

<p>第一步环形传递K，然后再算一次注意力矩阵。</p>
<img data-src="/zh-CN/2024/12/26/Sequence-Parallelism/step_2.png" class="">

<p>第二步环形传递K，然后再算一次注意力矩阵。</p>
<img data-src="/zh-CN/2024/12/26/Sequence-Parallelism/step_3.png" class="">

<p>第三步全部传完，得到完整的Sn。</p>
<p>然后 Sn 和 Vn 的计算也是类似的，经过三次环形传递Vn，然后每一份可以单独和小s的那一份做乘法。</p>
<p>所以K和V的传播都要经历 3 次(N-1)的集合通信。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/25/Mooncacke%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/12/25/Mooncacke%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">Mooncacke分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-25 18:06:12" itemprop="dateCreated datePublished" datetime="2024-12-25T18:06:12+08:00">2024-12-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/25/Mooncacke%E5%88%86%E6%9E%90/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/25/Mooncacke分析/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>LLM推理的核心在于KVCache的调度。</p>
<ol>
<li>尽可能多次重用KV缓存，以减少所需的计算资源；</li>
<li>每批次最大化token数量，从而改善Model FLOPs Utilization (MFU)。</li>
</ol>
<p>如果从远程内存获取KVCache，会增加数据传输时间，从而延长TTFT（Time To First Token）。因此，当本地KVCache的增量计算时间少于传输时间时，可以复用本地的KVCache，即使它不是最匹配的。而增大batch意味着系统处理的大批量数据，导致TBT（Token Between Token）延长，可以将负载均衡到低负载的Decode Instance。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><img data-src="/zh-CN/2024/12/25/Mooncacke%E5%88%86%E6%9E%90/arch.png" class="">

<p>Mooncake的架构图主要分为三个部分：Prefill Instance，Decode Instance，Conductor。</p>
<ol>
<li>Cache-aware Prefill Scheduler：负责调度Request到Prefill Instance，主要考虑load和KVCache的复用率。</li>
<li>KVCache Balance Scheduler：负责从匹配最多前缀的P2P传输KVCache到Instance（Decode和Prefill）。</li>
<li>Load-balance Decoding Scheduler：负责负载均衡调度Request到Decode Instance。</li>
</ol>
<p>Prefill Instance要满足TTFT SLO，最小化MFU，保证KVCache &lt; DRAM。<br>Decode Instance要满足TBT SLO，保证KVCache &lt; VRAM。<br>Inter-Node Transfer基于RDMA的P2P，这也是一个较大的开销。</p>
<p>Mooncake的方法总结如下：</p>
<ol>
<li>转移可重用的KVCache，将尽可能多的可重用KVCache转移至Prefill Instance，减少增量计算的时间。</li>
<li>Prefill Instance Pool分层并分块处理，并持续输出给对应的Decode Instance。分层指的是Layer-wise KVCache的异步保存，分块指的是Chunked Pipeline Parallelism。</li>
<li>独立的Decode Instance Pool加载KVCache，通过连续批处理解码tokens。</li>
</ol>
<p>Mooncake的主要特点是将prefill和decode拆开，并调度KVCache块。</p>
<p>Reject Policy：如果一个请求不能在服务水平内完成其完整的执行，那么就应该尽早拒绝这个请求，基于这个理念需要设计一些拒绝策略，被称作Overloaded-Scheduling。</p>
<h3 id="KVCache的复制"><a href="#KVCache的复制" class="headerlink" title="KVCache的复制"></a>KVCache的复制</h3><p>KVCache的调度主要是利用KV Cache（VRAM，DRAM），利用RDMA带宽。</p>
<p>下图是一个Prefill和Decode分离的计算过程。</p>
<img data-src="/zh-CN/2024/12/25/Mooncacke%E5%88%86%E6%9E%90/kv_transfer.png" class="">

<p>如果了解vLLM中的prefill和decode以及管理block的方法，这个图其实很简单。</p>
<p>首先通过Hash判断block是否相同，例如很多系统提示词都是一样的，这部分的复用率很高。</p>
<p>Prefill Instance已经有了ABCDE（这里是一个P2P的过程，但我看开源的版本有个KVCache Store的WIP，不知道后面会不会有一个中心化的KVCache Store的组件）。然后计算了FGHI，存入了KV Cache（在CPU mem上），论文里面提到这个prefill在超过<code>prefill_chunk</code> tokens数量会做chunked prefill。</p>
<p>接着通过Messenger以RDMA的方式发给Decode Instance。Decode Instance基于ABCDEFGHI的prompt对应的KV Cache开始decode的过程。</p>
<p>根据请求模式，它可以使用缓存淘汰算法，如LRU（最近最少使用），LFU（最不常用的），或基于请求特征的算法。这些KVCache块在CPU和GPU之间的传输由一个独立的（GPUDirect）RDMA组件Messenger处理。这种架构还使我们能够为外部用户提供KVCache缓存API，从而实现更高的缓存重用性。</p>
<p>Mooncake已经开源了他的<a target="_blank" rel="noopener" href="https://github.com/kvcache-ai/Mooncake">代码</a>，目前只有Transfer Engine。</p>
<p>基于这个架构，Conductor的主要功能是：</p>
<ol>
<li>根据当前的KVCache分布和工作负载，分发请求。</li>
<li>复制或交换某些KVCache块，以便于未来推理。如果某些块的数据在未来被频繁访问，Conductor可能会将其复制到其他节点上，从而提高推理效率。</li>
</ol>
<p>Mooncake的一个争论点是，是否需要在存在chunked prefill的情况下采用这种分离架构。毕竟，chunked prefill可以填补许多pipeline中的气泡，并且能让prefill和decode节点相对统一，只需要关心一种instance，对于scheduler比较友好。</p>
<ol>
<li><p>不分离的优点：</p>
<ul>
<li>所有节点被视为平等，使调度更简单；</li>
<li>将chunked prefill内联到解码批处理中可以提高解码批次的计算强度，从而提高MFU。</li>
</ul>
</li>
<li><p>分离的优点：</p>
<ul>
<li>长文本的跨节点并行和VRAM的节省。长文本输入是输出的10倍甚至100倍，对于相同的模型来说，prefill需要多节点配置才能满足显存需求。prefill阶段可以进行layer-wise prefill，每次保存大量KVCache，而decode阶段每次只需保存一个KVCache。因此，prefill阶段可以通过layer-wise prefill来减少VRAM占用。</li>
</ul>
</li>
</ol>
<blockquote>
<p>是这么理解么？异步的Store KVCache可以节省保存的时间，但这是Prefill和Decode分离的理由么？Decode阶段应该是不保存KVCache?</p>
</blockquote>
<p>然而，经过仔细考虑，论文决定保持Mooncake的分离架构。只有在请求的prefill可以不进行chunking且不影响TBT SLO的情况下，才会将其内联到解码批次中。我们这样决定的主要原因有两个：</p>
<ol>
<li><p>Prefill节点需要不同的跨节点并行设置来处理长上下文 (§5.1)。</p>
</li>
<li><p>这为节省VRAM提供了独特的机会 (§5.2)。</p>
</li>
<li><p>大模型需要部署在多机上，进行TP后，每一层都需要进行一次基于RDMA的reduce，这个过程开销巨大。虽然有一些Sequence Parallelism的方法，但效果并不理想，且无法避免跨节点通信。而Mooncake采用的是CPP（Chunked Parallelism Pipeline），将序列按<code>prefill_chunk</code>大小切分，交给prefill pool的不同节点，这些节点被切分成更小的节点池（pipelined prefill node group）。</p>
</li>
</ol>
<blockquote>
<p>疑问：他们是pipe的不同部分？还是完全对等的？目前感觉是PP是分layer做Pipe，而CPP是sequence分chunked做pipe。24引用的论文中提到的Sequence Pipeline可以再看一下，应该对理解这个有帮助。</p>
</blockquote>
<ol start="2">
<li>Layer-wise prefill，这有点像airllm项目，在计算过程中动态加载KVCache。在每次注意力计算时，KVCache是异步加载的，计算当前层时可以异步加载下一层，并且当前层结束后可以异步保存当前层。论文中认为KVCache的保存时间可以被完全省略（相较于加载计算保存的线性循环）。这样也可以降低VRAM的占用。</li>
</ol>
<h3 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h3><ol>
<li><p><strong>选择Prefill实例</strong>：</p>
<ul>
<li>如果Prefill节点上缓存了足够的前缀（由<code>kvcache_balancing_threshold</code>控制），则选择预估TTFT最小的实例：<code>TTFT = min(T_queue + T_prefill)</code>。</li>
<li>如果Prefill节点上缓存不足，则选择<code>TTFT = min(T_queue + T_prefill + T_transfer)</code>最小的实例，其中<code>T_transfer</code>指的是有最长匹配的KVCache的实例拷贝到当前实例的预估时间。</li>
</ul>
</li>
<li><p><strong>选择Decode实例</strong>：</p>
<ul>
<li>通过负载均衡的方式预估TBT。</li>
<li>如果TBT和TTFT不满足SLO，则拒绝请求，并触发KVCache的传输。</li>
</ul>
</li>
<li><p><strong>预测模型</strong>：</p>
<ul>
<li>预估模型用于预测传输时间和决策传输。</li>
<li>数据传输时间难以预测，因为它不仅取决于数据大小，还依赖于当前网络状态，特别是当发送节点处于拥塞状态时。</li>
</ul>
</li>
<li><p><strong>KVCache复制</strong>：</p>
<ul>
<li>热门的KVCache块需要被复制以确保高可用性。</li>
</ul>
</li>
<li><p><strong>调度器目标</strong>：</p>
<ul>
<li>保证低Cache负载和高Cache命中率。</li>
</ul>
</li>
<li><p><strong>高负载情况下的策略</strong>：</p>
<ul>
<li>请求可能不会被直接发送给缓存最长前缀的实例，而是转发给备选实例。备选实例会主动从缓存持有者处检索KV缓存并存储本地。</li>
<li>当最佳的远程前缀匹配长度不超过当前本地可重用前缀的阈值时，系统优先使用本地缓存，而不是从远程实例获取令牌。</li>
</ul>
</li>
</ol>
<p>这些策略不仅减少了请求的Prefill时间，还自动复制热点缓存，使其在多台机器上更广泛地分布。</p>
<h3 id="拒绝策略"><a href="#拒绝策略" class="headerlink" title="拒绝策略"></a>拒绝策略</h3><p>论文提到了一种基于预测的拒绝策略。Prefill和Decode的负载节奏是相反的，可能在Decode负载高时，Prefill负载较低。此时如果拒绝请求，会导致Decode负载下降，而Prefill完成后Decode负载又会升高，进而再次拒绝请求。引入预测拒绝策略后，可以使Prefill过程更加平滑，减少频繁拒绝请求的情况，从而减小负载节奏的波动。</p>
<img data-src="/zh-CN/2024/12/25/Mooncacke%E5%88%86%E6%9E%90/reject_policy.png" class="">

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/" class="post-title-link" itemprop="url">Rust从零实现llama</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-23 16:57:18" itemprop="dateCreated datePublished" datetime="2024-12-23T16:57:18+08:00">2024-12-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/23/Rust%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0llama/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/23/Rust从零实现llama/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>参考Pytorch版本的<a target="_blank" rel="noopener" href="https://github.com/bkitano/llama-from-scratch">llama-from-scratch</a>。原文中的RmsNorm的平均值多算了一个维度，这里改成了正确的版本。</p>
<p>首先需要下载<a target="_blank" rel="noopener" href="https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt">TinyShakespeare</a>数据集，这是一个莎士比亚文字数据集。本文档将<em>大致</em>遵循论文的布局，并跳过一些明显的步骤，比如设置虚拟环境和安装依赖项。</p>
<p>我们最终将实现的内容预览：</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">println!</span>(<span class="title function_ invoke__">generate</span>(llama, MASTER_CONFIG, <span class="number">500</span>, device)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">ZELBETH:</span><br><span class="line">Sey solmenter! <span class="symbol">&#x27;tis</span> tonguerered <span class="keyword">if</span> berryishdd, and What his stabe, you, and, but all I pilJefals, mode with,</span><br><span class="line">Vurint <span class="keyword">as</span> steolated have loven OlD the queen<span class="symbol">&#x27;d</span> refore</span><br><span class="line">Are been, good plmp:</span><br><span class="line"></span><br><span class="line">Proforne, wift<span class="symbol">&#x27;es</span> swleen, was no bunderes<span class="symbol">&#x27;d</span> a a quain beath!</span><br><span class="line">Tybell is my gateer stalk smen<span class="symbol">&#x27;d</span> <span class="keyword">as</span> be matious dazest brink thou</span><br><span class="line">lord</span><br><span class="line">Enves were cIUll, afe and whwas seath This a is, an tale hoice his his onety Meall-tearn not murkawn, fase bettizen<span class="symbol">&#x27;d</span> her,</span><br><span class="line">To belacquesterer? baxewed wupl usweggs yet tall</span><br><span class="line">An</span><br></pre></td></tr></table></figure>

<p>实现过程中可能涉及一些 Rust 的使用方法，与 Python 有所不同，这里不做过多说明，具体的 Rust 语法可以参考其他文档。</p>
<h2 id="迭代工作：从小模块开始，保持确定性，然后逐步构建"><a href="#迭代工作：从小模块开始，保持确定性，然后逐步构建" class="headerlink" title="迭代工作：从小模块开始，保持确定性，然后逐步构建"></a>迭代工作：从小模块开始，保持确定性，然后逐步构建</h2><ol>
<li>创建所有需要的辅助函数，以便定量测试模型（数据拆分、训练、绘制损失）。</li>
<li>从论文中挑选出不同的组件，然后逐一实现，边训练边评估。</li>
</ol>
<h2 id="确保你的层按预期工作"><a href="#确保你的层按预期工作" class="headerlink" title="确保你的层按预期工作"></a>确保你的层按预期工作</h2><ol>
<li>经常使用 <code>.shape()</code>。<code>assert</code> 是你的朋友。</li>
<li>先在不进行矩阵乘法的情况下计算结果，然后使用 <code>candle</code> 函数使其高效。</li>
<li>有一个测试来确保你的层是正确的。例如，RoPE 嵌入有一个特定的属性，你可以测试它。对于 Transformer，你可以通过查看注意力矩阵来测试注意力是否正常工作。</li>
<li>在各种批次、序列和嵌入大小上测试你的层。即使它适用于一种大小，它可能不适用于其他大小，这将在推理时导致问题。</li>
</ol>
<h2 id="关于-Llama"><a href="#关于-Llama" class="headerlink" title="关于 Llama"></a>关于 Llama</h2><p>Llama 是一种基于 Transformer 的语言建模模型。它是一个自回归模型，也称为 CausalModel，模型会将输出中的 token 加入到输入中，不断迭代推理，直到超过上下文长度或遇到停止符。Meta AI <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">开源</a>了 Llama，并明确表示他们的目标是使模型在推理时更高效，而不是优化训练成本。</p>
<p>接下来，我们将加载库并开始实现。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> candle_core::&#123;DType, Device, IndexOp, <span class="type">Result</span>, Tensor, D&#125;;</span><br><span class="line"><span class="keyword">use</span> candle_nn::ops::softmax;</span><br><span class="line"><span class="keyword">use</span> candle_nn::&#123;</span><br><span class="line">    embedding, linear, loss, AdamW, Embedding, Init, Linear, Module, Optimizer, ParamsAdamW,</span><br><span class="line">    VarBuilder,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">use</span> core::<span class="type">f32</span>;</span><br><span class="line"><span class="keyword">use</span> rand::Rng;</span><br><span class="line"><span class="keyword">use</span> std::collections::HashMap;</span><br><span class="line"><span class="keyword">use</span> std::fs;</span><br><span class="line"><span class="keyword">use</span> std::time;</span><br></pre></td></tr></table></figure>

<h2 id="设置数据集"><a href="#设置数据集" class="headerlink" title="设置数据集"></a>设置数据集</h2><p>虽然 Llama 在 1.4T 个标记上进行训练，但我们的数据集 TinyShakespeare，即莎士比亚所有作品的集合，大约只有 100 万个字符。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> std::collections::HashMap;</span><br><span class="line"><span class="keyword">use</span> std::fs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="comment">// Read the entire content of the file</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">lines</span> = fs::<span class="title function_ invoke__">read_to_string</span>(<span class="string">&quot;./input.txt&quot;</span>)</span><br><span class="line">        .<span class="title function_ invoke__">expect</span>(<span class="string">&quot;Failed to read the file&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a sorted set of unique characters</span></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">vocab</span>: <span class="type">Vec</span>&lt;<span class="type">char</span>&gt; = lines.<span class="title function_ invoke__">chars</span>().<span class="title function_ invoke__">collect</span>();</span><br><span class="line">    vocab.<span class="title function_ invoke__">sort_unstable</span>();</span><br><span class="line">    vocab.<span class="title function_ invoke__">dedup</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create itos and stoi mappings</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">itos</span>: HashMap&lt;<span class="type">usize</span>, <span class="type">char</span>&gt; = vocab.<span class="title function_ invoke__">iter</span>().<span class="title function_ invoke__">enumerate</span>().<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (i, ch)).<span class="title function_ invoke__">collect</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">stoi</span>: HashMap&lt;<span class="type">char</span>, <span class="type">usize</span>&gt; = vocab.<span class="title function_ invoke__">iter</span>().<span class="title function_ invoke__">enumerate</span>().<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (ch, i)).<span class="title function_ invoke__">collect</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print the first 30 characters of the file</span></span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, &amp;lines[..<span class="number">30</span>.<span class="title function_ invoke__">min</span>(lines.<span class="title function_ invoke__">len</span>())]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<pre><code>First Citizen:
Before we proce
</code></pre>
<p>他们使用了<a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a>字节对编码分词器，但我们将只使用一个简单的字符级分词器。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> std::collections::HashMap;</span><br><span class="line"><span class="keyword">use</span> std::fs;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Vocab</span> &#123;</span><br><span class="line">    itos: HashMap&lt;<span class="type">u32</span>, <span class="type">char</span>&gt;,</span><br><span class="line">    stoi: HashMap&lt;<span class="type">char</span>, <span class="type">u32</span>&gt;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Vocab</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(itos: HashMap&lt;<span class="type">u32</span>, <span class="type">char</span>&gt;, stoi: HashMap&lt;<span class="type">char</span>, <span class="type">u32</span>&gt;) <span class="punctuation">-&gt;</span> Vocab &#123;</span><br><span class="line">        Vocab &#123; itos, stoi &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">decode</span>(&amp;<span class="keyword">self</span>, ids: &amp;[<span class="type">u32</span>]) <span class="punctuation">-&gt;</span> <span class="type">String</span> &#123;</span><br><span class="line">        ids.<span class="title function_ invoke__">iter</span>().<span class="title function_ invoke__">map</span>(|&amp;id| <span class="keyword">self</span>.itos[&amp;id]).<span class="title function_ invoke__">collect</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">encode</span>(&amp;<span class="keyword">self</span>, text: &amp;<span class="type">str</span>) <span class="punctuation">-&gt;</span> <span class="type">Vec</span>&lt;<span class="type">u32</span>&gt; &#123;</span><br><span class="line">        text.<span class="title function_ invoke__">chars</span>().<span class="title function_ invoke__">map</span>(|ch| <span class="keyword">self</span>.stoi[&amp;ch]).<span class="title function_ invoke__">collect</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">len</span>(&amp;<span class="keyword">self</span>) <span class="punctuation">-&gt;</span> <span class="type">usize</span> &#123;</span><br><span class="line">        <span class="keyword">self</span>.itos.<span class="title function_ invoke__">len</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">build</span>(lines: &amp;<span class="type">str</span>) <span class="punctuation">-&gt;</span> <span class="keyword">Self</span> &#123;</span><br><span class="line">        <span class="comment">// Create a sorted set of unique characters</span></span><br><span class="line">        <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">vocab</span>: <span class="type">Vec</span>&lt;<span class="type">char</span>&gt; = lines.<span class="title function_ invoke__">chars</span>().<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        vocab.<span class="title function_ invoke__">sort</span>();</span><br><span class="line">        vocab.<span class="title function_ invoke__">dedup</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Create itos and stoi mappings</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">itos</span>: HashMap&lt;<span class="type">u32</span>, <span class="type">char</span>&gt; = vocab</span><br><span class="line">            .<span class="title function_ invoke__">iter</span>()</span><br><span class="line">            .<span class="title function_ invoke__">enumerate</span>()</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (i <span class="keyword">as</span> <span class="type">u32</span>, ch))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">stoi</span>: HashMap&lt;<span class="type">char</span>, <span class="type">u32</span>&gt; = vocab</span><br><span class="line">            .<span class="title function_ invoke__">iter</span>()</span><br><span class="line">            .<span class="title function_ invoke__">enumerate</span>()</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|(i, &amp;ch)| (ch, i <span class="keyword">as</span> <span class="type">u32</span>))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">Self</span> &#123; itos, stoi &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="comment">// Read the entire content of the file</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">lines</span> = fs::<span class="title function_ invoke__">read_to_string</span>(<span class="string">&quot;./input.txt&quot;</span>).<span class="title function_ invoke__">expect</span>(<span class="string">&quot;Failed to read the file&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">vocab</span> = Vocab::<span class="title function_ invoke__">build</span>(&amp;lines);</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;vocab size = &#123;&#125;&quot;</span>, vocab.<span class="title function_ invoke__">len</span>());</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, vocab.<span class="title function_ invoke__">decode</span>(&amp;vocab.<span class="title function_ invoke__">encode</span>(<span class="string">&quot;hello&quot;</span>)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<pre><code>vocab size = 65
hello
</code></pre>
<p>由于数据集较小，我们无需担心内存存储问题。</p>
<p>我们创建了一个 <code>config</code> 对象来存储基本的模型参数。这样可以提高代码的可读性，并且便于修改配置。Rust 是强类型语言，因此所有变量都有明确的类型。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">modeConfig</span> = ModelConfig &#123;</span><br><span class="line">    vocab_size: vocab.<span class="title function_ invoke__">len</span>(),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable">dataset</span> = Tensor::<span class="title function_ invoke__">from_slice</span>(&amp;vocab.<span class="title function_ invoke__">encode</span>(&amp;lines), (lines.<span class="title function_ invoke__">len</span>(),), &amp;Device::Cpu).<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;:?&#125;&quot;</span>, dataset.<span class="title function_ invoke__">shape</span>());</span><br></pre></td></tr></table></figure>

<pre><code>[1115394]
</code></pre>
<p>让我们创建一个方法 <code>get_batches</code> 来生成训练数据和目标的批次。我们将使用相同的方法来生成验证和测试数据，通过 <code>split</code> 参数来控制。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">get_batches</span>(</span><br><span class="line">    dataset: &amp;Tensor,</span><br><span class="line">    split: &amp;<span class="type">str</span>,</span><br><span class="line">    batch_size: <span class="type">usize</span>,</span><br><span class="line">    context_length: <span class="type">usize</span>,</span><br><span class="line">) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, Tensor)&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">len_of_dataset</span> = dataset.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>).<span class="title function_ invoke__">unwrap</span>() <span class="keyword">as</span> <span class="type">f32</span>;</span><br><span class="line">    <span class="comment">// 按照 0.8 0.1 0.1 的比例切分训练集, 验证集和测试集</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">batch_data</span> = <span class="keyword">match</span> split &#123;</span><br><span class="line">        <span class="string">&quot;val&quot;</span> =&gt; &amp;dataset.<span class="title function_ invoke__">i</span>((<span class="number">0.8</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>..(<span class="number">0.9</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>)?,</span><br><span class="line">        <span class="string">&quot;test&quot;</span> =&gt; &amp;dataset.<span class="title function_ invoke__">i</span>((<span class="number">0.9</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>..)?,</span><br><span class="line">        _ =&gt; &amp;dataset.<span class="title function_ invoke__">i</span>(..(<span class="number">0.8</span> * len_of_dataset) <span class="keyword">as</span> <span class="type">usize</span>)?,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// 生成随机index</span></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">rng</span> = rand::<span class="title function_ invoke__">thread_rng</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">data_len</span> = batch_data.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">indices</span>: <span class="type">Vec</span>&lt;<span class="type">usize</span>&gt; = (<span class="number">0</span>..batch_size)</span><br><span class="line">        .<span class="title function_ invoke__">map</span>(|_| rng.<span class="title function_ invoke__">gen_range</span>(<span class="number">0</span>..data_len - context_length - <span class="number">1</span>))</span><br><span class="line">        .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">x_batches</span> = <span class="type">Vec</span>::<span class="title function_ invoke__">with_capacity</span>(batch_size);</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">y_batches</span> = <span class="type">Vec</span>::<span class="title function_ invoke__">with_capacity</span>(batch_size);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="variable">idx</span> <span class="keyword">in</span> indices &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = batch_data.<span class="title function_ invoke__">i</span>(idx..(idx + context_length))?;</span><br><span class="line">        <span class="comment">// y 是 x 后面的一个字符</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = batch_data.<span class="title function_ invoke__">i</span>((idx + <span class="number">1</span>)..(idx + context_length + <span class="number">1</span>))?;</span><br><span class="line">        x_batches.<span class="title function_ invoke__">push</span>(x);</span><br><span class="line">        y_batches.<span class="title function_ invoke__">push</span>(y);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// stack 和 cat 的区别是, stack 是在新的维度上堆叠, cat 是在已有的维度上堆叠</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x_tensor</span> = Tensor::<span class="title function_ invoke__">stack</span>(&amp;x_batches, <span class="number">0</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">y_tensor</span> = Tensor::<span class="title function_ invoke__">stack</span>(&amp;y_batches, <span class="number">0</span>)?;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>((x_tensor, y_tensor))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// in fn main</span></span><br><span class="line">modeConfig.context_length = <span class="number">16</span>;</span><br><span class="line">modeConfig.batch_size = <span class="number">8</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">batch</span> = <span class="title function_ invoke__">get_batches</span>(</span><br><span class="line">    &amp;dataset,</span><br><span class="line">    <span class="string">&quot;train&quot;</span>,</span><br><span class="line">    modeConfig.batch_size,</span><br><span class="line">    modeConfig.context_length,</span><br><span class="line">)?;</span><br><span class="line"><span class="built_in">println!</span>(</span><br><span class="line">    <span class="string">&quot;batch size &#123;&#125;, context_length &#123;&#125;&quot;</span>,</span><br><span class="line">    batch.<span class="number">0</span>.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?,</span><br><span class="line">    batch.<span class="number">0</span>.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?</span><br><span class="line">);</span><br><span class="line"><span class="keyword">for</span> <span class="variable">i</span> <span class="keyword">in</span> <span class="number">0</span>..modeConfig.batch_size &#123;</span><br><span class="line">    <span class="built_in">println!</span>(</span><br><span class="line">        <span class="string">&quot;&#123;:?&#125;, &#123;:?&#125;&quot;</span>,</span><br><span class="line">        vocab.<span class="title function_ invoke__">decode</span>(&amp;batch.<span class="number">0</span>.<span class="title function_ invoke__">i</span>(i)?.<span class="title function_ invoke__">to_vec1</span>()?),</span><br><span class="line">        vocab.<span class="title function_ invoke__">decode</span>(&amp;batch.<span class="number">1</span>.<span class="title function_ invoke__">i</span>(i)?.<span class="title function_ invoke__">to_vec1</span>()?),</span><br><span class="line">    );</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>&quot;:\nBut, that I&#39;ll&quot;, &quot;\nBut, that I&#39;ll &quot;
&quot;ng?\nWhy, then th&quot;, &quot;g?\nWhy, then the&quot;
&quot;s so blind, but &quot;, &quot; so blind, but s&quot;
&quot;thy offices,\nSo &quot;, &quot;hy offices,\nSo r&quot;
&quot;ords, how plainl&quot;, &quot;rds, how plainly&quot;
&quot;IET:\nHere&#39;s such&quot;, &quot;ET:\nHere&#39;s such &quot;
&quot;wer\nTo take off &quot;, &quot;er\nTo take off s&quot;
&quot; hurry from the &quot;, &quot;hurry from the f&quot;
</code></pre>
<p>实现论文有趣的一点在于，模型“工作”有两个方面：编译（你的张量是否在各层之间匹配）和训练（损失是否下降）。<br>我们还要定义评估模型的方法。我们希望在定义模型之前就这样做，因为我们希望在训练模型时使用它来评估模型的性能。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">evaluate_loss</span>(</span><br><span class="line">    model: &amp;SimpleBrokenModel,</span><br><span class="line">    dataset: &amp;Tensor,</span><br><span class="line">    vocab: &amp;Vocab,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;HashMap&lt;<span class="type">String</span>, <span class="type">f32</span>&gt;&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">out</span> = HashMap::<span class="title function_ invoke__">new</span>();</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">split</span> <span class="keyword">in</span> [<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>] &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">losses</span> = <span class="type">Vec</span>::<span class="title function_ invoke__">new</span>();</span><br><span class="line">        <span class="keyword">for</span> <span class="variable">_</span> <span class="keyword">in</span> <span class="number">0</span>..<span class="number">10</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> (xs, ys) = <span class="title function_ invoke__">get_batches</span>(&amp;dataset, split, config.batch_size, config.context_length)?;</span><br><span class="line">            <span class="keyword">let</span> (_, loss) = model.forward(&amp;xs, <span class="title function_ invoke__">Some</span>(&amp;ys))?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss.<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line">            losses.<span class="title function_ invoke__">push</span>(loss.to_scalar::&lt;<span class="type">f32</span>&gt;()?);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">avg_loss</span> = losses.<span class="title function_ invoke__">iter</span>().sum::&lt;<span class="type">f32</span>&gt;() / losses.<span class="title function_ invoke__">len</span>() <span class="keyword">as</span> <span class="type">f32</span>;</span><br><span class="line">        out.<span class="title function_ invoke__">insert</span>(split.<span class="title function_ invoke__">to_owned</span>(), avg_loss);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(out)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="设置一个可以工作的简单模型"><a href="#设置一个可以工作的简单模型" class="headerlink" title="设置一个可以工作的简单模型"></a>设置一个可以工作的简单模型</h2><p>这是一个带有嵌入的基本前馈神经网络。它是我们将要开始的基础模型，然后我们将逐步替换其部分内容，直到最终得到 Llama 论文中描述的模型。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="comment">// 潜入层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 线性和激活层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;embeds)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果提供了targets就计算loss，不然视为推理，计算logits就可以。</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="comment">// 负的似然函数</span></span><br><span class="line">            <span class="comment">// -log(x) 越大，loss 越小</span></span><br><span class="line">            <span class="comment">// y =  [0, 0 , 0, 0, 1, ...,0,0]</span></span><br><span class="line">            <span class="comment">// y&#x27; = [4, 5,  6, 7, 8, ...,11,12 ]</span></span><br><span class="line">            <span class="comment">// 这个 cross_entropy 帮我们做了一个 log softmax</span></span><br><span class="line">            <span class="comment">// y&#x27; = [0.1, 0.12, 0.13, 0.64, ..., 0,0]</span></span><br><span class="line">            <span class="comment">// loss = -log(0.64)</span></span><br><span class="line">            <span class="comment">// 当 -log(q) = 4.17 q = 0.015 大概 1/64,vocab_size = 65,所以基本是在瞎猜。</span></span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// VarBuilder是用来构建参数的，我们目前不加载和保存模型参数，但是candle的用法必须基于这个。</span></span><br><span class="line">    <span class="comment">// vb.pp 会在参数树中加入参数的前缀，这样可以方便的查看参数的结构。</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// in fn main</span></span><br><span class="line">modeConfig.d_model = <span class="number">128</span>;</span><br><span class="line">modeConfig.batch_size = <span class="number">32</span>;</span><br><span class="line"><span class="keyword">let</span> (xs, ys) = <span class="title function_ invoke__">get_batches</span>(</span><br><span class="line">    &amp;dataset,</span><br><span class="line">    <span class="string">&quot;train&quot;</span>,</span><br><span class="line">    modeConfig.batch_size,</span><br><span class="line">    modeConfig.context_length,</span><br><span class="line">)?;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">varmap</span> = candle_nn::VarMap::<span class="title function_ invoke__">new</span>();</span><br><span class="line"><span class="keyword">let</span> <span class="variable">vb</span> = candle_nn::VarBuilder::<span class="title function_ invoke__">from_varmap</span>(&amp;varmap, DType::F32, &amp;Device::Cpu);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">model</span> = SimpleBrokenModel::<span class="title function_ invoke__">load</span>(vb, modeConfig)?;</span><br><span class="line"><span class="keyword">let</span> (logits, loss) = model.forward(&amp;xs, <span class="title function_ invoke__">Some</span>(&amp;ys))?;</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;:?&#125; &#123;:?&#125;&quot;</span>, logits, loss);</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">params_count</span>: <span class="type">usize</span> = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (name, var) <span class="keyword">in</span> varmap.<span class="title function_ invoke__">data</span>().<span class="title function_ invoke__">lock</span>().<span class="title function_ invoke__">unwrap</span>().<span class="title function_ invoke__">iter</span>() &#123;</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;: &#123;:?&#125;&quot;</span>, name, var.<span class="title function_ invoke__">elem_count</span>());</span><br><span class="line">    params_count += var.<span class="title function_ invoke__">elem_count</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;params count: &#123;&#125;&quot;</span>, params_count);</span><br></pre></td></tr></table></figure>

<pre><code>Tensor[dims 32, 16, 65; f32] Some(Tensor[5.266067; f32])
model.fc2.weight: 8320
model.embed_tokens.weight: 8320
model.fc1.bias: 128
model.fc2.bias: 65
model.fc1.weight: 16384
params count: 33217
</code></pre>
<p>在这一点上，我们必须开始关注张量的形状，并让矩阵的维度匹配。查看我们模型定义中的这一行：</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">    &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">    &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">)?;</span><br></pre></td></tr></table></figure>

<p>我们必须调整 <code>logits</code> 和 <code>targets</code> 张量的形状，以便在比较时它们的维度匹配。我们使用 <code>reshape</code> 方法来实现这一点。<br><code>()</code> 参数的意思是“从其他维度推断这个维度”。所以，在这种情况下，我们是在说“将 <code>logits</code> 和 <code>targets</code> 重新调整为具有相同行数的形状，并使用所需的列数来实现这一点”。这是处理批量数据时的常见模式。</p>
<p>让我们训练我们的 <code>SimpleBrokenModel</code> 以确保梯度流动。在确认这一点之后，我们可以替换它的部分内容以匹配 Llama，再次训练并跟踪我们的进展。在这一点上，我开始记录我的训练运行日志，这样如果我搞砸了，我可以轻松地回到之前的运行。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">train</span>(</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">    model: &amp;SimpleBrokenModel,</span><br><span class="line">    opt: &amp;<span class="keyword">mut</span> AdamW,</span><br><span class="line">    dataset: &amp;Tensor,</span><br><span class="line">    vocab: &amp;Vocab,</span><br><span class="line">) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;()&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">start_time</span> = std::time::Instant::<span class="title function_ invoke__">now</span>();</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">epoch</span> <span class="keyword">in</span> <span class="number">0</span>..config.epochs &#123;</span><br><span class="line">        <span class="keyword">let</span> (xs, ys) = <span class="title function_ invoke__">get_batches</span>(&amp;dataset, <span class="string">&quot;train&quot;</span>, config.batch_size, config.context_length)?;</span><br><span class="line">        <span class="keyword">let</span> (_, loss) = model.forward(&amp;xs, <span class="title function_ invoke__">Some</span>(&amp;ys))?;</span><br><span class="line">        opt.<span class="title function_ invoke__">backward_step</span>(&amp;loss.<span class="title function_ invoke__">unwrap</span>())?;</span><br><span class="line">        <span class="keyword">if</span> epoch % config.log_interval == <span class="number">0</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">batch_duration</span> = start_time.<span class="title function_ invoke__">elapsed</span>().<span class="title function_ invoke__">as_secs_f32</span>();</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = <span class="title function_ invoke__">evaluate_loss</span>(&amp;model, dataset, vocab, config)?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">val_loss</span> = loss.<span class="title function_ invoke__">get</span>(<span class="string">&quot;val&quot;</span>).<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">eta</span> = batch_duration * (config.epochs - epoch) <span class="keyword">as</span> <span class="type">f32</span>;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">eta</span> = eta.<span class="title function_ invoke__">round</span>();</span><br><span class="line">            <span class="built_in">println!</span>(</span><br><span class="line">                <span class="string">&quot;Epoch: &#123;epoch&#125; | Loss: &#123;val_loss&#125; | Time: &#123;batch_duration&#125; | ETA in seconds &#123;eta&#125;&quot;</span></span><br><span class="line">            );</span><br><span class="line">            start_time = time::Instant::<span class="title function_ invoke__">now</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(())</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// in fn main</span></span><br><span class="line">modeConfig.log_interval = <span class="number">10</span>;</span><br><span class="line">modeConfig.epochs = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">opt</span> = candle_nn::AdamW::<span class="title function_ invoke__">new</span>(varmap.<span class="title function_ invoke__">all_vars</span>(), ParamsAdamW::<span class="title function_ invoke__">default</span>())?;</span><br><span class="line"><span class="title function_ invoke__">train</span>(modeConfig, &amp;model, &amp;<span class="keyword">mut</span> opt, &amp;dataset, &amp;vocab)?;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">out</span> = <span class="title function_ invoke__">evaluate_loss</span>(&amp;model, &amp;dataset, &amp;vocab, modeConfig);</span><br><span class="line"><span class="built_in">println!</span>(<span class="string">&quot;&#123;:?&#125;&quot;</span>, out);</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 10 | Loss: 3.9159875 | Time: 6.5813394 | ETA in seconds 599
Epoch: 20 | Loss: 3.26492 | Time: 6.3639965 | ETA in seconds 515
Epoch: 30 | Loss: 2.9944448 | Time: 6.3596206 | ETA in seconds 452
Epoch: 40 | Loss: 2.8793342 | Time: 6.357106 | ETA in seconds 388
Epoch: 50 | Loss: 2.7827232 | Time: 6.3562865 | ETA in seconds 324
Epoch: 60 | Loss: 2.764416 | Time: 6.352279 | ETA in seconds 260
Epoch: 70 | Loss: 2.7196321 | Time: 6.356127 | ETA in seconds 197
Epoch: 80 | Loss: 2.7631993 | Time: 6.357493 | ETA in seconds 134
Epoch: 90 | Loss: 2.696882 | Time: 6.358631 | ETA in seconds 70
Epoch: 100 | Loss: 2.670012 | Time: 6.3603354 | ETA in seconds 6
Ok(&#123;&quot;train&quot;: 2.591057, &quot;val&quot;: 2.6625311&#125;)
</code></pre>
<!-- 注意我们得到的训练曲线几乎没有下降。我们怎么知道它几乎没有训练？
我们必须使用基本原理。训练前的交叉熵损失是4.17，1000个epoch后的损失是3.93。我们如何直观地理解这一点？ -->

<!-- 在这种情况下，交叉熵指的是我们选择错误单词的可能性。所以这里，

$$
H(T, q) = - \sum_{i = 1}^{N} \frac{1}{N} \log q(x_i)
$$

其中 $q(x_i)$ 是模型估计的选择正确单词的概率。如果 $q(x_i)$ 接近1，那么 $\log q$ 接近0；同样，如果 $q$ 很小，那么 $\log q$ 是一个大的负数，
所以 $-\log q$ 将是一个大的正数。现在建立直觉：开始时，$-\log q = 4.17$，所以 $q = 0.015$，大约是 $\frac{1}{64.715}$。
回想一下，词汇量 $|V| = 65$，所以我们基本上在说模型在选择下一个字母时和从我们的词汇表中随机选择一样好。
训练后，$-\log q = 3.93$，所以我们现在基本上在50个字母之间选择。这是一个非常小的改进，所以可能有问题。

为了直观地理解损失与模型性能的关系，想象模型在 $\tilde V$ 个标记之间选择；当 $\tilde V$ 很小时，模型更有可能猜对。此外，我们知道 $\max \tilde V = V$，这可以帮助我们理解我们的模型是否在学习。

$$\tilde V = \exp(L)$$

让我们尝试调试发生了什么。注意在我们的模型中，我们在logits上使用了softmax层，这是一个将数字向量压缩成概率分布的函数。但是对于使用内置的 `F.cross_entropy` 函数，我们需要直接传递[未归一化的logits](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)。所以让我们从模型中删除它并再次尝试。 -->
<!-- 
很好，现在我们的损失是 $2.54$，所以我们从 $12.67$ 个字符中进行选择。这比我们开始时的 65 个字符要好得多。让我们为我们的模型添加一个生成方法，这样我们就可以直观地看到模型的结果。 -->


<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">generate</span>(model: &amp;SimpleBrokenModel, vocab: &amp;Vocab, max_tokens: <span class="type">usize</span>) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;()&gt; &#123;</span><br><span class="line">    <span class="comment">// batch size 5, initial token = 0</span></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">token_ids</span> = Tensor::<span class="title function_ invoke__">zeros</span>((<span class="number">5</span>, <span class="number">1</span>), DType::U32, &amp;Device::Cpu).<span class="title function_ invoke__">unwrap</span>();</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">_</span> <span class="keyword">in</span> <span class="number">0</span>..max_tokens &#123;</span><br><span class="line">        <span class="keyword">let</span> (logits, _) = model.forward(&amp;token_ids, <span class="literal">None</span>)?;</span><br><span class="line">        <span class="built_in">assert!</span>(logits.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?, <span class="number">65</span>]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">last_step_logits</span> = logits.<span class="title function_ invoke__">i</span>((.., logits.<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)? - <span class="number">1</span>))?;</span><br><span class="line">        <span class="built_in">assert!</span>(last_step_logits.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, <span class="number">65</span>]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">probs</span> = <span class="title function_ invoke__">softmax</span>(&amp;last_step_logits, last_step_logits.<span class="title function_ invoke__">dims</span>().<span class="title function_ invoke__">len</span>() - <span class="number">1</span>)?;</span><br><span class="line">        <span class="built_in">assert!</span>(probs.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, <span class="number">65</span>]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">next_token</span> = probs.<span class="title function_ invoke__">argmax</span>(probs.<span class="title function_ invoke__">dims</span>().<span class="title function_ invoke__">len</span>() - <span class="number">1</span>)?;</span><br><span class="line">        <span class="built_in">assert!</span>(next_token.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [token_ids.<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?]);</span><br><span class="line">        token_ids = Tensor::<span class="title function_ invoke__">cat</span>(&amp;[token_ids, next_token.<span class="title function_ invoke__">reshape</span>(((), <span class="number">1</span>))?], <span class="number">1</span>)?;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">lines</span> = fs::<span class="title function_ invoke__">read_to_string</span>(<span class="string">&quot;./input.txt&quot;</span>).<span class="title function_ invoke__">expect</span>(<span class="string">&quot;Failed to read the file&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">v</span> <span class="keyword">in</span> &amp;token_ids.<span class="title function_ invoke__">to_vec2</span>()? &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">text</span> = vocab.<span class="title function_ invoke__">decode</span>(v);</span><br><span class="line">        <span class="built_in">println!</span>(<span class="string">&quot;&#123;&#125;&quot;</span>, text);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(())</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// fn in main</span></span><br><span class="line"><span class="title function_ invoke__">generate</span>(&amp;model, &amp;vocab, <span class="number">10</span>, device)?;</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;\nFind!\nD:\nAr t,\nLis sthte o t l&#39;,
 &#39;\nAnd ronnot ar\nBE:\nKINRDYOrspr;&#39;,
 &#39;\nI t athe momyengthend thanswal&#39;,
 &#39;\nFis t bp he\nLacarn.\nA:\nYOMI wi&#39;,
 &#39;\nWh ly sck\nB-de pll t\nHERIns ou&#39;]
</code></pre>
<p>这还算不错，但也不算太好。不过现在我们有了一个可以训练到验证损失的工作模型。因此，我们将在此基础上迭代我们的模型，使其更接近 Llama。</p>
<h2 id="Llama-具体细节"><a href="#Llama-具体细节" class="headerlink" title="Llama 具体细节"></a>Llama 具体细节</h2><p>Llama 对原始 Transformer 进行了三项架构修改：</p>
<ol>
<li>用于预归一化的 RMSNorm</li>
<li>旋转嵌入 RoPE</li>
<li>SwiGLU 激活函数</li>
</ol>
<p>我们将逐一添加每个修改到我们的基础模型，并进行迭代。</p>
<h3 id="RMSNorm"><a href="#RMSNorm" class="headerlink" title="RMSNorm"></a>RMSNorm</h3><p>在 Vaswani 2017 中，原始的 Transformer 使用了 BatchNormalization。在 Llama 中，作者使用了 RMSNorm，这是一种在不进行中心化的情况下通过方差来缩放向量的方法。此外，虽然 Vaswani 将归一化应用于注意力层的输出（后归一化），但 Llama 将其应用于输入之前（前归一化）。</p>
<p><a target="_blank" rel="noopener" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/tutorials/rmsnorm.html">这篇文章</a>对于RMSNorm有一个很好的解释。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pub</span> <span class="keyword">struct</span> <span class="title class_">RmsNorm</span> &#123;</span><br><span class="line">    scale: Tensor,</span><br><span class="line">    eps: <span class="type">f64</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">RmsNorm</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(size: <span class="type">usize</span>, vb: VarBuilder) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(RmsNorm &#123;</span><br><span class="line">            scale: vb.<span class="title function_ invoke__">get_with_hints</span>(size, <span class="string">&quot;weight&quot;</span>, Init::<span class="title function_ invoke__">Const</span>(<span class="number">1</span>.))?,</span><br><span class="line">            eps: <span class="number">1e-6</span>,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">pub</span> <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x_sqr</span> = x.<span class="title function_ invoke__">sqr</span>()?;</span><br><span class="line">        <span class="built_in">assert!</span>(x_sqr.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>());</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">norm_x</span> = (x.<span class="title function_ invoke__">mean</span>(D::Minus1)? + <span class="keyword">self</span>.eps)?.<span class="title function_ invoke__">sqrt</span>()?;</span><br><span class="line">        <span class="built_in">assert!</span>(norm_x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == [x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?, x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?]);</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x_normed</span> = x.<span class="title function_ invoke__">broadcast_div</span>(&amp;norm_x.<span class="title function_ invoke__">reshape</span>((</span><br><span class="line">            norm_x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">0</span>)?,</span><br><span class="line">            norm_x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dim</span>(<span class="number">1</span>)?,</span><br><span class="line">            (),</span><br><span class="line">        ))?)?;</span><br><span class="line">        <span class="built_in">assert!</span>(x_normed.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>() == x.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>());</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (x_normed.<span class="title function_ invoke__">broadcast_mul</span>(&amp;<span class="keyword">self</span>.scale))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="variable">varmap</span> = candle_nn::VarMap::<span class="title function_ invoke__">new</span>();</span><br><span class="line"><span class="keyword">let</span> <span class="variable">vb</span> = candle_nn::VarBuilder::<span class="title function_ invoke__">from_varmap</span>(&amp;varmap, DType::F32, &amp;Device::Cpu);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> <span class="variable">rms_norms</span> = RmsNorm::<span class="title function_ invoke__">new</span>(<span class="number">2</span>, vb)?;</span><br><span class="line"><span class="comment">// (2,3,2)</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable">batch</span> = Tensor::<span class="title function_ invoke__">new</span>(</span><br><span class="line">    <span class="built_in">vec!</span>[</span><br><span class="line">        <span class="built_in">vec!</span>[<span class="built_in">vec!</span>[<span class="number">1f32</span>, <span class="number">1f32</span>], <span class="built_in">vec!</span>[<span class="number">1.2f32</span>, <span class="number">2f32</span>], <span class="built_in">vec!</span>[<span class="number">3f32</span>, <span class="number">3f32</span>]],</span><br><span class="line">        <span class="built_in">vec!</span>[<span class="built_in">vec!</span>[<span class="number">4f32</span>, <span class="number">43f32</span>], <span class="built_in">vec!</span>[<span class="number">5f32</span>, <span class="number">5f32</span>], <span class="built_in">vec!</span>[<span class="number">61f32</span>, <span class="number">6f32</span>]],</span><br><span class="line">    ],</span><br><span class="line">    &amp;Device::Cpu,</span><br><span class="line">)?;</span><br><span class="line"><span class="keyword">let</span> <span class="variable">vb</span> = candle_nn::VarBuilder::<span class="title function_ invoke__">from_varmap</span>(&amp;varmap, DType::F32, &amp;Device::Cpu);</span><br><span class="line"><span class="keyword">let</span> <span class="variable">out</span> = rms_norms.forward(&amp;batch)?;</span><br></pre></td></tr></table></figure>

<pre><code>Tensor[dims 2, 3, 2; f32]
</code></pre>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    rms_norm: RmsNorm,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">SimpleBrokenModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="comment">// Embedding</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line">        <span class="comment">// RMSNorm</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">normed_embeds</span> = <span class="keyword">self</span>.rms_norm.forward(&amp;embeds)?;</span><br><span class="line">        <span class="comment">// Linear layers and activation</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;normed_embeds)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Calculate loss if targets are provided</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="comment">// 负的似然函数</span></span><br><span class="line">            <span class="comment">// log(x) 越大，loss 越小</span></span><br><span class="line">            <span class="comment">// y =  [0, 0 , 0, 0, 1, ...,0,0]</span></span><br><span class="line">            <span class="comment">// y&#x27; = [4, 5,  6, 7, 8, ...,11,12 ]</span></span><br><span class="line">            <span class="comment">// 这个 cross_entropy 帮我们做了一个 log softmax</span></span><br><span class="line">            <span class="comment">// y&#x27; = [0.1, 0.12, 0.13, 0.64, ..., 0,0]</span></span><br><span class="line">            <span class="comment">// loss = -log(0.64)</span></span><br><span class="line">            <span class="comment">// -log(q) = 4.17 q = 0.015 大概 1/64,vocab_size = 65,所以基本是在瞎猜。</span></span><br><span class="line">            <span class="comment">// println!(&quot;&#123;:?&#125;&quot;, targets.shape());</span></span><br><span class="line">            <span class="comment">// println!(&quot;&#123;:?&#125;&quot;, logits.shape());</span></span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// VarBuilder是用来构建参数的。</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_norm</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.rms_norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">            rms_norm,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 10 | Loss: 4.1559505 | Time: 6.779387 | ETA in seconds 617
Epoch: 20 | Loss: 4.14648 | Time: 6.7727704 | ETA in seconds 549
Epoch: 30 | Loss: 4.1364665 | Time: 6.776428 | ETA in seconds 481
Epoch: 40 | Loss: 4.125594 | Time: 6.772582 | ETA in seconds 413
Epoch: 50 | Loss: 4.120083 | Time: 6.7661977 | ETA in seconds 345
Epoch: 60 | Loss: 4.1099877 | Time: 6.760399 | ETA in seconds 277
Epoch: 70 | Loss: 4.0996284 | Time: 6.7623253 | ETA in seconds 210
Epoch: 80 | Loss: 4.0902996 | Time: 6.761824 | ETA in seconds 142
Epoch: 90 | Loss: 4.0833025 | Time: 6.76845 | ETA in seconds 74
Epoch: 100 | Loss: 4.070025 | Time: 6.7624454 | ETA in seconds 7
Ok(&#123;&quot;train&quot;: 4.072861, &quot;val&quot;: 4.0711236&#125;)
</code></pre>
<p>从这里得到的结果来看，范化以后，模型的表现并没有提升，所以我们需要继续迭代，只是梯度的下降变得比较平滑了。</p>
<h3 id="Rotary-Embeddings"><a href="#Rotary-Embeddings" class="headerlink" title="Rotary Embeddings"></a>Rotary Embeddings</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864.pdf">RoPE</a> 是一种用于 Transformer 的位置编码方法。在《Attention is All You Need》中，作者提出了两种位置编码方法：学习的和固定的。在 RoPE 中，作者通过旋转嵌入来表示序列中标记的位置，并在每个位置使用不同的旋转角度。</p>
<p>其中的 cos 和 sin 值可以预先计算并缓存，避免重复计算，后续会统一存放在一个缓存结构中。</p>
<p>RoPE 将 <code>hidden_state</code> 中每两个 x 组成的向量与旋转矩阵相乘来实现位置编码。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[x0, x1, .... ,xn]</span><br><span class="line">y0 = x0 * <span class="title function_ invoke__">cos</span>(theta) - x1 * <span class="title function_ invoke__">sin</span>(theta)</span><br><span class="line">y1 = x0 * <span class="title function_ invoke__">sin</span>(theta) + x1 * <span class="title function_ invoke__">cos</span>(theta)</span><br><span class="line">[y0, y1, ...., yn]</span><br></pre></td></tr></table></figure>
<p><code>n</code> 是 <code>d_model</code> 的一半。<br><code>theta</code> 是一个根据位置得到的固定值，计算公式为：<br><code>theta = m / 10000^(2i/n)</code><br>其中，<code>m</code> 是在序列中的位置，<code>i</code> 是在 <code>d_model</code> 中的位置。</p>
<p>这个公式的含义是将特征向量中的 <code>x0</code> 和 <code>x1</code> 进行一个固定的旋转，这个旋转不是通过学习得到的，而是预先计算的。它可以用于表示相对位置信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">```Rust</span><br><span class="line">pos_index = 0, 中的 x0,x1</span><br><span class="line">pos_index = 1, 中的 x0,x1</span><br></pre></td></tr></table></figure>
<p>隔了一个恒定的调度旋转。</p>
<p>freq_cis缓存住提前算好的cos和sin的值，这部分不用重复计算。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    cos: Tensor,</span><br><span class="line">    sin: Tensor,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(context_length: <span class="type">usize</span>, n_elem: <span class="type">usize</span>, vb: VarBuilder) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Cache&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">theta</span>: <span class="type">Vec</span>&lt;_&gt; = (<span class="number">0</span>..n_elem)</span><br><span class="line">            .<span class="title function_ invoke__">step_by</span>(<span class="number">2</span>)</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|i| <span class="number">1f32</span> / <span class="number">10000f32</span>.<span class="title function_ invoke__">powf</span>(i <span class="keyword">as</span> <span class="type">f32</span> / n_elem <span class="keyword">as</span> <span class="type">f32</span>))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">theta</span> = Tensor::<span class="title function_ invoke__">new</span>(theta.<span class="title function_ invoke__">as_slice</span>(), vb.<span class="title function_ invoke__">device</span>())?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">idx_theta</span> = Tensor::<span class="title function_ invoke__">arange</span>(<span class="number">0</span>, context_length <span class="keyword">as</span> <span class="type">u32</span>, vb.<span class="title function_ invoke__">device</span>())?</span><br><span class="line">            .<span class="title function_ invoke__">to_dtype</span>(DType::F32)?</span><br><span class="line">            .<span class="title function_ invoke__">reshape</span>((context_length, <span class="number">1</span>))?</span><br><span class="line">            .<span class="title function_ invoke__">matmul</span>(&amp;theta.<span class="title function_ invoke__">reshape</span>((<span class="number">1</span>, theta.<span class="title function_ invoke__">elem_count</span>()))?)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">freq_cis_real</span> = idx_theta.<span class="title function_ invoke__">cos</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">freq_cis_imag</span> = idx_theta.<span class="title function_ invoke__">sin</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = freq_cis_real.<span class="title function_ invoke__">reshape</span>((context_length, n_elem / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = freq_cis_imag.<span class="title function_ invoke__">reshape</span>((context_length, n_elem / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(Cache &#123; cos, sin&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>rope计算的时候</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">apply_rotary_emb</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">    <span class="keyword">let</span> (b_sz, seq_len, n_embd) = x.<span class="title function_ invoke__">dims3</span>()?;</span><br><span class="line">    <span class="comment">// println!(&quot;shape of cache.cos &#123;:?&#125;&quot;, cache.cos.shape());</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">cos</span> = cache.cos.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">sin</span> = cache.sin.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">    <span class="comment">// println!(&quot;cos shape &#123;:?&#125;&quot;, cos.shape());</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">cos</span> = cos.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">sin</span> = sin.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">    <span class="comment">// println!(&quot;broadcast cos shape &#123;:?&#125;&quot;, cos.shape());</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x</span> = x.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_embd / <span class="number">2</span>, <span class="number">2</span>))?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x0</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">0</span>, <span class="number">1</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">x1</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">1</span>, <span class="number">1</span>)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">dst0</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)? - x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)?)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">dst1</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)? + x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)?)?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">rope</span> = Tensor::<span class="title function_ invoke__">cat</span>(&amp;[&amp;dst0, &amp;dst1], D::Minus1)?.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_embd))?;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(rope)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h3><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    rms_norm: RmsNorm,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">    self_attention: SelfAttention,</span><br><span class="line">    cache: Cache,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="comment">// 嵌入层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line">        <span class="comment">// 范化层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">normed_embeds</span> = <span class="keyword">self</span>.rms_norm.forward(&amp;embeds)?;</span><br><span class="line">        <span class="comment">// 自注意力层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.self_attention.forward(&amp;normed_embeds, &amp;<span class="keyword">self</span>.cache)?;</span><br><span class="line">        <span class="comment">// 线性和激活层</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;normed_embeds)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="comment">// 负的似然函数</span></span><br><span class="line">            <span class="comment">// -log(x) 越大，loss 越小</span></span><br><span class="line">            <span class="comment">// y =  [0, 0 , 0, 0, 1, ...,0,0]</span></span><br><span class="line">            <span class="comment">// y&#x27; = [4, 5,  6, 7, 8, ...,11,12 ]</span></span><br><span class="line">            <span class="comment">// 这个 cross_entropy 帮我们做了一个 log softmax</span></span><br><span class="line">            <span class="comment">// y&#x27; = [0.1, 0.12, 0.13, 0.64, ..., 0,0]</span></span><br><span class="line">            <span class="comment">// loss = -log(0.64)</span></span><br><span class="line">            <span class="comment">// 例如 -log(q) = 4.17 q = 0.015 大概 1/64,vocab_size = 65,所以基本是在瞎猜。</span></span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_norm</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.rms_norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">self_attention</span> = SelfAttention::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.self_attention&quot;</span>), config)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">            rms_norm,</span><br><span class="line">            self_attention,</span><br><span class="line">            cache: Cache::<span class="title function_ invoke__">new</span>(config.context_length, config.d_model, vb)?,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>提示：了解训练时张量维度与推理时张量维度的区别。</p>
</blockquote>
<p>虽然在训练时，你可以期望张量维度与模型参数紧密匹配，例如 <code>batch.shape = (config[&#39;batch_size&#39;], config[&#39;context_window&#39;], config[&#39;d_model&#39;])</code>，但在推理时，你可能需要处理单个示例，例如 <code>batch.shape = (1, 1, config[&#39;d_model&#39;])</code>。因此，你需要确保在 <code>forward</code> 传递中进行索引时，使用从输入派生的形状，而不一定是模型参数。</p>
<h3 id="MultiHeadRopeAttention"><a href="#MultiHeadRopeAttention" class="headerlink" title="MultiHeadRopeAttention"></a>MultiHeadRopeAttention</h3><p>让我们为这个单一的注意力头设置一个多头注意力层，看看训练时会发生什么。</p>
<p>这里实现的是GQA的注意力头。<code>n_kv_head=1</code>时就是MQA，<code>n_kv_head&gt;1</code>且<code>n_kv_head&lt;n_head</code>时就是GQA，<code>n_kv_head=n_head</code>时就是原本的MHA。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    q_proj: Linear,</span><br><span class="line">    k_proj: Linear,</span><br><span class="line">    v_proj: Linear,</span><br><span class="line">    o_proj: Linear,</span><br><span class="line">    n_head: <span class="type">usize</span>,</span><br><span class="line">    n_kv_head: <span class="type">usize</span>,</span><br><span class="line">    head_dim: <span class="type">usize</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.q_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.k_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.v_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">o_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.o_proj&quot;</span>))?;</span><br><span class="line">        <span class="built_in">println!</span>(</span><br><span class="line">            <span class="string">&quot;MHA config n_head &#123;&#125; n_kv_head &#123;&#125;&quot;</span>,</span><br><span class="line">            config.n_head, config.n_kv_head</span><br><span class="line">        );</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            q_proj,</span><br><span class="line">            k_proj,</span><br><span class="line">            v_proj,</span><br><span class="line">            o_proj,</span><br><span class="line">            n_head: config.n_head,</span><br><span class="line">            n_kv_head: config.n_kv_head,</span><br><span class="line">            head_dim: config.d_model / config.n_head,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">apply_rotary_emb</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (b_sz, seq_len, h, n_embd) = x.<span class="title function_ invoke__">dims4</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = cache.cos.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = cache.sin.<span class="title function_ invoke__">i</span>(..seq_len)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = cos.<span class="title function_ invoke__">unsqueeze</span>(<span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = sin.<span class="title function_ invoke__">unsqueeze</span>(<span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">cos</span> = cos.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, <span class="number">1</span>, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">sin</span> = sin.<span class="title function_ invoke__">broadcast_as</span>((b_sz, seq_len, <span class="number">1</span>, n_embd / <span class="number">2</span>, <span class="number">1</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = x.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, h, n_embd / <span class="number">2</span>, <span class="number">2</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x0</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">0</span>, <span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x1</span> = x.<span class="title function_ invoke__">narrow</span>(D::Minus1, <span class="number">1</span>, <span class="number">1</span>)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">dst0</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)? - x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)?)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">dst1</span> = (x0.<span class="title function_ invoke__">broadcast_mul</span>(&amp;sin)? + x1.<span class="title function_ invoke__">broadcast_mul</span>(&amp;cos)?)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rope</span> = Tensor::<span class="title function_ invoke__">cat</span>(&amp;[&amp;dst0, &amp;dst1], D::Minus1)?.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, h, n_embd))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(rope)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (b_sz, seq_len, n_embd) = x.<span class="title function_ invoke__">dims3</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算 q k v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.q_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.k_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.v_proj.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">assert!</span>(n_embd == <span class="keyword">self</span>.n_head * <span class="keyword">self</span>.head_dim);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对  q  和 k 做位置编码</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;q, cache)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;k, cache)?;</span><br><span class="line">        <span class="comment">// 复制成 n_head / n_kv_head 份</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(k)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(v)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 把 seq_len 和 n_head 交换</span></span><br><span class="line">        <span class="comment">// 这转换一下是为了做一个 cat single head 的简单操作</span></span><br><span class="line">        <span class="comment">// 相当于 n_head 个的seq_len*seq_len的注意力。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// q*k^T / sqrt(d_k) d_k = d_model</span></span><br><span class="line">        <span class="comment">// 这里是 (bs,n_head) 个 (seq_len, seq_len) 的注意力</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = (q.<span class="title function_ invoke__">matmul</span>(&amp;k.<span class="title function_ invoke__">t</span>()?)? / (<span class="keyword">self</span>.head_dim <span class="keyword">as</span> <span class="type">f64</span>).<span class="title function_ invoke__">sqrt</span>())?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里是头内的softmax (seq_len,seq_len)的行总和为1</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = <span class="title function_ invoke__">softmax</span>(&amp;attn, D::Minus1)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 再乘 * (bs, n_head, seq_len, head_dim) 得到 (bs, n_head）个注意力头对应的加权的v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = attn.<span class="title function_ invoke__">matmul</span>(&amp;v)?;</span><br><span class="line">        <span class="comment">// 把 n_head 和 seq_len 交换回来，得到 (bs, seq_len, n_head, head_dim) 然后reshape以后</span></span><br><span class="line">        <span class="comment">// 得到 (bs, seq_len, n_head * head_dim) 把头给cat到一起。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = y.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">reshape</span>(&amp;[b_sz, seq_len, n_embd])?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.o_proj.forward(&amp;y)?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(y)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">repeat_kv</span>(&amp;<span class="keyword">self</span>, x: Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">n_rep</span> = <span class="keyword">self</span>.n_head / <span class="keyword">self</span>.n_kv_head;</span><br><span class="line">        <span class="keyword">if</span> n_rep == <span class="number">1</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> (b_sz, seq_len, n_kv_head, head_dim) = x.<span class="title function_ invoke__">dims4</span>()?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">x</span> = x</span><br><span class="line">                .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">3</span>)?</span><br><span class="line">                .<span class="title function_ invoke__">expand</span>((b_sz, seq_len, n_kv_head, n_rep, head_dim))?</span><br><span class="line">                .<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_kv_head * n_rep, head_dim))?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>完整模型</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    mlp: Sequential,</span><br><span class="line">    rms_norm: RmsNorm,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">    self_attention: MultiHeadAttention,</span><br><span class="line">    cache: Cache,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">AttentionModel</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embeds</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">normed_embeds</span> = <span class="keyword">self</span>.rms_norm.forward(&amp;embeds)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.self_attention.forward(&amp;normed_embeds, &amp;<span class="keyword">self</span>.cache)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.mlp.forward(&amp;y)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(<span class="keyword">Self</span>)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embedding</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_norm</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.rms_norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">self_attention</span> = MultiHeadAttention::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.multi_head_attention&quot;</span>), config)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mlp</span> = sequential::<span class="title function_ invoke__">seq</span>()</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.d_model,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc1&quot;</span>),</span><br><span class="line">            )?)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(Activation::Relu)</span><br><span class="line">            .<span class="title function_ invoke__">add</span>(<span class="title function_ invoke__">linear</span>(</span><br><span class="line">                config.d_model,</span><br><span class="line">                config.vocab_size,</span><br><span class="line">                vb.<span class="title function_ invoke__">push_prefix</span>(<span class="string">&quot;model.fc2&quot;</span>),</span><br><span class="line">            )?);</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding,</span><br><span class="line">            mlp,</span><br><span class="line">            config,</span><br><span class="line">            rms_norm,</span><br><span class="line">            self_attention,</span><br><span class="line">            cache: Cache::<span class="title function_ invoke__">new</span>(config.context_length, config.d_model/config.n_head, vb)?,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_ invoke__">generate</span>(&amp;model, &amp;vocab, <span class="number">10</span>, device)?;</span><br></pre></td></tr></table></figure>




<pre><code>[&#39;\n\n\n\n\n\n\n\nI\n\nOOOOOOOOOFOOtOOOOOOO&#39;,
 &#39;\nIIIIII IIIIIIIIIIIIIIIIIIIIIII&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naaame&#39;,
 &#39;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&#39;]
</code></pre>
<p>所以看起来很糟糕。这里发生了什么？让我们通过查看注意力来开始调试。</p>
<p>目前的注意力是没有masked的，任何位置的字符都在关注任何其他位置的字符。<br>这有什么不好呢？我们试图仅基于之前的标记来预测下一个标记，但这里我们看到模型正在关注之后的标记。<br>换句话说，模型在作弊，或者从未来泄露信息。这是一个问题，这就是为什么我们需要使用因果掩码。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">pub</span> <span class="keyword">struct</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// mask 也可以 cached</span></span><br><span class="line">    mask: Tensor,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Cache</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">new</span>(context_length: <span class="type">usize</span>, n_elem: <span class="type">usize</span>, vb: VarBuilder) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Cache&gt; &#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        <span class="comment">// _ 表示类型由编译器推断，</span></span><br><span class="line">        <span class="comment">// 默认的collect 是 [_]，但是这个大小是不可变的要编译期间决定</span></span><br><span class="line">        <span class="comment">// 所以这里还是要提示编译器要 collect 成 vec.</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mask</span>: <span class="type">Vec</span>&lt;_&gt; = (<span class="number">0</span>..context_length)</span><br><span class="line">            .<span class="title function_ invoke__">flat_map</span>(|i| (<span class="number">0</span>..context_length).<span class="title function_ invoke__">map</span>(<span class="keyword">move</span> |j| <span class="type">u8</span>::<span class="title function_ invoke__">from</span>(j &gt; i)))</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mask</span> = Tensor::<span class="title function_ invoke__">from_slice</span>(&amp;mask, (context_length, context_length), vb.<span class="title function_ invoke__">device</span>())?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(Cache &#123; cos, sin, mask &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    q_proj: Linear,</span><br><span class="line">    k_proj: Linear,</span><br><span class="line">    v_proj: Linear,</span><br><span class="line">    o_proj: Linear,</span><br><span class="line">    n_head: <span class="type">usize</span>,</span><br><span class="line">    n_kv_head: <span class="type">usize</span>,</span><br><span class="line">    head_dim: <span class="type">usize</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">MultiHeadAttention</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.q_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.k_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v_proj</span> = <span class="title function_ invoke__">linear</span>(</span><br><span class="line">            config.d_model,</span><br><span class="line">            (config.d_model / config.n_head) * config.n_kv_head,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.v_proj&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">o_proj</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.o_proj&quot;</span>))?;</span><br><span class="line">        <span class="built_in">println!</span>(</span><br><span class="line">            <span class="string">&quot;MHA config n_head &#123;&#125; n_kv_head &#123;&#125;&quot;</span>,</span><br><span class="line">            config.n_head, config.n_kv_head</span><br><span class="line">        );</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            q_proj,</span><br><span class="line">            k_proj,</span><br><span class="line">            v_proj,</span><br><span class="line">            o_proj,</span><br><span class="line">            n_head: config.n_head,</span><br><span class="line">            n_kv_head: config.n_kv_head,</span><br><span class="line">            head_dim: config.d_model / config.n_head,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (b_sz, seq_len, n_embd) = x.<span class="title function_ invoke__">dims3</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算 q k v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.q_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.k_proj.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.v_proj.forward(x)?;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">assert!</span>(n_embd == <span class="keyword">self</span>.n_head * <span class="keyword">self</span>.head_dim);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, <span class="keyword">self</span>.n_kv_head, <span class="keyword">self</span>.head_dim))?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对  q  和 k 做位置编码</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;q, cache)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">apply_rotary_emb</span>(&amp;k, cache)?;</span><br><span class="line">        <span class="comment">// 复制成 n_head / n_kv_head 份</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(k)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = <span class="keyword">self</span>.<span class="title function_ invoke__">repeat_kv</span>(v)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// println!(&quot;q.shape &#123;:?&#125;&quot;, q.shape());</span></span><br><span class="line">        <span class="comment">// 把 seq_len 和 n_head 交换</span></span><br><span class="line">        <span class="comment">// 这转换一下是为了做一个 cat single head 的简单操作</span></span><br><span class="line">        <span class="comment">// 相当于 n_head 个的seq_len*seq_len的注意力。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">q</span> = q.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">k</span> = k.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">v</span> = v.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">contiguous</span>()?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// let tmp = q.matmul(&amp;k.t()?)?;</span></span><br><span class="line">        <span class="comment">// 这个结果是有负数的，但是注意力层不会有负数。</span></span><br><span class="line">        <span class="comment">// 计算结果出了很多 NaN，感觉应该是范化没做好。</span></span><br><span class="line">        <span class="comment">// 后面发现是没有用 sqrt 用了开方，导致负数变成了NaN。</span></span><br><span class="line">        <span class="comment">// q*k^T / sqrt(d_k) d_k = d_model</span></span><br><span class="line">        <span class="comment">// 这里是 (bs,n_head) 个 (seq_len, seq_len) 的注意力</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = (q.<span class="title function_ invoke__">matmul</span>(&amp;k.<span class="title function_ invoke__">t</span>()?)? / (<span class="keyword">self</span>.head_dim <span class="keyword">as</span> <span class="type">f64</span>).<span class="title function_ invoke__">sqrt</span>())?;</span><br><span class="line">        <span class="comment">// 在 softmax 之前，把未来的token位置变为负无穷，这样在softmax之后，这些位置的概率就会变为0</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">mask</span> = cache</span><br><span class="line">            .mask</span><br><span class="line">            .<span class="title function_ invoke__">i</span>((..seq_len, ..seq_len))?</span><br><span class="line">            .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">0</span>)?</span><br><span class="line">            .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">0</span>)?</span><br><span class="line">            .<span class="title function_ invoke__">broadcast_as</span>(attn.<span class="title function_ invoke__">shape</span>())?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">on_true</span> =</span><br><span class="line">            Tensor::<span class="title function_ invoke__">new</span>(<span class="type">f32</span>::NEG_INFINITY, attn.<span class="title function_ invoke__">device</span>())?.<span class="title function_ invoke__">broadcast_as</span>(mask.<span class="title function_ invoke__">shape</span>().<span class="title function_ invoke__">dims</span>())?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = mask.<span class="title function_ invoke__">where_cond</span>(&amp;on_true, &amp;attn)?;</span><br><span class="line">        <span class="comment">// 取一个例子</span></span><br><span class="line">        <span class="comment">// 这里是头内的softmax (seq_len,seq_len)的每行 (seq_len) 总和为1</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = <span class="title function_ invoke__">softmax</span>(&amp;attn, D::Minus1)?;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 再乘 * (bs, n_head, seq_len, head_dim) 得到 (bs, n_head）个注意力头对应的加权的v</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = attn.<span class="title function_ invoke__">matmul</span>(&amp;v)?;</span><br><span class="line">        <span class="comment">// 把 n_head 和 seq_len 交换回来，得到 (bs, seq_len, n_head, head_dim) 然后reshape以后</span></span><br><span class="line">        <span class="comment">// 得到 (bs, seq_len, n_head * head_dim) 把头给cat到一起。</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = y.<span class="title function_ invoke__">transpose</span>(<span class="number">1</span>, <span class="number">2</span>)?.<span class="title function_ invoke__">reshape</span>(&amp;[b_sz, seq_len, n_embd])?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">y</span> = <span class="keyword">self</span>.o_proj.forward(&amp;y)?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(y)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">repeat_kv</span>(&amp;<span class="keyword">self</span>, x: Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">n_rep</span> = <span class="keyword">self</span>.n_head / <span class="keyword">self</span>.n_kv_head;</span><br><span class="line">        <span class="keyword">if</span> n_rep == <span class="number">1</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">let</span> (b_sz, seq_len, n_kv_head, head_dim) = x.<span class="title function_ invoke__">dims4</span>()?;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">x</span> = x</span><br><span class="line">                .<span class="title function_ invoke__">unsqueeze</span>(<span class="number">3</span>)?</span><br><span class="line">                .<span class="title function_ invoke__">expand</span>((b_sz, seq_len, n_kv_head, n_rep, head_dim))?</span><br><span class="line">                .<span class="title function_ invoke__">reshape</span>((b_sz, seq_len, n_kv_head * n_rep, head_dim))?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>现在，我们可以让注意力激活的上三角部分（对应未来的部分）几乎被归零了。让我们看看训练时会发生什么。</p>
<h3 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h3><p>正如论文中所述，“我们用SwiGLU激活函数替换了ReLU非线性函数……我们使用$\frac{2}{3} 4d$的维度，而不是PaLM中的$4d$。” SwiGLU定义为：<br>$$<br>\text{SwiGLU}(x) = \text{Swish}_\beta (xW + b) \otimes (xV + c)<br>$$<br>其中$\otimes$是逐元素乘积。Swish函数定义为：<br>$$<br>\text{Swish}_\beta(x) = x \sigma(\beta x)<br>$$<br>其中$\beta$是一个可学习的参数。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">silu</span>(xs: &amp;Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">    xs / (xs.<span class="title function_ invoke__">neg</span>()?.<span class="title function_ invoke__">exp</span>()? + <span class="number">1.0</span>)?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">SwiGLU</span> &#123;</span><br><span class="line">    c_fc1: Linear,</span><br><span class="line">    c_fc2: Linear,</span><br><span class="line">    c_proj: Linear,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 新的 mlp 是三层的带gate</span></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">SwiGLU</span> &#123;</span><br><span class="line">    <span class="comment">// silu 的特征是允许有一点点的负数</span></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="comment">// 这里就是 SwiGLU SiLU(W_1 * x) * (W_2 * x) 是 element wise 的，这个可以作为gate门信号</span></span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (<span class="title function_ invoke__">silu</span>(&amp;<span class="keyword">self</span>.c_fc1.forward(x)?)? * <span class="keyword">self</span>.c_fc2.forward(x)?)?;</span><br><span class="line">        <span class="keyword">self</span>.c_proj.forward(&amp;x)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, cfg: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">h_size</span> = cfg.d_model;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">i_size</span> = cfg.hidden_dim;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">c_fc1</span> = <span class="title function_ invoke__">linear</span>(h_size, i_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;gate_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">c_fc2</span> = <span class="title function_ invoke__">linear</span>(h_size, i_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;up_proj&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">c_proj</span> = <span class="title function_ invoke__">linear</span>(i_size, h_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;down_proj&quot;</span>))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            c_fc1,</span><br><span class="line">            c_fc2,</span><br><span class="line">            c_proj,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一个llama block res 两次，一次在attention之前，一次在swiglu之前。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Block</span> &#123;</span><br><span class="line">    rms_1: RmsNorm,</span><br><span class="line">    attn: MultiHeadAttention,</span><br><span class="line">    rms_2: RmsNorm,</span><br><span class="line">    swiglu: SwiGLU,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Block</span> &#123;</span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">forward</span>(&amp;<span class="keyword">self</span>, x: &amp;Tensor, cache: &amp;Cache) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;Tensor&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">residual</span> = x;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = <span class="keyword">self</span>.rms_1.forward(x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (<span class="keyword">self</span>.attn.forward(&amp;x, cache)? + residual)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">residual</span> = &amp;x;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = (<span class="keyword">self</span>.swiglu.forward(&amp;<span class="keyword">self</span>.rms_2.forward(&amp;x)?)? + residual)?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(x)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, cfg: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">attn</span> = MultiHeadAttention::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;self_attn&quot;</span>), cfg)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">swiglu</span> = SwiGLU::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;mlp&quot;</span>), cfg)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_1</span> = RmsNorm::<span class="title function_ invoke__">new</span>(cfg.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;input_layernorm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">rms_2</span> = RmsNorm::<span class="title function_ invoke__">new</span>(cfg.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;post_attention_layernorm&quot;</span>))?;</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            rms_1,</span><br><span class="line">            attn,</span><br><span class="line">            rms_2,</span><br><span class="line">            swiglu,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>现在，让我们通过创建块来添加多个层的最后完整的模型。</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Llama</span> &#123;</span><br><span class="line">    embedding: Embedding,</span><br><span class="line">    blocks: <span class="type">Vec</span>&lt;Block&gt;,</span><br><span class="line">    ln_f: RmsNorm,</span><br><span class="line">    lm_head: Linear,</span><br><span class="line">    cache: Cache,</span><br><span class="line">    config: ModelConfig,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">impl</span> <span class="title class_">Llama</span> &#123;</span><br><span class="line">    <span class="keyword">pub</span> <span class="keyword">fn</span> <span class="title function_">forward</span>(</span><br><span class="line">        &amp;<span class="keyword">self</span>,</span><br><span class="line">        x: &amp;Tensor,</span><br><span class="line">        targets: <span class="type">Option</span>&lt;&amp;Tensor&gt;,</span><br><span class="line">    ) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;(Tensor, <span class="type">Option</span>&lt;Tensor&gt;)&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> (_b_sz, _seq_len) = x.<span class="title function_ invoke__">dims2</span>()?;</span><br><span class="line">        <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">x</span> = <span class="keyword">self</span>.embedding.forward(x)?;</span><br><span class="line">        <span class="keyword">for</span> <span class="variable">block</span> <span class="keyword">in</span> &amp;<span class="keyword">self</span>.blocks &#123;</span><br><span class="line">            x = block.forward(&amp;x, &amp;<span class="keyword">self</span>.cache)?;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = <span class="keyword">self</span>.ln_f.forward(&amp;x)?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">logits</span> = <span class="keyword">self</span>.lm_head.forward(&amp;x)?;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Some</span>(targets) = targets &#123;</span><br><span class="line">            <span class="keyword">let</span> <span class="variable">loss</span> = loss::<span class="title function_ invoke__">cross_entropy</span>(</span><br><span class="line">                &amp;logits.<span class="title function_ invoke__">reshape</span>(((), <span class="keyword">self</span>.config.vocab_size))?,</span><br><span class="line">                &amp;targets.<span class="title function_ invoke__">reshape</span>(((),))?,</span><br><span class="line">            )?;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="title function_ invoke__">Some</span>(loss)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="title function_ invoke__">Ok</span>((logits, <span class="literal">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">pub</span> <span class="keyword">fn</span> <span class="title function_">load</span>(vb: VarBuilder, config: ModelConfig) <span class="punctuation">-&gt;</span> <span class="type">Result</span>&lt;<span class="keyword">Self</span>&gt; &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">embed_layer</span> = <span class="title function_ invoke__">embedding</span>(</span><br><span class="line">            config.vocab_size,</span><br><span class="line">            config.d_model,</span><br><span class="line">            vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.embed_tokens&quot;</span>),</span><br><span class="line">        )?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">lm_head</span> = <span class="title function_ invoke__">linear</span>(config.d_model, config.vocab_size, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;lm_head&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">ln_f</span> = RmsNorm::<span class="title function_ invoke__">new</span>(config.d_model, vb.<span class="title function_ invoke__">pp</span>(<span class="string">&quot;model.norm&quot;</span>))?;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">blocks</span>: <span class="type">Vec</span>&lt;_&gt; = (<span class="number">0</span>..config.n_layers)</span><br><span class="line">            .<span class="title function_ invoke__">map</span>(|i| Block::<span class="title function_ invoke__">load</span>(vb.<span class="title function_ invoke__">pp</span>(<span class="built_in">format!</span>(<span class="string">&quot;model.layers.&#123;i&#125;&quot;</span>)), config).<span class="title function_ invoke__">unwrap</span>())</span><br><span class="line">            .<span class="title function_ invoke__">collect</span>();</span><br><span class="line">        <span class="title function_ invoke__">Ok</span>(<span class="keyword">Self</span> &#123;</span><br><span class="line">            embedding: embed_layer,</span><br><span class="line">            blocks,</span><br><span class="line">            ln_f,</span><br><span class="line">            lm_head,</span><br><span class="line">            config,</span><br><span class="line">            cache: Cache::<span class="title function_ invoke__">new</span>(config.context_length, config.d_model / config.n_head, vb)?,</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>这里主要是训练的部分，在推理的过程中还涉及到一个比较重要的kv cache。<br>kv cache主要是缓存自回归过程中的kv，这个kv是不变的，因为这个k 和 v 只和之前的token有关系，所以可以缓存下来，<br>这样在推理的时候就不用重复计算了。<br>围绕着prompt产生第一个Token的prefill的阶段和计算完prompt之后的decode阶段也是目前业界比较关注的推理优化的方向。<br>源代码在<a target="_blank" rel="noopener" href="https://github.com/ggaaooppeenngg/rust-llama-from-scratch">这里</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">vLLM 分析 3 推理优化</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-13 17:14:56" itemprop="dateCreated datePublished" datetime="2024-12-13T17:14:56+08:00">2024-12-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/13/vLLM-分析-3-推理优化/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇文章主要基于vLLM中做推理的优化做一个总结。</p>
<h2 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h2><p>从<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.06180">原始论文来看</a>，显存的浪费主要有几种。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/memory_waste.jpeg" class="">

<p>这张图里面表示的是每个token对应的kvcache的slot。对于一个context长度中用不到的slots部分，有预留的slots和不同空隙之间的slots空隙。</p>
<p>vLLM参考了虚拟内存和内存页分配的逻辑构造了一个block table用于 kv cache block slots 和 token之间的关系。通过block表的管理<br>可以像操作系统一下减少内存碎片。<br>因为不同的sequence中token是有位置信息的，所有他们对应的kv slot也不一定一样。下图展示了他们的关系。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/block_table.png" class="">

<h2 id="Continuous-Batching"><a href="#Continuous-Batching" class="headerlink" title="Continuous Batching"></a>Continuous Batching</h2><p>“连续批量处理”（Continuous Batching），也称为”动态批量处理”（Dynamic Batching）或”迭代级调度批量处理”（Batching with Iteration-Level Scheduling），是一种选择Batch行的技术，用于优化计算和资源利用率。<br>它的主要区别在于与静态批处理相比，它会在每次推理迭代过程中动态调整Batch中的序列，例如vLLM会抢占部分序列，将序列的prefill阶段和decode阶段分开，不在同一个batch中处理。</p>
<p>而HuggingFace的text-generation-inference的router<a target="_blank" rel="noopener" href="https://github.com/huggingface/text-generation-inference/blob/main/router/README.md">文档</a>中提到的：为了提高效率，特别是在文本生成和内存受限的LLM上，可以让客户端发送单个查询，然后路由器将这些查询合并或分离成批次，以最大限度地利用计算资源。这是因为在LLM中，运行模型的计算成本远远高于批处理的成本。当新请求到达时，当前正在<code>forward</code>的前向传播不中断，而是继续等待执行完毕。然后将当前正在处理的请求与新到的请求合并成一个批处理请求，再进行<code>forward</code>前向传播。在批处理请求中，任何一个请求完成（即模型产生了终止符或达到允许的最大长度），则从批处理请求中移除该请求，并释放相关资源。这种方法可以应用于多个请求，并且支持在不同参数下的处理（例如采样、不采样、温度控制等），因为每个请求在批处理中都可以独立进行处理。Anyscale 对这个过程有很好的<a target="_blank" rel="noopener" href="https://www.anyscale.com/blog/continuous-batching-llm-inference">解释</a>。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/continous_batching.png" class="">

<h3 id="Prefill-和-Decode"><a href="#Prefill-和-Decode" class="headerlink" title="Prefill 和 Decode"></a>Prefill 和 Decode</h3><p>在LLM推理过程中，一个Prompt的第一次执行（称为prefill）和后续的前向传播（称为decode）是不同的。Prefill阶段需要计算整个注意力矩阵并将其缓存到KV缓存中，计算规模较大，尤其是对于长度为10K或100K的提示词。而在decode阶段，只需计算新生成的token的注意力矩阵，计算规模较小。</p>
<p>从Kimi的<a target="_blank" rel="noopener" href="https://arxiv.org/html/2407.00079v2">Mooncake</a>论文中的图片来看：</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/prefill_decode.png" class="">

<p>左图显示，当prompt长度增加时，计算时间呈平方级别增加。右图显示，decode阶段只生成一个token，计算规模比线性增长还要慢一点，但由于需要复用之前的KV缓存，因此显存开销较大。Mooncake的解决方案是将prefill和decode分开处理，prefill计算规模大，decode计算规模小，通过KV缓存共享机制传递KV缓存。论文还提到了一些关于缓存调度的细节，这里不再展开。</p>
<p>在Prefill阶段，所有的提示词（prompt）都是已知的，因此可以并行计算多个token，计算并行度较高。而在Decode阶段，只需根据新生成的token计算下一个token（之前的KV缓存已经保存了中间结果），因此计算规模较小。如果将Prefill和Decode放在同一个batch中计算，由于计算规模不对等，容易产生计算空隙（bubble）。</p>
<p>Prefill像是一个矩阵和矩阵的乘法，而Decode则是一个向量和矩阵的乘法。</p>
<h2 id="Chunked-Prefill"><a href="#Chunked-Prefill" class="headerlink" title="Chunked Prefill"></a>Chunked Prefill</h2><p>vLLM的<a target="_blank" rel="noopener" href="https://docs.google.com/document/d/1Z9TvqzzBPnh5WHcRwjvK2UEeFeq5zMZb5mFE8jR0HCs/edit?pli=1&tab=t.0">文档</a>很好的解释了prefill阶段和decode阶段的区别。</p>
<p>考虑到vLLM进行生成的序列“ABC”。当它到达时，KV缓存基于block size的预设值（这里是2）在内存中分配对应的block（B1，B2，B3，B4），但它是空的。<br>我们知道序列的内容（A，B，C），但我们没有token id到块索引的映射。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/prefill_1.png" class="">

<p>考虑到这种情况，下一步是为序列ABC进行prefill。在调度过程中，我们为序列中每个token块分配块索引（B3，B4），即（[A, B], [C, _]）。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/prefill_2.png" class="">

<p>一旦确定了块映射，我们就可以通过运行模型的前向传播。这会将ABC token的KV激活值写入KV缓存中的相应位置。此外，前向传播将导致新token “D” 被采样。D的KV值尚未知晓。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/prefill_3.png" class="">

<p>现在，序列已经完成了预填充。我们可以安排一个解码步骤。这涉及为新token “D” 分配块映射。然而，由于它适合现有的块映射，调度器不需要分配新的映射。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/prefill_4.png" class="">

<p>然后我们再次运行模型，计算并将“D”的KV写入KV缓存。这会生成一个新的token “E”。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/prefill_5.png" class="">

<p>这个过程会重复进行直到解码完成。请注意，后续的分配可能是不连续的。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/prefill_6.png" class="">


<h2 id="Speculative-Decoding"><a href="#Speculative-Decoding" class="headerlink" title="Speculative Decoding"></a>Speculative Decoding</h2><p>Speculative Decoding利用小型、快速的草案（draft）模型生成初始token（token是输入信息的基本单元）时的高效性，而在验证阶段则依赖更大的、更准确的大型语言模型（LLM）进行验证。</p>
<p>这个过程可以分为两步：</p>
<ol>
<li><strong>初始token生成</strong>：使用小型、快速的草案模型生成初始token。这一步骤快速生成token序列，使得下一步骤可以快速开始。</li>
<li><strong>验证</strong>：使用更大的、更准确的LLM对初始token进行验证。这一步骤确保生成的token序列是准确的和有效的。</li>
</ol>
<p>这种技术通过将任务分成两个阶段来实现高效性和准确性：快速生成初始token，然后验证这些token以确保准确性。</p>
<p>其实现方式如下：</p>
<ul>
<li>使用小模型进行多次decode，生成多个token序列。</li>
<li>将这些token序列传递给大模型进行验证。大模型会生成对应的logits，形状为<code>(batch, sequence, vocab_size)</code>，并进行softmax处理。</li>
<li>对于单步decode，logits的形状为<code>(batch, 1, vocab_size)</code>，其中的1表示最后一个token，在词汇表(vocab_size)上的概率分布。</li>
</ul>
<p>例如，将小模型生成的”abcd” token序列传递给大模型，得到”ABCD”的logits。A对应的是用于预测B的概率分布，在这个序列中就是预测第二个token的概率分布。将b对应的token id在A的<code>vocab_size</code>长度的log prob中的值取出，对应的可能是B token id的概率，也可能不是。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a | b | c | d</span><br><span class="line">    ^   ^   ^</span><br><span class="line">  | A | B | C | D </span><br></pre></td></tr></table></figure>

<p>最后，将b、c、d在大模型中对应的logits prob求和并取平均值。如果这个值大于一个阈值，就认为这个token是合理的，否则就拒绝。</p>
<p>这种方法通过结合小模型的快速生成能力和大模型的高准确性，实现了高效且准确的token生成过程。</p>
<p>根据论文[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.14066]%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%9C%A8continus">https://arxiv.org/pdf/2406.14066]来说，在continus</a> batching的情况下，Speculative Decoding可以提高推理速度，减少计算资源的浪费。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/spd.png" class="">

<p>在Target模型验证完以后，还会多生成一个bonus token，也就是上面的那个D。</p>
<p>其中Decoding也产生了很多的方法，有基于模型的，也有model free的，才用大模型的一部分，或者直接从外部数据库来获取。</p>
<blockquote>
<p>结果表明，在低请求率下（具体来说，请求率为 4），提出 3 个或 5 个令牌会带来最显著的加速。然而，随着请求率的增加，提出更多令牌的优势迅速减弱：当请求率超过 12 时，提出 5 个令牌不再带来性能提升。同样，在请求率大于 16 时，提出 3 个令牌会导致性能下降。</p>
</blockquote>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/p_l.png" class="">

<p>其中不同的颗粒度的猜测长度也会对性能有影响。</p>
<p>全局统一的长度；每个step所有request用一个长度；每个step每个request用不同的长度。</p>
<p>相较于吞吐，Goodput规定只有没被拒绝的token才计算，用来衡量最总的性能。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/pl_bs_gp_relation.png" class="">

<p>这张图展示了猜测长度和batch size对于Goodput的影响。</p>
<p>对于小批次，要多猜测（propose），小批次尺寸下每次请求需提议超 4 个 token 以实现最大有效吞吐量（goodput），且随着批次尺寸增大，最优猜测长度会降低；<br>对于大批次，则要少猜测，甚至不进行推测反而能获得更高有效吞吐量，因为大批次下推测失败成本显著增加，超过潜在收益。</p>
<p>除了朴素的猜测模型，里面也提到了Medusa风格的猜测模型。预测3个token就有三个head分别预测每个位置。<br>里面的例子head 1猜了三个，对了1个，head 2猜了2个对了一个，head3猜了3个全错了，然后加上LLM的bonus token。</p>
<img data-src="/zh-CN/2024/12/13/vLLM-%E5%88%86%E6%9E%90-3-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/medusa.png" class="">

<p>SmartSpec 估算Goodput的方法就是根据成功率计算期望长度再乘上Request对应的时间。<br>然后根据不同的batch size计算Goodput，让Goodput最大化，得到最佳Goodput从而让吞吐最大化。</p>
<h2 id="Automatic-Prefix-Caching"><a href="#Automatic-Prefix-Caching" class="headerlink" title="Automatic Prefix Caching"></a>Automatic Prefix Caching</h2><p>一般的LLM请求的提示词会非常长，据说OpenAI的系统提示词已经有几K了，这个规模很适合做前缀缓存。<br>前缀缓存是指将提示词分成多个前缀，然后将这些前缀缓存到KV缓存中，这样在生成token的时候就可以直接使用KV缓存中的值，<br>而不需要重新计算。这样可以减少计算量，提高推理速度。<br>当然在vLLM中kv cache是分块的，所以prefix 也是分块的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">                    Block 1                  Block 2                  Block 3</span><br><span class="line">         [A gentle breeze stirred] [the leaves as children] [laughed in the distance]</span><br><span class="line">Block 1: |&lt;--- block tokens ----&gt;|</span><br><span class="line">Block 2: |&lt;------- prefix ------&gt;| |&lt;--- block tokens ---&gt;|</span><br><span class="line">Block 3: |&lt;------------------ prefix --------------------&gt;| |&lt;--- block tokens ----&gt;|</span><br></pre></td></tr></table></figure>
<h2 id="Multi-LoRA-Serving"><a href="#Multi-LoRA-Serving" class="headerlink" title="Multi-LoRA Serving"></a>Multi-LoRA Serving</h2><p>VLLM当中的LoRA Adaptor是可以动态加载的，因为他本身和基座模型保持独立。<br>这里要注意的一个点是，如果词汇表修改了，会影响最后的llm head，比如英文基座模型用中文词汇表，那么 vocab_size 就不一样了。<br>会导致llm head的输出维度不一样，这个时候就需要重新训练llm head。<br>所以要注意最后的llm head的输出维度。</p>
<h2 id="Tensor-Parallelism"><a href="#Tensor-Parallelism" class="headerlink" title="Tensor Parallelism"></a>Tensor Parallelism</h2><p>张量并行（Tensor Parallelism）是一种将大型模型的计算任务分解到多个GPU上并行执行的技术。它通过将模型的权重矩阵切分成多个子矩阵，并将这些子矩阵分配到不同的GPU上进行计算，从而实现并行计算。</p>
<p>在Transformer模型中，QKV（Query, Key, Value）矩阵乘法是计算量最大的部分之一。通过将QKV矩阵切分成更小的子矩阵，并将这些子矩阵分配到不同的GPU上，可以显著提高计算效率。</p>
<p>例如，对于一个具有<code>d_model</code>维度的QKV矩阵，可以将其切分成<code>n</code>个子矩阵，每个子矩阵的维度为<code>d_model / n</code>。然后，将这些子矩阵分配到<code>n</code>个GPU上进行并行计算。这样，每个GPU只需要计算一个较小的子矩阵，从而减少了计算时间。</p>
<p>张量并行的实现方式如下：</p>
<ol>
<li><strong>切分权重矩阵</strong>：将模型的权重矩阵切分成多个子矩阵。例如，对于一个<code>d_model x d_model</code>的权重矩阵，可以将其切分成<code>n</code>个<code>d_model x (d_model / n)</code>的子矩阵。</li>
<li><strong>分配子矩阵</strong>：将切分后的子矩阵分配到不同的GPU上。例如，将第一个子矩阵分配到GPU 0，第二个子矩阵分配到GPU 1，以此类推。</li>
<li><strong>并行计算</strong>：在每个GPU上并行执行矩阵乘法计算。例如，在GPU 0上计算输入矩阵与第一个子矩阵的乘积，在GPU 1上计算输入矩阵与第二个子矩阵的乘积，以此类推。</li>
<li><strong>聚合结果</strong>：将所有GPU上的计算结果聚合起来，得到最终的输出。例如，将所有子矩阵的乘积结果相加，得到最终的QKV矩阵乘积结果。</li>
</ol>
<p>通过张量并行，可以显著提高大型模型的计算效率，减少推理时间，从而提高模型的推理速度和性能。</p>
<h2 id="Pipeline-Parallelism"><a href="#Pipeline-Parallelism" class="headerlink" title="Pipeline Parallelism"></a>Pipeline Parallelism</h2><p>Pipeline Parallelism通过将模型的不同层分配到不同的GPU上来实现并行计算，特别是一些大型模型。这种方法可以提高计算效率，但也会引入一些挑战，例如在层之间传递数据时可能会产生延迟。此外，由于不同层的计算时间可能不一致，可能会导致某些GPU在等待其他GPU完成计算时处于空闲状态，从而产生计算空隙（bubble）。</p>
<p>为了减少这些计算空隙，可以使用Chunked Prefill技术。Chunked Prefill通过将计算任务分成更小的块，从而缩短每个计算任务的时间。这使得在流水线并行中可以更灵活地安排计算任务，从而尽可能地填满计算空隙，提高整体计算效率。</p>
<p>通过结合Pipeline Parallelism和Chunked Prefill，可以在保持高效计算的同时，最大限度地利用计算资源，减少计算空隙，提高模型推理的速度和效率。</p>
<p>在vLLM当中 TP=n 且 PP=m 时，vLLM 引擎总共会有 n*m + 1 个进程。即使使用单个 GPU，我们也会有 2 个进程。</p>
<h2 id="衡量LLM的服务指标"><a href="#衡量LLM的服务指标" class="headerlink" title="衡量LLM的服务指标"></a>衡量LLM的服务指标</h2><p>在衡量LLM服务性能时，Token相关的数据是一个重要的方面。以下是一些常见的Token相关指标：</p>
<h3 id="Token-Throughput"><a href="#Token-Throughput" class="headerlink" title="Token Throughput"></a>Token Throughput</h3><p>Token Throughput表示每秒生成的token数量。它是衡量模型生成速度的一个重要指标，通常以tokens per minute (TPM)为单位。</p>
<h3 id="Token-Latency"><a href="#Token-Latency" class="headerlink" title="Token Latency"></a>Token Latency</h3><p>Token Latency表示生成一个token所需的时间。它是衡量模型响应速度的一个重要指标，通常以毫秒(ms)为单位。Token Latency包括以下几个子指标：</p>
<ul>
<li>**TTFT (Time To First Token)**：从请求到第一个token生成的时间。例如，当prompt变长时，TTFT会变长；或者当kv cache不足时被抢占，TTFT也会变长。</li>
<li>**TBT (Time Between Tokens)**：生成两个token之间的时间。例如，当batch size变大时，TBT会变大。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/10/vLLM-%E5%88%86%E6%9E%902-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/12/10/vLLM-%E5%88%86%E6%9E%902-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/" class="post-title-link" itemprop="url">vLLM 分析2 计算引擎</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-10 15:25:12" itemprop="dateCreated datePublished" datetime="2024-12-10T15:25:12+08:00">2024-12-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/10/vLLM-%E5%88%86%E6%9E%902-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/10/vLLM-分析2-计算引擎/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>vLLM的一个主要贡献就是PagedAttention，可以实现更高效的推理。</p>
<p>高效的语言模型服务系统（LLM）需要批量处理多个请求。然而，现有系统存在以下问题：</p>
<ul>
<li>每个请求的key-value缓存（KV缓存）内存巨大，动态增长和减少。</li>
<li>容易因为碎片化和冗余复制导致内存浪费，限制了批量大小。</li>
</ul>
<p>为了解决这些问题，提出了PagedAttention，一个基于虚拟内存和分页技术的注意力算法。基于此，开发了vLLM，一个LLM服务系统，实现了以下两个目标：</p>
<ol>
<li>KV缓存显存的几乎零浪费，减少了显存碎片。</li>
<li>KV缓存在请求之间和请求内共享，进一步减少显存使用。</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.06180">论文</a>包含了他早期设计。</p>
<p>一次调用的示例如<a target="_blank" rel="noopener" href="https://blog.vllm.ai/2023/06/20/vllm.html">博客</a>中展示的。</p>
<img data-src="/zh-CN/2024/12/10/vLLM-%E5%88%86%E6%9E%902-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/annimation1.gif" class="">

<h2 id="AsyncLLM"><a href="#AsyncLLM" class="headerlink" title="AsyncLLM"></a>AsyncLLM</h2><p><code>generate</code>细节：</p>
<ul>
<li>如果引擎没有运行，启动后台循环，循环调用 <code>_run_output_handler</code> 方法来处理等待的请求。</li>
<li>从 <code>AsyncStream</code> 中等待请求输出并生成它们。</li>
</ul>
<p>engine会在启动之前profile一下，把剩余的显存分配给kv cache用。</p>
<p>AsyncStream 对 asyncio.Queue的封装，支持了终止的能力，当finish的时候会丢入一个STOP_ITERATION的exception，这样可以让调用者知道这个stream已经结束了。</p>
<p>每当有一个对话请求的时候调用<code>add_request</code>就会生成一个这样的AsycStream用于处理对话的输出，其中副作用就是判断backgroud loop没有启动的时候，启动backgroundloop。</p>
<p>AsyncEngine本身有一个<code>_new_request</code>的Queue用户保存request的AsyncStream。</p>
<p><code>generate</code>方法会不断从AsyncStream中yield出结果，直到遇到STOP_ITERATION。</p>
<p>loop的主体如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1) Pull EngineCoreOutput from the EngineCore.</span></span><br><span class="line">outputs = <span class="keyword">await</span> <span class="variable language_">self</span>.engine_core.get_output_async()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2) Detokenize based on the output.</span></span><br><span class="line">request_outputs, reqs_to_abort = <span class="variable language_">self</span>.detokenizer.step(outputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3) Put the RequestOutputs into the per-request AsyncStreams.</span></span><br><span class="line"><span class="variable language_">self</span>._process_request_outputs(request_outputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4) Abort any requests that finished due to stop strings.</span></span><br><span class="line"><span class="keyword">await</span> <span class="variable language_">self</span>.engine_core.abort_requests_async(reqs_to_abort)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5) Abort any requests due to client cancellations.</span></span><br><span class="line"><span class="keyword">await</span> <span class="variable language_">self</span>._process_cancellations()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<blockquote>
<p>When TP=n &amp; PP=m, vLLM engine will have n*m + 1 processes in total.<br>Corollary: even when using a single GPU, we will have 2 processes.</p>
</blockquote>
<h2 id="EngineCore"><a href="#EngineCore" class="headerlink" title="EngineCore"></a>EngineCore</h2><p>EngineCore主要是完成 schedule、execute 和 output 的循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>) -&gt; <span class="type">List</span>[EngineCoreOutput]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Schedule, execute, and make output.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.scheduler.has_unfinished_requests():</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    scheduler_output = <span class="variable language_">self</span>.scheduler.schedule()</span><br><span class="line">    output = <span class="variable language_">self</span>.model_executor.execute_model(scheduler_output)</span><br><span class="line">    engine_core_outputs = <span class="variable language_">self</span>.scheduler.update_from_output(</span><br><span class="line">        scheduler_output, output)</span><br><span class="line">    <span class="keyword">return</span> engine_core_outputs</span><br></pre></td></tr></table></figure>
<h2 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h2><p>在具体分析之前，先看看 Request 的定义，这个数据结构串联了很多东西。</p>
<p>属性 <code>num_tokens</code> 代表的是 <code>prompt_tokens</code> 和 <code>output_tokens</code> 的总数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">num_tokens</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>._all_token_ids)</span><br></pre></td></tr></table></figure>

<p><code>num_output_tokens</code> 代表 output tokens 的数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">num_output_tokens</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>._output_token_ids)</span><br></pre></td></tr></table></figure>

<p><code>append_output_token_ids</code> 会改变上述的两个属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">append_output_token_ids</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    token_ids: <span class="type">Union</span>[<span class="built_in">int</span>, <span class="type">List</span>[<span class="built_in">int</span>]],</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(token_ids, <span class="built_in">int</span>):</span><br><span class="line">        token_ids = [token_ids]</span><br><span class="line">    <span class="variable language_">self</span>._output_token_ids.extend(token_ids)</span><br><span class="line">    <span class="variable language_">self</span>._all_token_ids.extend(token_ids)</span><br></pre></td></tr></table></figure>

<p>在 <code>__init__</code> 方法中，会设置 <code>num_prompt_tokens</code>，这个是不变的，<code>num_computed_tokens</code> 会初始化为 0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.prompt = <span class="variable language_">self</span>.inputs.prompt</span><br><span class="line"><span class="variable language_">self</span>.prompt_token_ids = <span class="variable language_">self</span>.inputs.prompt_token_ids</span><br><span class="line"><span class="variable language_">self</span>.num_prompt_tokens = <span class="built_in">len</span>(<span class="variable language_">self</span>.prompt_token_ids)</span><br><span class="line"><span class="variable language_">self</span>.num_computed_tokens = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>所以在最开始时，<code>num_tokens</code> 和 <code>num_prompt_tokens</code> 是相等的。当 <code>prefill</code> 以后，<code>num_computed_tokens</code> 会逐渐（逐渐的原因是 prefill 可能会被 chunked 掉）等于 <code>num_prompt_tokens</code>。<code>decode</code> 以后，<code>num_tokens</code> 会等于 <code>num_prompt_tokens</code> 加上 <code>num_output_tokens</code>。如果 <code>computed_tokens</code> 等于 <code>num_tokens</code>，说明已经开始 decode 了，要开始一个 token 一个 token 计算了。</p>
<p>在调度过程中没有直接用 <code>computed_tokens</code> 等于 <code>num_prompt_tokens</code> 的原因是：如果一个 request 被抢占掉，那么 <code>num_tokens</code> 在 request 恢复的时候其实应该是 <code>num_prompt_tokens</code> 加上 <code>num_output_tokens</code>，这里做了一个统一的判断。如果把preempted的request重新处理的话其实相当于多了一些output tokens的prompt的新request。</p>
<h2 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h2><p>从 EngineCore 的 step 方法来看，目前的调度是同步的 <code>schedule | execute model | update_from_output | schedule | execute model | update_from_output</code>，这样会导致计算和调度之间的时间差，这个时间差会导致计算的时间没有充分利用，从而导致资源的浪费。后面的版本应该会有优化。</p>
<p>Scheduler 的 V1 版本把一些 chunked prefill 还有 prefix caching 的内容拆离出去，做得比较通用。</p>
<p>vLLM 实现了一种所有或无（all-or-nothing）驱逐策略，即要么驱逐一个序列中的所有块，要么不驱逐任何块。</p>
<p>接下来看看来自 <code>v1/core/scheduler.py</code> 的 V1 版本的 <code>schedule</code> 实现。</p>
<p>Scheduler 有个 waiting list 和 running list（位置代表权重，是 FIFO 的）。</p>
<p>从 running list 中获取 request 然后通过 <code>kv_cache_manager</code> 执行 <code>append_slots</code> 把新的 block 追加到 request 的 block chain 当中。如果最后一个 block 的 slot 还够的话，就不会追加新的 block。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new_blocks = <span class="variable language_">self</span>.kv_cache_manager.append_slots(</span><br><span class="line">    request, num_new_tokens)</span><br></pre></td></tr></table></figure>

<p>如果当前的 <code>kv_cache</code> 的 block table 满了，则会抢占一个 running list 中的 request（放入 waiting list 中）并且把他的 cache block 都 free 掉，这里的 free 是引用计数的形式，如果引用计数为 0 就会被释放，但如果多个 request 共享了一个 block 就还不会被真正释放。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> new_blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># The request cannot be scheduled.</span></span><br><span class="line">    <span class="comment"># Preempt the lowest-priority request.</span></span><br><span class="line">    preempted_req = <span class="variable language_">self</span>.running.pop()</span><br><span class="line">    <span class="variable language_">self</span>.kv_cache_manager.free(preempted_req)</span><br><span class="line">    preempted_req.status = RequestStatus.PREEMPTED</span><br><span class="line">    <span class="variable language_">self</span>.waiting.appendleft(preempted_req)</span><br><span class="line">    preempted_reqs.append(preempted_req)</span><br></pre></td></tr></table></figure>

<p>加入到 <code>scheduled_running_reqs</code> 中，消耗这次调度的 token budget，这个 budget 用完以后就会停止调度了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scheduled_running_reqs.append(request)</span><br><span class="line">req_to_new_block_ids[request.request_id] = [</span><br><span class="line">    b.block_id <span class="keyword">for</span> b <span class="keyword">in</span> new_blocks</span><br><span class="line">]</span><br><span class="line">num_scheduled_tokens[request.request_id] = num_new_tokens</span><br><span class="line">token_budget -= num_new_tokens</span><br><span class="line">req_index += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>如果没有抢占请求则说明还是比较富裕的，尝试从 waiting list 中获取 request，waiting list 可能有新请求也可能有之前被抢占的请求，然后执行一遍上面的代码，不同的是需要从 <code>kv_cache_manager</code> 计算 <code>computed_tokens</code>，因为被之前被抢占的或者一些有共同前缀的 kv cache block 是已经缓存过的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">request = <span class="variable language_">self</span>.waiting[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># Get already-cached tokens.</span></span><br><span class="line">computed_blocks = <span class="variable language_">self</span>.kv_cache_manager.get_computed_blocks(</span><br><span class="line">    request)</span><br><span class="line"><span class="comment"># NOTE(woosuk): Since incomplete blocks are not eligible for</span></span><br><span class="line"><span class="comment"># sharing, `num_computed_tokens` is always a multiple of</span></span><br><span class="line"><span class="comment"># `block_size`.</span></span><br><span class="line">num_computed_tokens = <span class="built_in">len</span>(computed_blocks) * <span class="variable language_">self</span>.block_size</span><br></pre></td></tr></table></figure>

<p>最后把每个 request 分配到的 tokens 数量记录到 <code>SchedulerOutput</code> 当中。</p>
<p><code>update_from_output</code> 接受 <code>SchedulerOutput</code> 和 <code>ModelExecutorOutput</code>，更新 request 的状态，例如更新已经计算的 token 数量，更新 kv cache 的 block 等。对于每个请求都会检查 <code>request.num_computed_tokens == request.num_tokens</code> 从而判断是否已经开始 decode 的部分了。然后构造 <code>EngineCoreOutput</code>，并且检查是否需要停止这个 request。<code>_check_stop</code> 方法会检查是否已经生成了 eos token 或者已经达到了最大长度，并且 free 掉对应的 request。所有没有 stop 的 request 会重新加入到 running 队列中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> request.num_computed_tokens == request.num_tokens:</span><br><span class="line">    req_index = model_runner_output.req_id_to_index[req_id]</span><br><span class="line">    <span class="comment"># NOTE(woosuk): Currently, we assume that each request</span></span><br><span class="line">    <span class="comment"># generates at most one token at each step.</span></span><br><span class="line">    token_id = sampled_token_ids[req_index]</span><br><span class="line">    request.append_output_token_ids(token_id)</span><br><span class="line">    num_new_tokens = <span class="number">1</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Update the KV cache manager for prefix caching.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check for stop and update request state.</span></span><br><span class="line">    <span class="comment"># This must be called before me make the EngineCoreOutput.</span></span><br><span class="line">    stopped = <span class="variable language_">self</span>._check_stop(request)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add EngineCoreOutput for this Request.</span></span><br><span class="line">    output = EngineCoreOutput(</span><br><span class="line">        request_id=req_id,</span><br><span class="line">        new_token_ids=request.output_token_ids[-num_new_tokens:],</span><br><span class="line">        finished=request.is_finished(),</span><br><span class="line">        finish_reason=request.get_finished_reason(),</span><br><span class="line">        stop_reason=request.stop_reason)</span><br><span class="line">    engine_core_outputs.append(output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Breakout of the loop.</span></span><br><span class="line">    <span class="keyword">if</span> stopped:</span><br><span class="line">        <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>

<h3 id="SchedulerOutput"><a href="#SchedulerOutput" class="headerlink" title="SchedulerOutput"></a>SchedulerOutput</h3><p>该类包含了调度器的输出信息。以下是各个字段的作用：</p>
<ul>
<li><p><strong>scheduled_new_reqs: List[NewRequestData]</strong></p>
<ul>
<li>作用：存储新请求的数据列表，这些请求是刚刚被调度的。</li>
</ul>
</li>
<li><p><strong>scheduled_resumed_reqs: List[ResumedRequestData]</strong></p>
<ul>
<li>作用：存储恢复请求的数据列表，这些请求是之前被暂停，现在重新被调度的。</li>
</ul>
</li>
<li><p><strong>scheduled_running_reqs: List[RunningRequestData]</strong></p>
<ul>
<li>作用：存储正在运行请求的数据列表，这些请求在当前调度周期内继续运行。</li>
</ul>
</li>
<li><p><strong>num_scheduled_tokens: Dict[str, int]</strong></p>
<ul>
<li>作用：存储每个请求调度的token数量，键是请求的ID，值是对应的token数量。</li>
</ul>
</li>
<li><p><strong>total_num_scheduled_tokens: int</strong></p>
<ul>
<li>作用：存储所有请求调度的token总数。</li>
</ul>
</li>
<li><p><strong>scheduled_encoder_inputs: Dict[str, List[int]]</strong></p>
<ul>
<li>作用：存储每个请求的编码器输入，键是请求的ID，值是对应的编码器输入列表。</li>
</ul>
</li>
<li><p><strong>preempted_req_ids: Set[str]</strong></p>
<ul>
<li>作用：存储被抢占的请求ID集合，这些请求在当前调度周期内被暂停。</li>
</ul>
</li>
<li><p><strong>finished_req_ids: Set[str]</strong></p>
<ul>
<li>作用：存储已完成的请求ID集合，这些请求在当前调度周期内完成。</li>
</ul>
</li>
<li><p><strong>free_encoder_input_ids: List[Tuple[str, int]]</strong></p>
<ul>
<li>作用：存储空闲的编码器输入ID列表，每个元素是一个元组，包含请求ID和对应的编码器输入ID。</li>
</ul>
</li>
</ul>
<p>这些字段共同描述了调度器在一个调度周期内的所有操作和状态变化。</p>
<h2 id="KVCacheManager"><a href="#KVCacheManager" class="headerlink" title="KVCacheManager"></a>KVCacheManager</h2><p>来自<code>v1/core/kv_cache_manager.py</code>，这是v1版本的实现。</p>
<p>kv cache比较简单，<br>这个<a target="_blank" rel="noopener" href="https://medium.com/my-musings-with-llms/understanding-kv-cache-and-paged-attention-in-llms-a-deep-dive-into-efficient-inference-62fa372432ce">博客</a>中的图片很好地阐述了kvcache的作用。</p>
<img data-src="/zh-CN/2024/12/10/vLLM-%E5%88%86%E6%9E%902-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/kvcache.gif" class="" title="KV Cache">

<p>但涉及PagedAttention的实现，就需要管理block。这类似于操作系统中的虚拟地址、页表和物理页的关系。</p>
<p>PagedAttention的主要思想是基于操作系统中分页（paging）的经典概念。传统的注意力算法通常要求keys和values在内存空间中连续存储，<br>而PagedAttention则允许在非连续的内存空间中存储keys和values。</p>
<p>PagedAttention将每个序列（sequence）的KV缓存（KV cache）分成固定大小的块（block）。<br>每个块包含一个固定数量的token的key和value向量。这意味着，即使keys和values不连续存储，也可以有效地访问和操作它们。</p>
<p>block的管理会有一个类似于页表的结构，用于映射block的逻辑地址到物理地址。</p>
<img data-src="/zh-CN/2024/12/10/vLLM-%E5%88%86%E6%9E%902-%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E/block_table.png" class="">

<p>论文中的这个图很好的表示了他们的关系，如果新生成的token填满了当前block就会分配一个新的block用于新token的生成。</p>
<p>共享的prefix cache指的是提示词的前缀一样的情况，他们的位置编码也不变的情况下可以在不同的sequence之间共享。<br>例如对于一个英语到法语翻译的提示词，前面有很多事可以共享的，对于跨请求的kv cache来说可以基于这个前缀来共享kv cache的block。</p>
<table>
<thead>
<tr>
<th>序列</th>
<th>前缀 (Prefix)</th>
<th>输入任务 (Task Input)</th>
<th>完整提示 (Complete Prompt)</th>
<th>LLM 输出 (LLM Output)</th>
<th>输出任务 (Task Output)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Sequence A</strong></td>
<td>Translate English to French: <br> “sea otter” =&gt; “loutre de mer” <br> “peppermint” =&gt; “menthe poivrée” <br> “plush giraffe” =&gt; “girafe en peluche”</td>
<td>“cheese” =&gt;</td>
<td>Translate English to French: <br> “sea otter” =&gt; “loutre de mer” <br> “peppermint” =&gt; “menthe poivrée” <br> “plush giraffe” =&gt; “girafe en peluche” <br> “cheese” =&gt;</td>
<td>fromage</td>
<td>fromage</td>
</tr>
<tr>
<td><strong>Sequence B</strong></td>
<td>Translate English to French: <br> “sea otter” =&gt; “loutre de mer” <br> “peppermint” =&gt; “menthe poivrée” <br> “plush giraffe” =&gt; “girafe en peluche”</td>
<td>“I love you” =&gt;</td>
<td>Translate English to French: <br> “sea otter” =&gt; “loutre de mer” <br> “peppermint” =&gt; “menthe poivrée” <br> “plush giraffe” =&gt; “girafe en peluche” <br> “I love you” =&gt;</td>
<td>Je t’aime</td>
<td>Je t’aime</td>
</tr>
</tbody></table>
<p><code>free_block_queue</code>是一个链表，用于分配block，初始化时将<code>block_pool</code>中的所有blocks串起来。它通过链表实现对<code>KVCacheBlock</code>的管理，删除操作是O(1)的，没有使用标准库中的dequeue。</p>
<p><code>KVCacheBlock</code>除了prev和next指针，还有<code>ref_count</code>和<code>_block_hash</code>，用于prefix caching的计算。其key是父block的hash和当前block的tokens ids的hash。</p>
<p><code>block_pool</code>代表物理block的映射关系，例如<code>0 -&gt; 第一块block</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A Block pool of all kv-cache blocks.</span></span><br><span class="line"><span class="variable language_">self</span>.block_pool: <span class="type">List</span>[KVCacheBlock] = [</span><br><span class="line">    KVCacheBlock(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(num_gpu_blocks)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><code>cached_block_hash_to_block</code> 保存的数据结构是 <code>&#123;block_hash: &#123;block ID: block&#125;&#125;</code></p>
<p><code>req_to_blocks</code> 保存了 request到 block列表的映射关系，<code>&#123;request ID: [block ID]&#125;</code></p>
<p>block的eviction的定义，<code>eviction candidate == in free queue and ref_cnt == 0</code>。</p>
<h3 id="get-computed-blocks方法"><a href="#get-computed-blocks方法" class="headerlink" title="get_computed_blocks方法"></a><code>get_computed_blocks</code>方法</h3><p>根据request获取已经计算过（缓存过）的block，获取kv cache blocks的方式是通过block hash 从<code>cached_block_hash_to_block</code>寻找的。<br>hash的计算是之前的block hash加上当前token ids做一次hash，第一个block则没有父block只用当前自己的token ids做hash。</p>
<h3 id="append-slots方法"><a href="#append-slots方法" class="headerlink" title="append_slots方法"></a><code>append_slots</code>方法</h3><p>会为需要新计算的token ids分配block（如果现有的block不够的话）。</p>
<h2 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h2><h3 id="GPUModelRunner"><a href="#GPUModelRunner" class="headerlink" title="GPUModelRunner"></a>GPUModelRunner</h3><p><code>v1/worker</code>中的<code>gpu_runner.py</code>v1版本的实现。</p>
<p>首先依赖一个大的config参数<code>vllm_config</code>，包含了<code>model_config</code>，<code>cache_config</code>，<code>scheduler_config</code>，<code>device_config</code>等。</p>
<p>初始化kv cache的dtype，对照表如下，half就是fp16，float就是fp32，默认是和模型的dtype一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">STR_DTYPE_TO_TORCH_DTYPE = &#123;</span><br><span class="line">    <span class="string">&quot;half&quot;</span>: torch.half,</span><br><span class="line">    <span class="string">&quot;bfloat16&quot;</span>: torch.bfloat16,</span><br><span class="line">    <span class="string">&quot;float&quot;</span>: torch.<span class="built_in">float</span>,</span><br><span class="line">    <span class="string">&quot;fp8&quot;</span>: torch.uint8,</span><br><span class="line">    <span class="string">&quot;fp8_e4m3&quot;</span>: torch.uint8,</span><br><span class="line">    <span class="string">&quot;fp8_e5m2&quot;</span>: torch.uint8,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>初始化<code>sliding_window</code>的配置，这个东西在Qwen里面才用到。</p>
<p>初始化<code>block_size</code>，决定了kv cache中连续保存的token的数量，也就是PagedAttention中的那个block的大小，Prefix cache也是以block为维度的。</p>
<p>初始化<code>kv_heads</code>，这个决定了kv head的数量，如果指定了 <code>tensor_parallel_size</code>，会根据这个参数平均分给每个GPU。</p>
<p>初始化<code>head_size</code>，基于model config，是model config里面的<code>head_dim</code>。</p>
<p>初始化<code>hidden_size</code>，就是model config里面的<code>hidden_size</code>，就是<code>d_model</code>或者<code>embed_dim</code>，代表同一个长度。</p>
<p>初始化<code>kv_cache</code>。</p>
<p>初始化<code>encoder_cache</code> encoder结果的缓存。</p>
<p>初始化<code>input_registry</code> 和多模态的支持有关系。</p>
<p>初始化<code>requests</code> dict用于request的状态保存，这里的request就是一个文本的sequence。</p>
<p>初始化<code>InputBatch</code>，<code>max_num_seq</code>决定了batch的宽度，<code>max_model_len</code>决定了batch的长度。这个Batch对象负责管理在用于前向传播的batch当中的request的插入和删除。</p>
<p>初始化<code>use_cuda_graph</code> 这个由 enforce_eager 决定，默认是会加载整个计算图。</p>
<p>初始化<code>positions</code>: <code>torch.zeros(self.max_num_tokens, dtype=torch.int64, device=self.device)</code>。</p>
<p>初始化<code>input_embeds</code>，可以看到，宽度是<code>max_num_tokens</code>，长度是<code>hidden_size</code>，这个是用来存储输入的embedding的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.inputs_embeds = torch.zeros(</span><br><span class="line">    (<span class="variable language_">self</span>.max_num_tokens, <span class="variable language_">self</span>.hidden_size),</span><br><span class="line">    dtype=<span class="variable language_">self</span>.dtype,</span><br><span class="line">    device=<span class="variable language_">self</span>.device)</span><br></pre></td></tr></table></figure>

<h3 id="InputBatch"><a href="#InputBatch" class="headerlink" title="InputBatch"></a>InputBatch</h3><p>InputBatch在整个工程中负责管理和处理批量输入请求，确保请求的高效处理和管理。</p>
<h3 id="execute-model-方法"><a href="#execute-model-方法" class="headerlink" title="execute_model 方法"></a><code>execute_model</code> 方法</h3><p><code>execute_model</code>是整个<code>schedule | compute | update</code>循环中的核心部分，负责执行模型的前向传播。</p>
<h4 id="update-states方法"><a href="#update-states方法" class="headerlink" title="_update_states方法"></a><code>_update_states</code>方法</h4><p>在每次运行一个 batch 时，会根据调度器（scheduler）的要求调整每个 batch 中请求的优先级。调度器会更新请求的状态缓存 <code>id -&gt; CachedRequestState</code> 和 <code>input_batch</code> 的缓存，移除被抢占和停止的请求，并将新加入的请求放入 batch 中。因此，runner 只负责执行，具体的策略由调度器决定。</p>
<p><code>CachedRequestState</code> 记录了请求 ID、使用的缓存块 ID 以及已计算的 token 数量。</p>
<h4 id="excute-encoder方法"><a href="#excute-encoder方法" class="headerlink" title="_excute_encoder方法"></a><code>_excute_encoder</code>方法</h4><p>执行多模态中的encoder，对于新的多模态的encode，调用<code>model.process_mm_inputs</code>存入到encoder_cache当中。</p>
<p><code>self.model.compute_logits</code>使用<br><code>vllm/model_executor/layers/logits_processor.py</code>中的<code>LogitsProcessor</code>，从<code>hidden_states</code>计算<code>logits</code>。</p>
<p><code>self.model.sample</code>使用<code>vllm/model_executor/layers/sampler.py</code>中的sampler进行sample。</p>
<p>最终得到<code>sampled_token_ids = sampler_output.sampled_token_ids</code>。</p>
<h4 id="gather-encoder-outputs-方法"><a href="#gather-encoder-outputs-方法" class="headerlink" title="_gather_encoder_outputs 方法"></a><code>_gather_encoder_outputs</code> 方法</h4><p>从<code>encoder_cache</code>中获取当前batch需要用到的encoder的输出。</p>
<h4 id="prepare-inputs方法"><a href="#prepare-inputs方法" class="headerlink" title="_prepare_inputs方法"></a><code>_prepare_inputs</code>方法</h4><p><code>input_batch.block_table</code> 在 GPU 上，而 <code>input_batch.block_table_cpu_tensor</code> 在 CPU 上。<br>前面提到 batch 的整理是在 CPU 上进行的，这里是将要推理的部分拷贝到 GPU 上的 <code>block_table</code> 中。由于使用了 PagedAttention，因此所有的序列都是按 block 为粒度进行切分的。</p>
<p>获取<code>input_ids</code>，构造出传给FlashAttention的数据，例如<code>block_table</code>，和<code>query_start_loc</code>和<code>seq_start_loc</code>用于定位query和seq的位置。</p>
<p>input_ids, attn_metadata, logits_indices</p>
<h4 id="prepare-sampling方法"><a href="#prepare-sampling方法" class="headerlink" title="_prepare_sampling方法"></a><code>_prepare_sampling</code>方法</h4><p>构造出sampling的参数，获取每个request的<code>temperature</code>，<code>top_k</code>，<code>top_p</code>等参数。</p>
<h3 id="GPUWorker"><a href="#GPUWorker" class="headerlink" title="GPUWorker"></a>GPUWorker</h3><p><code>v1/worker</code>中的<code>gpu_worker.py</code>v1版本的实现。<br>初始化GPUModelRunner，如果开始了<code>VLLM_TORCH_PROFILER_DIR</code>就会调用<code>torch.profiler.profile</code>。</p>
<p><code>determine_num_available_blocks</code>会通过profile的方式决定可以使用的block数量。<br>然后根据block数量调用Runner的<code>initialize_kv_cache</code>。</p>
<p>做一些GPU的dtype支持检查，比如一些老的GPU是不支持bf16的。</p>
<p>FlashAttentionMetadata 包含了input的结构和对应的block table的映射。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/03/FlashAttention-%E8%A7%A3%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/12/03/FlashAttention-%E8%A7%A3%E6%9E%90/" class="post-title-link" itemprop="url">FlashAttention 解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-03 16:40:50" itemprop="dateCreated datePublished" datetime="2024-12-03T16:40:50+08:00">2024-12-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/03/FlashAttention-%E8%A7%A3%E6%9E%90/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/03/FlashAttention-解析/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>FlashAttention是一种新型的注意力算法，它能够准确计算注意力，且只需进行远远少于传统方法的内存访问。这个算法的主要目标是尽可能避免内存的读取和写入，这是注意力机制性能瓶颈的一个关键因素。该论文提出了一个IO-aware的精确注意力算法，它使用tiling（贴瓷砖，代表数据分片）来减少GPU高带宽内存与低带宽内存之间的内存读取/写入次数。</p>
<p>该算法基于注意力矩阵通常稀疏这一观察结果：注意力矩阵只有少数元素非零。它通过将输入矩阵Q、K、V分成更小的块来实现，从而避免了计算全矩阵乘积Q*K^T的内存占用问题。通过块级别的处理，FlashAttention使得矩阵操作可以在现代GPU的内存限制下进行，并仅读取/写入每个切片的非零元素。这降低了需要的内存访问次数，使整个过程更快和更高效。</p>
<p>FlashAttention通过分“瓦片化”的方式计算能够更快的一个原因是将矩阵放入更高速的缓存当中，高速的叫SRAM，低速的叫HBM。</p>
<p>第一代 FlashAttention 只是把QK切片，这个只要把矩阵切分在SRAM，然后计算出结果再存回HBM，这个比较简单。</p>
<p>第二代 FlashAttention 把 softmax 的计算也放在了SRAM上。</p>
<p>源自<a target="_blank" rel="noopener" href="https://jrodthoughts.medium.com/understanding-flashattention-3-one-of-the-most-important-algortihms-to-make-transformers-fast-7d21b0f6e6a4">博客</a>描述的结构中可以看出。</p>
<img data-src="/zh-CN/2024/12/03/FlashAttention-%E8%A7%A3%E6%9E%90/qkv.webp" class="">

<p>他这里面标得感觉不是很清楚，其中的O_2应该是最终的结果O，里面的l_1/l_2 * A^1 / l_1 就还原出了最终结果的分母，也就是scale法则。</p>
<p>第三代 FlashAttention 减少了上面提到的scale，不再每一步做除法，而是放到最后再除。还有就是针对交叉注意力中的mask的优化，跳过了被mask的部分。还有就是CUDA Thread warps的优化提高了并行度。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>FlashAttention通过利用高速缓存和分块技术，显著减少了内存访问次数，提高了注意力计算的效率。第一代主要通过切分QK矩阵并利用SRAM缓存，第二代将softmax计算也放入SRAM，第三代则进一步优化了scale计算和mask处理，并提升了并行度。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/12/02/vLLM-%E5%88%86%E6%9E%901-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/12/02/vLLM-%E5%88%86%E6%9E%901-1/" class="post-title-link" itemprop="url">vLLM 分析 1 提示词的前置处理和流式响应</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-02 15:03:50" itemprop="dateCreated datePublished" datetime="2024-12-02T15:03:50+08:00">2024-12-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/12/02/vLLM-%E5%88%86%E6%9E%901-1/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/12/02/vLLM-分析1-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>vLLM 基于 uvicorn + FastAPI 的异步 Web 框架构成。vLLM 的主体是 LLMEngine，它是一个单例类，负责管理所有的模型和数据。在异步 API 中使用的是一个 AsyncEngine。在分析 AsyncEngine 之前，我们先将 Web 部分单独拆出来看一下。</p>
<p>vLLM 的 CLI 入口是 <code>vllm/scripts.py</code>，其中 <code>serve</code> 的启动是通过 <code>uvloop.run</code> 的方式启动的。uvloop 是一个替代默认 asyncio 事件循环的库，它使用 libuv 作为事件循环的实现，从而提高性能。uvicorn 是一个基于 uvloop 的 ASGI 服务器，它可以将 ASGI 应用部署到 Web 服务器上。FastAPI 是一个基于 Starlette 的 Web 框架，它提供了许多便利的功能，比如自动文档生成、请求参数校验等。</p>
<p>参数经过解析以后会进入 <code>run_server</code>，通过 <code>uvloop.run(run_server(args))</code>。<code>run_server</code> 在 <code>entrypoints/openai/api_server.py</code> 下面。<code>AsyncEngineArgs.from_cli_args(args)</code> 使用命令行参数初始化 <code>AsyncEngineArgs</code>，如果要自行封装的话可以直接初始化 <code>AsyncEngineArgs</code>。<code>AsyncEngineArgs</code> 继承自 <code>EngineArgs</code>，其中的参数都是用来控制推断命令的。</p>
<p>比较常用的几个参数：</p>
<ul>
<li><p><strong>model</strong>: 模型的路径，可以是一个目录，也可以是 hf 上的一个 repo。</p>
</li>
<li><p><strong>model_name</strong>: 如果是目录的话，期望的模型名称，或者想要改个别名，对应的是 API 中指定模型的名称。</p>
</li>
<li><p><strong>tensor_parallel_size</strong>: tensor parallel 副本数，如果用多个 GPU 可以用到，会根据这个将 kv head 平分到不同的 GPU 上。</p>
</li>
<li><p><strong>pipeline_parallel_size</strong>: pipeline stages 数，如果用多个 GPU 可以用到，会根据这个将模型的前向计算的layers分成多个阶段，每个阶段在不同的 GPU 上计算。</p>
<p>  可以参考下面这个例子：</p>
<p>  假设我们有 8 个 GPU，分别表示为 g0 … g7，并且我们使用 2 个 GPU 来并行化模型张量，使用 4 个 GPU 来并行化模型流水线。当前函数将创建 4 个张量模型并行组和 2 个流水线模型并行组：</p>
<p>  4 个张量模型并行组：</p>
<ul>
<li>[g0, g1]</li>
<li>[g2, g3]</li>
<li>[g4, g5]</li>
<li>[g6, g7]</li>
</ul>
<p>  2 个流水线模型并行组：</p>
<ul>
<li>[g0, g2, g4, g6]</li>
<li>[g1, g3, g5, g7]</li>
</ul>
<p>  注意，为了提高效率，调用者应确保相邻的 rank 位于同一个 DGX 盒子上。例如，如果我们使用 2 个 DGX-1 盒子，总共有 16 个 GPU，rank 0 到 7 属于第一个盒子，rank 8 到 15 属于第二个盒子。</p>
</li>
<li><p><strong>num_seqs</strong>: 最大的序列数，其实就是 batch size，会翻倍得增加显存使用，这个貌似在启动之前的 profile 阶段可能会导致大量显存的占用。</p>
</li>
<li><p><strong>quantization</strong>: 量化的方法，可以是 bitsandbytes 等，可能需要和 <code>load_format</code> 结合使用。</p>
</li>
<li><p><strong>load_format</strong>: 加载模型的格式，可以是 pt, safetensors, bitsandbytes 等等，如果用到量化的模型基本要改成 bitsandbytes。</p>
</li>
<li><p><strong>dtype</strong>: 数据类型，fp32, fp16，bf16 等等，如果模型是 bf16 的话，他默认是 bf16 的模型用 bf16，有些显卡不支持 bf 浮点数所以要设置成 half 也就是 fp16。</p>
</li>
<li><p><strong>host</strong>: 监听地址。</p>
</li>
<li><p><strong>port</strong>: 监听端口。</p>
</li>
<li><p><strong>max_model_len</strong>: 上下文长度，适合显存不足的显卡，把默认的上下文长度改下一点。</p>
</li>
<li><p><strong>enforce_eager</strong>: 是否强制使用 eager 模式，如果显存不够的需要开启这个模式，不完全加载计算图的方式可以减少显存的使用。</p>
</li>
</ul>
<p><code>api_server</code> 中的 <code>build_app</code> 会使用 <code>APIRouter</code> 初始化路由，并通过 <code>app.include_router</code> 引入。</p>
<p>主要看 <code>@router.post(&quot;/v1/chat/completions&quot;)</code> 注册的 <code>async def create_chat_completion</code> 是最常用的函数调用。</p>
<p><code>init_app_state</code> 在 <code>app.state</code> 中保存了 <code>openai_serving_chat</code>，以及其他一些接口的状态，这取决于模型配置中是否包含这些功能。例如，文本嵌入等功能（通常都有）。当调用 <code>create_chat_completion</code> 时，会调用 <code>openai_serving_chat</code> 对应的 <code>OpenAIServingChat</code> 类的方法。因此，Serving 的主体可以通过查看这个对象的方法来理解其功能。</p>
<p>构建 AsyncEngine -&gt; 构建 app 对象。</p>
<h2 id="OpenAIServingChat-create-chat-completion-主体流程"><a href="#OpenAIServingChat-create-chat-completion-主体流程" class="headerlink" title="OpenAIServingChat.create_chat_completion 主体流程"></a><code>OpenAIServingChat.create_chat_completion</code> 主体流程</h2><ol>
<li><p><strong>检查模型</strong>：</p>
<ul>
<li>是否支持 model，model 是否是 rola，model 是否是 prompt adapter 等。<blockquote>
<p>vLLM 的 rola 不是和基座合并在一起的，是支持基座模型加多了个 lora 模型的形式。prompt adapter 看起是多模态架构中的 adaptor。</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>从 Engine 中获取 Tokenizer</strong>：</p>
<ul>
<li>主要是基于 model path 获取对应的 tokenizer 文件，并初始化对应的 tokenizer。</li>
</ul>
</li>
<li><p><strong>_preprocess_call</strong>：对输入进行预处理 </p>
<ul>
<li><code>resolve_chat_template_content_format</code>：检查对话模板格式，因为每种大模型的用于生成文本的训练数据的格式有所不同，要确认对应的格式，LLAMA 有 LLAMA 的格式，可以参考下面的例子。</li>
<li><code>parse_chat_messages_futures</code>：解析输入的聊天消息，生成一个对话消息列表，变成有类型的对话消息。其中 <code>mm_tracker</code> 要处理 <code>image_url</code> 和 <code>audio_url</code> 的消息，会根据构造 <code>placeholder</code>，<code>placeholder</code> 是一个特殊的字符串，用来标记这个位置是一个占位符。<code>llama3.2</code> 用的是 <code>&lt;|image|&gt;</code>。</li>
<li><code>apply_&#123;hf,mistral&#125;_chat_template</code>：模板会给提示词添加提示词的开头和结束的标志，从而和实际训练的数据标注对齐，比如 <code>llama3</code> 用 <code>&lt;|eot_id|&gt;</code> 标记结束，<code>padding</code> 等。<code>request_prompt</code> 和 <code>engine_prompt</code> 包含 <code>token ids</code> 和多模态数据。<br>例如：</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">chat = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: [</span><br><span class="line">            &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;image&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;If I had to write a haiku for this one, it would be: &quot;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p> 会变成 <code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&lt;|image|&gt;If I had to write a haiku for this one, it would be: &lt;|eot_id|&gt;</code> 中，<code>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</code> 标识 <code>header</code>（也就是 <code>role</code>），<code>&lt;|begin_of_text|&gt;</code> 标识上下文的开头，<code>&lt;|eot_id|&gt;</code> 标识一个消息的结束。除此之外，对于function call的处理，可以参考 <code>examples/tool_chat_template_llama3.2_json.jinja</code> 的一部分可以看出，会把对应工具的调用和提示词加入到用户对话前面，作为 user 的 text 的前缀中的内容形成提示词的一部分上下文。</p>
 <figure class="highlight jinja"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&#123;#- Custom tools are passed in a user message with some extra guidance #&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">&#123;%- <span class="name"><span class="name">if</span></span> tools_in_user_message and not tools is none %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="comment">&#123;#- Extract the first user message so we can plug it in here #&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">&#123;%- <span class="name"><span class="name">if</span></span> messages | length != 0 %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    </span><span class="template-tag">&#123;%- <span class="name"><span class="name">if</span></span> messages[0][&#x27;content&#x27;] is string %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">        </span><span class="template-tag">&#123;%- <span class="name">set</span> first_user_message = messages[0][&#x27;content&#x27;]|trim %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    </span><span class="template-tag">&#123;%- <span class="name"><span class="name">else</span></span> %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">        </span><span class="template-tag">&#123;%- <span class="name">set</span> first_user_message = messages[0][&#x27;content&#x27;] | selectattr(&#x27;type&#x27;, &#x27;equalto&#x27;, &#x27;text&#x27;) | map(attribute=&#x27;text&#x27;) | map(&#x27;trim&#x27;) | join(&#x27;\n&#x27;) %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    </span><span class="template-tag">&#123;%- <span class="name"><span class="name">endif</span></span> %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    </span><span class="template-tag">&#123;%- <span class="name">set</span> messages = messages[1:] %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">&#123;%- <span class="name"><span class="name">else</span></span> %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    </span><span class="template-variable">&#123;&#123;- raise_exception(&quot;Cannot put tools in the first user message when there&#x27;s no first user message!&quot;) &#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">&#123;%- <span class="name"><span class="name">endif</span></span> %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;- &#x27;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&#x27; -&#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;- &quot;Given the following functions, please respond with a JSON for a function call &quot; &#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;- &quot;with its proper arguments that best answers the given prompt.\n\n&quot; &#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;- &#x27;Respond in the format &#123;&quot;name&quot;: function name, &quot;parameters&quot;: dictionary of argument name and its value&#125;. &#x27; &#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;- &quot;Do not use variables.\n\n&quot; &#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">&#123;%- <span class="name"><span class="name">for</span></span> t <span class="keyword">in</span> tools %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    </span><span class="template-variable">&#123;&#123;- t | tojson(indent=4) &#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml">    </span><span class="template-variable">&#123;&#123;- &quot;\n\n&quot; &#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">&#123;%- <span class="name"><span class="name">endfor</span></span> %&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-variable">&#123;&#123;- first_user_message + &quot;&lt;|eot_id|&gt;&quot;&#125;&#125;</span><span class="language-xml"></span></span><br><span class="line"><span class="language-xml"></span><span class="template-tag">&#123;%- <span class="name"><span class="name">endif</span></span> %&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>请求处理：生成请求的 id <code>request_id = f&quot;chatcmpl-&#123;request.request_id&#125;&quot;</code>，确定采样方法 <code>beam_search</code> 还是 <code>sampling</code>，调用 AsyncEngine 的 <code>beam_search</code> 和 <code>generate</code> 方法获得一个 generator。</p>
</li>
<li><p><code>chat_completion_stream_generator</code> 是基于 generator 处理响应，这里主要看 streaming 的部分，同步的请求会直接返回结果。流式响应的格式是多个基于 json 格式的 chunk，类型是 <code>chat.completion.chunk</code>。</p>
</li>
</ul>
 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chatcmpl-1eadb733adf64f5b90114307b2d4d718&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;choices&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;delta&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;function_call&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;refusal&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;tool_calls&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="attr">&quot;finish_reason&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span> <span class="attr">&quot;logprobs&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="attr">&quot;created&quot;</span><span class="punctuation">:</span> <span class="number">1732869116</span><span class="punctuation">,</span> <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;llama3.2&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chat.completion.chunk&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;service_tier&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;system_fingerprint&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;usage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chatcmpl-1eadb733adf64f5b90114307b2d4d718&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;choices&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;delta&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;AI&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;function_call&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;refusal&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;tool_calls&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="attr">&quot;finish_reason&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span> <span class="attr">&quot;logprobs&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="attr">&quot;created&quot;</span><span class="punctuation">:</span> <span class="number">1732869116</span><span class="punctuation">,</span> <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;llama3.2&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chat.completion.chunk&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;service_tier&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;system_fingerprint&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;usage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chatcmpl-1eadb733adf64f5b90114307b2d4d718&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;choices&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;delta&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot; assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;function_call&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;refusal&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;tool_calls&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="attr">&quot;finish_reason&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span> <span class="attr">&quot;logprobs&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="attr">&quot;created&quot;</span><span class="punctuation">:</span> <span class="number">1732869116</span><span class="punctuation">,</span> <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;llama3.2&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chat.completion.chunk&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;service_tier&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;system_fingerprint&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;usage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chatcmpl-1eadb733adf64f5b90114307b2d4d718&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;choices&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;delta&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;function_call&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;refusal&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;tool_calls&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="attr">&quot;finish_reason&quot;</span><span class="punctuation">:</span> <span class="string">&quot;stop&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;index&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span> <span class="attr">&quot;logprobs&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="attr">&quot;created&quot;</span><span class="punctuation">:</span> <span class="number">1732869116</span><span class="punctuation">,</span> <span class="attr">&quot;model&quot;</span><span class="punctuation">:</span> <span class="string">&quot;llama3.2&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;object&quot;</span><span class="punctuation">:</span> <span class="string">&quot;chat.completion.chunk&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;service_tier&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;system_fingerprint&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;usage&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p> AsyncEngine Client 的 generate 会返回一个异步生成器，result_generator，通过 <code>async for</code> 遍历这个生成器 result，而 result 又是一个 output 的生成器。<code>num_cached_tokens</code> 表示前缀匹配的 kv cache 命中的 token 数量。<code>request.n</code> 代表要生成的选择的数量，一般是 1，如果大于 1 就会生成多个选择的分支，而 <code>response</code> 中的 index 就会代表不同的分支的序号。result 生成器对应的就是多个分支的结果，而 result 中的 output 就代表一个分支中的 chunk。处理过程中会把 output 转化成 <code>ChatCompletionStreamResponse</code>，输出成 <code>data: $json_dump</code> 的 SSE chunk 的形式。<code>stream_options.include_usage</code> 如果设置了的话会在 DONE 之前返回一个 usage stats 的 chunk。</p>
<ul>
<li><code>tool_parser</code>：解析工具描述。方法和对应的类在 <code>openai/tool_parsers</code> 下面，会根据传入的初始化参数决定对应的解析类。如果对应的 request 有 <code>tool_choice</code> 参数，就会使用到 <code>tool_parser</code>，tool_parser 主要用于处理响应中的 tool call 的文本内容。<code>tool_parser</code> 在 <code>tool_choice</code> 为 auto 的时候要调用对应的 <code>extract_tool_calls_streaming</code> 去解析函数调用的文本内容。例如 <code>pythonic_tool_parser</code> 会解释 <code>[func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]</code> 这种类似 Python 的文本内容并转化为响应中的 <code>ToolCall</code> 对象。如果是 llama3.1 的 template 的话，参考上面的格式，会把输出 <code>&#123;&quot;name&quot;: function name, &quot;parameters&quot;: dictionary of argument name and its value&#125;</code> 转化为 <code>ToolCall</code> 对象。</li>
</ul>
</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>vLLM 的主体是 LLMEngine，它是一个单例类，负责管理所有的模型和数据。在基于FastAPI的异步Restful API 中使用的是一个 AsyncEngine。在交给Engine处理之前会对一些请求参数进行预处理，比如对话模板的格式化，对话消息的解析，模板中的函数调用等。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ggaaooppeenngg.github.io/zh-CN/2024/11/27/VLM%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%92%8C%E6%8E%A8%E6%96%AD%E4%B8%AD%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ggaaooppeenngg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ggaaooppeenngg">
      <meta itemprop="description" content="为什么计算机科学是无限的但生命是有限的">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | ggaaooppeenngg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/zh-CN/2024/11/27/VLM%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%92%8C%E6%8E%A8%E6%96%AD%E4%B8%AD%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/" class="post-title-link" itemprop="url">VLM的计算过程和推断中的处理方式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-27 17:54:04" itemprop="dateCreated datePublished" datetime="2024-11-27T17:54:04+08:00">2024-11-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-03-28 18:39:05" itemprop="dateModified" datetime="2025-03-28T18:39:05+08:00">2025-03-28</time>
    </span>

  
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/zh-CN/2024/11/27/VLM%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E5%92%8C%E6%8E%A8%E6%96%AD%E4%B8%AD%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="zh-CN/2024/11/27/VLM的计算过程和推断中的处理方式/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282">这篇文章</a>对VLM的架构解释得非常清楚。</p>
<p>一种方法是使用适配器将图片转换为tokens，例如<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.08485">LLaVA</a>使用的prompt based适配器。这种方法类似于RAG的形式，将图片理解的内容补充在对话的上文中。这种适配器会占用LLM的上下文长度，因为图片的tokens会被放入LLM的上文中。目前来说性能会好一些。</p>
<p>另一种方法是基于交叉注意力的适配器，这种方法不会占用LLM的上下文长度，但需要大量参数来达到良好的质量。<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.21783">Llama3.2</a>就是这种结构。</p>
<blockquote>
<p>关于Llama3.2本身，它使用了GQA，将kv head分组，多头查询将原本的K和V头分成组并为每个组生成一个共享的Head，这样可以减少kv cache而不太丧失精度（相较于MQA这种只共享一个KV头的方法）。因此，分组多头查询在多头查询注意力和正常多头注意力之间维持了平衡，既考虑了速度，又考虑了输出质量。另一个优化是对一个上下文中的不同文档进行mask处理。由于大模型的上下文现在很长，会将多个文档放入一个上下文中进行训练，但为了避免文档之间的相互影响，需要在文档级别进行mask处理，即当前token不能看到之后的token，也不能看到同一上下文中其他文档的token。其他改动主要是训练规模的调整。</p>
</blockquote>
<p>根据Llama3.2的<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.21783">技术报告</a>，里面的image encoder用的是ViT架构。适配器在语言模型和图像编码器之间引入交叉注意力层（cross-attention layers），以提高模型的效率和准确性。交叉注意力层使用通用查询注意力（GQA）并在核心语言模型每四层之后应用。交叉注意力层增加了大量可训练参数，例如Llama 3 405B中约有100B个参数。</p>
<p>本质上，图片编码器的输出通过适配器后作为交叉注意力层的K，文本作为Q，V也来自图片适配器，从而计算文字和图片之间的注意力关系，然后与LLM的输出进行交叉注意力。在训练Llama3.2的适配器时，同时更新了图像编码器的参数，但刻意不更新语言模型的参数。这意味着在适配器训练过程中，Meta只关注图像编码器和适配器的学习，而不影响语言模型的预训练知识。<br>简而言之，这个适配器在功能上类似于最初的encoder-decoder Transformer中的encoder部分。</p>
<p>在具体的以vLLM推断过程的实现为例，对话的API中会包含<code>&#123;&quot;type&quot;:&quot;image&quot;,&quot;image_url&quot;:&quot;uri_of_the_image&quot;&#125;</code>，在应用对话模板以后会插入占位符，比如llama3.2用的就是<code>&lt;|image|&gt;</code>，原始的训练中的文本内容会变成类似<code>&quot;&lt;|image|&gt;If I had to write a haiku for this one&quot;</code>，以此标记图片的位置信息，实际上需要图片会通过<code>uri_of_the_image</code>被加载到encoder中并携带<code>&lt;|image|&gt;</code>所代表的位置信息编码。</p>
<p>总的来说，VLM的计算过程和推断中的处理方式通过引入适配器和交叉注意力层，实现了图片和文本的高效融合，为多模态任务提供了强大的支持。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2014 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">ggaaooppeenngg</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




<script class="next-config" data-name="disqus" type="application/json">{"enable":true,"shortname":"ggaaooppeenngg","count":true,"i18n":{"disqus":"disqus"}}</script>
<script src="/js/third-party/comments/disqus.js"></script>

</body>
</html>
